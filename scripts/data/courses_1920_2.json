{"Bioinfo": {"supervisions": 3, "lectures": 12, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-Bioinformatics.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Introduction to biological data:</b> Bioinformatics as an interesting\nfield in computer science. Computing and storing information with DNA\n(including Adleman\u2019s experiment).\n\n<p></p></li>\n<li><b>Dynamic programming.</b> Longest common subsequence, DNA global and\nlocal alignment, linear space alignment, Nussinov algorithm for RNA, heuristics\nfor multiple alignment. (Vol. 1, chapter 5)\n\n<p></p></li>\n<li><b>Sequence database search.</b> Blast. (see notes and textbooks)\n\n<p></p></li>\n<li><b>Genome sequencing.</b> De Bruijn graph. (Vol. 1, chapter 3)\n\n<p></p></li>\n<li><b>Phylogeny.</b> Distance based algorithms (UPGMA, Neighbour-Joining).\nParsimony-based algorithms. Examples in Computer Science. (Vol. 2, chapter 7)\n\n<p></p></li>\n<li><b>Clustering.</b> Hard and soft K-means clustering, use of Expectation\nMaximization in clustering, Hierarchical clustering, Markov clustering\nalgorithm. (Vol. 2, chapter 8)\n\n<p></p></li>\n<li><b>Genomics Pattern Matching.</b> Suffix Tree String Compression and the\nBurrows-Wheeler Transform. (Vol. 2, chapter 9)\n\n<p></p></li>\n<li><b>Hidden Markov Models.</b> The Viterbi algorithm, profile HMMs for\nsequence alignment, classifying proteins with profile HMMs, soft decoding\nproblem, Baum-Welch learning. (Vol. 2, chapter 10)\n\n<p></p></li>\n\nObjectives\nAt the end of this course students should\n\n\n<li>understand Bioinformatics terminology;\n\n<p></p></li>\n<li>have mastered the most important algorithms in the field;\n\n<p></p></li>\n<li>be able to work with bioinformaticians and biologists;\n\n<p></p></li>\n<li>be able to find data and literature in repositories.\n\n<p></p></li>\n\nRecommended reading\n* Compeau, P. &amp; Pevzner, P.A. (2015). <em>Bioinformatics algorithms: an\nactive learning approach</em>. Active Learning Publishers.\n<br/>Durbin, R., Eddy, S., Krough, A. &amp; Mitchison, G. (1998). <em>Biological sequence analysis: probabilistic models of proteins and\nnucleic acids</em>. Cambridge University Press.\n<br/>Jones, N.C. &amp; Pevzner, P.A. (2004). <em>An introduction to bioinformatics\nalgorithms</em>. MIT Press.\n<br/>Felsenstein, J. (2003). <em>Inferring phylogenies</em>. Sinauer Associates.\n\n\n", "course_name": "Bioinformatics", "course_code": "Bioinfo", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/Bioinfo", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "Business": {"supervisions": 2, "lectures": 8, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-BusinessStudies.html", "description": "\n\n\nAims\nSee also Business Seminars in the Easter Term.\n\nLectures\n<li><b>So you\u2019ve got an idea?</b>\nIntroduction. Why are you doing it and what is it? Types of\ncompany. Market analysis. The business plan. \n\n<p></p></li>\n<li><b>Money and tools for its management.</b>\nIntroduction to accounting: profit and loss, cash flow, balance sheet,\nbudgets. Sources of finance. Stocks and shares. Options and futures.\n\n<p></p></li>\n<li><b>Setting up: legal aspects.</b>\nCompany formation. Brief introduction to business law; duties of\ndirectors.  Shares, stock options, profit share schemes and the like.\nIntellectual Property Rights, patents, trademarks and\ncopyright. Company culture and management theory.\n\n<p></p></li>\n<li><b>People.</b>\nMotivating factors. Groups and teams. Ego. Hiring and firing:\nemployment law. Interviews. Meeting techniques.\n\n<p></p></li>\n<li><b>Project planning and management.</b>\nRole of a manager. PERT and GANTT charts, and critical path\nanalysis. Estimation techniques. Monitoring.\n\n<p></p></li>\n<li><b>Quality, maintenance and documentation.</b>\nDevelopment cycle. Productization. Plan for quality. Plan for\nmaintenance. Plan for documentation.\n\n<p></p></li>\n<li><b>Marketing and selling.</b>\nSales and marketing are different. Marketing; channels; marketing\ncommunications.  Stages in selling. Control and commissions.\n\n<p></p></li>\n<li><b>Growth and exit routes.</b>\nNew markets: horizontal and vertical expansion. Problems of growth;\nsecond system effects. Management structures. Communication. Exit\nroutes: acquisition, floatation, MBO or\nliquidation. Futures: some emerging\nideas for new computer businesses.\nSummary. Conclusion: now you do it!\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should\n\n\n<li>be able to write and analyse a business plan;\n\n<p></p></li>\n<li>know how to construct PERT and GANTT diagrams and perform critical\npath analysis;\n\n<p></p></li>\n<li>appreciate the differences between profitability and cash flow, and\nhave some notion of budget estimation;\n\n<p></p></li>\n<li>have an outline view of company formation, share structure, capital\nraising, growth and exit routes;\n\n<p></p></li>\n<li>have been introduced to concepts of team formation and management;\n\n<p></p></li>\n<li>know about quality documentation and productization processes;\n\n<p></p></li>\n<li>understand the rudiments of marketing and the sales process.\n\n<p></p></li>\n\nRecommended reading\nLang, J. (2001). <em>The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech company</em>. FT.COM/Prentice\u00a0Hall.\n\nStudents will be expected to be able to use Microsoft Excel and Microsoft\nProject.\n\nFor additional reading on a lecture-by-lecture basis, please see the course\nwebsite.\n\nStudents are strongly recommended to enter the CU Entrepreneurs Business\nIdeas Competition <a href=\"http://www.cue.org.uk/\" name=\"tex2html15\"><tt>http://www.cue.org.uk/</tt></a>\n\n", "course_name": "Business Studies", "course_code": "Business", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/Business", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "DenotSem": {"supervisions": 3, "lectures": 10, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-DenotationalSemantics.html", "description": "\n\n\nAims\nLectures\n<li><b>Introduction.</b>The denotational approach to the semantics of programming languages.Recursively defined objects as limits of successive approximations.\n<p></p></li>\n<li><b>Least fixed points.</b>Complete \npartial orders\u00a0(cpos) and least elements.Continuous \nfunctions and least fixed points.\n\n<p></p></li>\n<li><b>Constructions on domains.</b>Flat domains.Product domains. Function domains.\n\n<p></p></li>\n<li><b>Scott induction.</b>Chain-closed and admissible subsets of cpos and domains.Scott\u2019s fixed-point induction principle. \n<p></p></li>\n<li><b>PCF.</b>The Scott-Plotkin language\u00a0PCF.Evaluation. Contextual equivalence.\n\n<p></p></li>\n<li><b>Denotational semantics of PCF.</b>Denotation of types and terms. Compositionality. Soundness with respect to evaluation. [2\u00a0lectures].\n\n<p></p></li>\n<li><b>Relating denotational and operational semantics.</b>Formal approximation relation and its fundamental property.Computational adequacy of the PCF denotational semantics with respect to\nevaluation. Extensionality properties of contextual equivalence. [2\u00a0lectures].\n\n<p></p></li>\n<li><b>Full abstraction.</b>Failure of full abstraction for the domain model.  PCF with parallel\u00a0or.\n<p></p></li>\n\nObjectives\nAt the end of the course students should\n\n\n<li>be familiar with basic domain theory: cpos, continuous functions,\nadmissible subsets, least fixed points, basic constructions on domains;\n\n<p></p></li>\n<li>be able to give denotational semantics to simple programming languages\nwith simple types;\n\n<p></p></li>\n<li>be able to apply denotational semantics; in particular, to understand the\nuse of least fixed points to model recursive programs and be able to\nreason about least fixed points and simple recursive programs using\nfixed point induction;\n\n<p></p></li>\n<li>understand the issues concerning the relation between denotational and\noperational semantics, adequacy and full abstraction, especially with\nrespect to the language\u00a0PCF.\n\n<p></p></li>\n\nRecommended reading\nWinskel, G. (1993). <em>The formal semantics of programming languages: an introduction</em>. MIT Press.\n<br/>Gunter, C. (1992). <em>Semantics of programming languages: structures and techniques</em>. MIT Press.\n<br/>Tennent, R. (1991). <em>Semantics of programming languages</em>.  Prentice Hall.\n\n\n\n", "course_name": "Denotational Semantics", "course_code": "DenotSem", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/DenotSem", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "InfoTheory": {"supervisions": 0, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-InformationTheory.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Foundations:  probability, uncertainty, information.</b>\nHow concepts of randomness, redundancy, compressibility, noise,\nbandwidth, and uncertainty are related to information.  Ensembles,\nrandom variables, marginal and conditional probabilities.  How the\nmetrics of information are grounded in the rules of probability.\n\n<p></p></li>\n<li><b>Entropies defined, and why they are measures of information.</b> \nMarginal joint and conditional entropy; chain rule for entropy.\nCross-entropy and distances between distributions.  Mutual information\nbetween random variables.  Why entropy gives fundamental measures of\ninformation content.\n\n<p></p></li>\n<li><b>Source coding theorem; prefix, variable-, and fixed-length codes.</b>\nMarkov sources.  Entropy of a multi-state Markov process.  Symbol codes; \nHuffman codes and the prefix property.  Binary symmetric channels. \nCapacity of a noiseless discrete channel.\n\n<p></p></li>\n<li><b>Noisy discrete channel properties, and channel capacity.</b>\nPerfect communication through a noisy channel:  error-correcting codes.\nCapacity of a discrete channel as the maximum of its mutual information.\n\n<p></p></li>\n<li><b>Information represented by projections and in transforms.</b>\nExpressing data in vector spaces or as a linear combination of basis functions.\nInner product spaces and orthonormal systems.  Norms, span, and linear subspaces; \ndimensionality reduction.\n\n<p></p></li>\n<li><b>Fourier analysis:  series and transforms, discrete or continuous.</b>\nHow periodic and aperiodic data are analysed and represented by Fourier\nmethods.  Rates of convergence.  Information revealed in the Fourier domain.\nDiscrete, inverse, and Fast Fourier Transforms; butterfly algorithm.  \nDuality properties.  Wavelet transforms.\n\n<p></p></li>\n<li><b>Spectral properties of continuous-time signals and channels.</b>\nSignals represented as combinations of complex exponential eigenfunctions;\nchannels represented as spectral filters that add noise.  Convolution.\nApplying Fourier analysis to communication schemes.\n\n<p></p></li>\n<li><b>Continuous information; density; noisy channel coding theorem.</b>\nExtensions of discrete entropies and measures to the continuous case.\nSignal-to-noise ratio; power spectral density.  Gaussian channels.\nRelative significance of bandwidth and noise limitations.  \nThe Shannon rate limit for noisy continuous channels.\n\n<p></p></li>\n<li><b>Signal coding and transmission schemes using Fourier theorems.</b>\nNyquist Sampling Theorem.   Aliasing and its prevention.\nModulation and shift theorems; multiple carriers; frequency and\nphase modulation codes; ensembles.  Filters, coherence, demodulation;\nnoise removal by correlation.\n\n<p></p></li>\n<li><b>The quantized degrees-of-freedom in a continuous signal.</b>\nWhy a continuous signal of finite bandwidth and duration has a fixed\nnumber of degrees-of-freedom.  Diverse illustrations of the principle\nthat information, even in such a signal, comes in quantized, countable,\npackets.\n\n<p></p></li>\n<li><b>Gabor-Heisenberg-Weyl uncertainty relation.  Optimal \u201cLogons\u201d.</b>\nUnification of the time-domain and the frequency-domain as endpoints \nof a continuous deformation.  The Uncertainty Principle and its optimal\nsolution by Gabor\u2019s expansion basis of \u201clogons\u201d.  Multi-resolution \nwavelet codes.  Extension to images, for analysis and compression.\n\n<p></p></li>\n<li><b>Data compression codes and protocols.</b>\nRun-length coding; dictionary methods on strings; vector quantisation;\nJPEG and JP2K image compression; orthogonal subspace projections;\npredictive coding; the Laplacian pyramid; and wavelet scalar\nquantisation.\n\n<p></p></li>\n<li><b>Kolmogorov complexity.  Minimal description length.</b>\nDefinition of the algorithmic complexity of a data sequence, and \nits relation to the entropy of the distribution from which the data \nwas drawn.  Fractals.  Minimal description length, and why this measure\nof complexity is not computable.\n\n<p></p></li>\n<li><b>Applications of information theory in other sciences.</b>\nUse of information metrics and analysis in:  genomics; neuroscience;\nastrophysics; noisy signal classification; and pattern recognition\nincluding biometrics.\n</li>\n\nObjectives\nAt the end of the course students should be able to\n\n\n<li>calculate the information content of a random variable\nfrom its probability distribution;\n\n<p></p></li>\n<li>relate the joint, conditional, and marginal entropies \nof variables in terms of their coupled probabilities;\n\n<p></p></li>\n<li>define channel capacities and properties using Shannon\u2019s Theorems;\n\n<p></p></li>\n<li>construct efficient codes for data on imperfect communication channels;\n\n<p></p></li>\n<li>generalize the discrete concepts to continuous signals on continuous \nchannels;\n\n<p></p></li>\n<li>understand encoding and communication schemes in terms of\nthe spectral properties of signals and channels;\n\n<p></p></li>\n<li>describe compression schemes, and efficient coding using wavelets\nand other representations for data.\n</li>\n\nRecommended reading\n* Cover, T.M. &amp; Thomas, J.A. (2006). <em>Elements of information theory</em>. New York: Wiley.\n\n\n", "course_name": "Information Theory", "course_code": "InfoTheory", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/InfoTheory", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "TeX+MATLAB": {"supervisions": 0, "lectures": 2, "prerequisite_for": [], "past_exam_questions": null, "description": "\n\n\nAims\nLectures\n<li><b><span class=\"logo,LaTeX\">L<sup><small>A</small></sup>T<small>E</small>X</span>.</b> Workflow example, syntax, typesetting conventions,\n  non-ASCII characters, document structure, packages, mathematical\n  typesetting, graphics and figures, cross references, build tools.\n\n<p></p></li>\n<li><b>MATLAB.</b> Tools for technical computing and visualization.\n  The matrix type and its operators, 2D/3D plotting, common functions,\n  function definitions, toolboxes, vectorized audio demonstration.\n\n<p></p></li>\n\nObjectives\nStudents should be able to avoid the most common <span class=\"logo,LaTeX\">L<sup><small>A</small></sup>T<small>E</small>X</span> mistakes,\nto prototype simple image and signal processing\nalgorithms in MATLAB, and to visualize the results.\n\nRecommended reading\n* Lamport, L. (1994). <em><span class=\"logo,LaTeX\">L<sup><small>A</small></sup>T<small>E</small>X</span> - a documentation preparation system user\u2019s guide and reference manual</em>. Addison-Wesley (2nd ed.).\n\nMittelbach, F., et al. (2004). <em>The <span class=\"logo,LaTeX\">L<sup><small>A</small></sup>T<small>E</small>X</span> companion</em>. Addison-Wesley (2nd ed.).\n\n\n", "course_name": "LaTeX and MATLAB", "course_code": "TeX+MATLAB", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/TeX+MATLAB", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "PrincComm": {"supervisions": 4, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-PrinciplesofCommunications.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Introduction.</b>\nCourse overview. Abstraction, layering.\nReview of structure of real networks, links, end systems and switching\nsystems. [1\u00a0lecture]\n\n<p></p></li>\n<li><b>Routing.</b>\nCentral versus Distributed Routing\nPolicy Routing.\nMulticast Routing\nCircuit Routing\n[6\u00a0lectures]\n\n<p></p></li>\n<li><b>Flow control and resource optimisation.</b>\nControl theory is a branch of engineering familiar to people building dynamic machines. It can be\napplied to network traffic.\nStemming the flood, at source, sink, or in between? Optimisation as\na model of network&amp; user. TCP in the wild. \n[3\u00a0lectures]\n\n<p></p></li>\n<li><b>Packet Scheduling.</b>\nDesign choices for scheduling and queue management\nalgorithms for packet forwarding, and fairness.  \n[2\u00a0lectures]\n\n<p></p></li>\n<li><b>The big picture for managing traffic.</b>\nEconomics and policy are relevant to networks in many ways.\nOptimisation and game theory are both relevant topics discussed here.\n[2\u00a0lectures]\n\n<p></p></li>\n<li><b>System Structures and Summary.</b>\nAbstraction, layering.\nThe structure of real networks, links, end systems and switching.\n[2\u00a0lectures]\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should be able to explain the\nunderlying design and behaviour of protocols and networks, including capacity,\ntopology, control and use. Several specific mathematical approaches\nare covered (control theory, optimisation).\n\nRecommended reading\n* Keshav, S. (2012). <em>Mathematical Foundations of Computer Networking</em>. Addison Wesley. ISBN 9780321792105\n<br/>Background reading:\n<br/>Keshav, S. (1997). <em>An engineering approach to computer networking</em>. Addison-Wesley (1st ed.). ISBN 0201634422\n<br/>Stevens, W.R. (1994). <em>TCP/IP illustrated, vol.\u00a01: the protocols</em>. Addison-Wesley (1st ed.). ISBN 0201633469\n\n\n", "course_name": "Principles of Communications", "course_code": "PrincComm", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/PrincComm", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "Types": {"supervisions": 3, "lectures": 12, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-Types.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Introduction.</b>  The role of type systems in programming\n  languages. Review of rule-based formalisation of type\n  systems. [1\u00a0lecture]\n\n<p></p></li>\n<li><b>Propositions as types.</b> The Curry-Howard correspondence\n  between intuitionistic propositional calculus and simply-typed\n  lambda calculus. Inductive types and iteration. Consistency and\n  termination. [2\u00a0lectures]\n\n<p></p></li>\n<li><b>Polymorphic lambda calculus (PLC).</b>  PLC syntax and\n  reduction semantics. Examples of datatypes definable in the\n  polymorphic lambda calculus. Type inference. [3\u00a0lectures]\n\n<p></p></li>\n<li><b>Monads and effects</b>. Explicit versus implicit effects. Using\n  monadic types to control effects. References and\n  polymorphism. Recursion and looping. [2\u00a0lectures]\n\n<p></p></li>\n<li><b>Continuations and classical logic</b>. First-class continuations\n  and control operators. Continuations as Curry-Howard for classical logic.\n  Continuation-passing style. [2\u00a0lectures]\n\n<p></p></li>\n<li><b>Dependent types.</b> Dependent function types. Indexed datatypes.\n  Equality types and combining proofs with programming. [2\u00a0lectures]\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should\n\n\n<li>be able to use a rule-based specification of a type system to\n  carry out type checking and type inference;\n\n<p></p></li>\n<li>understand by example the Curry-Howard correspondence between\n  type systems and logics;\n\n<p></p></li>\n<li>understand how types can be used to control side-effects in\n  programming;\n\n<p></p></li>\n<li>appreciate the expressive power of parametric polymorphism and\n  dependent types.\n\n<p></p></li>\n\nRecommended reading\n* Pierce, B.C. (2002). <em>Types and programming languages</em>. MIT\nPress.\n<br/>Pierce, B. C. (Ed.) (2005). <em>Advanced Topics in Types and\n  Programming Languages</em>. MIT Press.\n<br/>Girard, J-Y. (tr. Taylor, P. &amp; Lafont, Y.) (1989). <em>Proofs and\n  types</em>. Cambridge University Press. \n\n\n", "course_name": "Types", "course_code": "Types", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/Types", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": true, "lent": false, "easter": false}, "CompArch": {"supervisions": 0, "lectures": 0, "prerequisite_for": [], "past_exam_questions": null, "description": null, "course_name": "Comparative Architectures", "course_code": "CompArch", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/CompArch", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "CompVision": {"supervisions": 3, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-ComputerVision.html", "description": "\n\n\nAims\nLectures\n<li><b>Goals of computer vision; why they are so difficult.</b>\nImage formation, and the ill-posed problem of making 3D\ninferences about objects and their properties from images. \n\n<p></p></li>\n<li><b>Image sensing, pixel arrays, CCD and CMOS cameras.</b>\nImage coding and information measures.  Elementary operations on image arrays.\n\n<p></p></li>\n<li><b>Biological visual mechanisms, from retina to cortex.</b>\nPhotoreceptor sampling; receptive field profiles; stochastic impulse \ncodes; channels and pathways.  Neural image encoding operators. \n\n<p></p></li>\n<li><b>Mathematical operations for extracting image structure.</b>\nFinite differences and directional derivatives.\nFilters; convolution; correlation.  2D Fourier domain theorems.\n\n<p></p></li>\n<li><b>Edge detection operators; the information revealed by edges.</b>\nGradient vector field;  Laplacian operator and its zero-crossings. \n\n<p></p></li>\n<li><b>Multi-scale contours, feature detection and matching.</b>   \nSIFT (scale-invariant feature transform); pyramids. 2D wavelets as \nvisual primitives.  Active contours.  Energy-minimising snakes.\n\n<p></p></li>\n<li><b>Higher visual operations in brain cortical areas.</b>\nMultiple parallel mappings; streaming and divisions of labour;\nreciprocal feedback through the visual system. \n\n<p></p></li>\n<li><b>Texture, colour, stereo, and motion descriptors.</b>\nDisambiguation and the achievement of invariances. \nColour computation, motion and image segmentation.\n\n<p></p></li>\n<li><b>Lambertian and specular surfaces; reflectance maps.</b>\nGeometric analysis of image formation from surfaces.  Discounting the \nilluminant when inferring 3D structure and surface properties.\n\n<p></p></li>\n<li><b>Shape representation.</b>  Inferring 3D shape from shading;\nsurface geometry.  Boundary descriptors; codons.  Object-centred\nvolumetric coordinates. \n\n<p></p></li>\n<li><b>Perceptual organisation and cognition.</b>  Vision\nas model-building and graphics in the brain.  Learning to see.\n\n<p></p></li>\n<li><b>Lessons from neurological trauma and visual deficits.</b>\nVisual agnosias and illusions, and what they may imply about how vision works.\n\n<p></p></li>\n<li><b>Bayesian inference in vision; knowledge-driven interpretations.</b>  \nClassifiers, decision-making, and pattern recognition.  \n\n<p></p></li>\n<li><b>Model estimation.</b> Machine learning and statistical methods\nin vision.\n\n<p></p></li>\n<li><b>Applications of machine learning in computer vision.</b>  \nDiscriminative and generative methods.  Content based image retrieval.\n\n<p></p></li>\n<li><b>Approaches to face detection, face recognition, and facial\ninterpretation.</b>  Cascaded detectors.  Appearance <em>versus</em> model-based\nmethods. \n\n<p></p></li>\n\nObjectives\nAt the end of the course students should\n\n\n<li>understand visual processing from both \u201cbottom-up\u201d (data oriented) and\n\u201ctop-down\u201d (goals oriented) perspectives;\n\n<p></p></li>\n<li>be able to decompose visual tasks into sequences of image analysis\noperations, representations, specific algorithms, and inference principles;\n\n<p></p></li>\n<li>understand the roles of image transformations and their invariances\nin pattern recognition and classification;\n\n<p></p></li>\n<li>be able to describe and contrast techniques for extracting and representing\nfeatures, edges, shapes, and textures;\n\n<p></p></li>\n<li>be able to describe key aspects of how biological visual systems work;\nand be able to think of ways in which biological visual strategies might be\nimplemented in machine vision, despite the enormous differences in hardware;\n\n<p></p></li>\n<li>be able to analyse the robustness, brittleness, generalizability,\nand performance of different approaches in computer vision;\n\n<p></p></li>\n<li>understand the roles of machine learning in computer vision today,\nincluding probabilistic inference, discriminative and generative methods; \n\n<p></p></li>\n<li>understand in depth at least one major practical application problem,\nsuch as face recognition, detection, or interpretation.\n\n<p></p></li>\n\nRecommended reading\n* Forsyth, D. A. &amp; Ponce, J. (2003).  <em>Computer Vision: A Modern Approach</em>.\nPrentice\u00a0Hall.\n\nShapiro, L. &amp; Stockman, G. (2001).  <em>Computer vision</em>. Prentice\u00a0Hall.\n\n\n", "course_name": "Computer Vision", "course_code": "CompVision", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/CompVision", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "Crypto": {"supervisions": 3, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-Cryptography.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Cryptography.</b> Overview, private vs. public-key ciphers,\n  MACs vs. signatures, certificates, capabilities of adversary,\n  Kerckhoffs\u2019 principle.\n\n<p></p></li>\n<li><b>Classic ciphers.</b> Attacks on substitution and transposition\n  ciphers, Vigen\u00e9re. Perfect secrecy: one-time pads.\n\n<p></p></li>\n<li><b>Private-key encryption.</b> Stream ciphers, pseudo-random\n  generators, attacking linear-congruential RNGs and LFSRs. Semantic\n  security definitions, oracle queries, advantage, computational\n  security, concrete-security proofs.\n\n<p></p></li>\n<li><b>Block ciphers.</b> Pseudo-random functions and permutations.\n  Birthday problem, random mappings. Feistel/Luby-Rackoff structure,\n  DES, TDES, AES.\n\n<p></p></li>\n<li><b>Chosen-plaintext attack security.</b> Security with multiple\n  encryptions, randomized encryption. Modes of operation: ECB, CBC,\n  OFB, CNT.\n\n<p></p></li>\n<li><b>Message authenticity.</b> Malleability, MACs, existential\n  unforgeability, CBC-MAC, ECBC-MAC, CMAC, birthday attacks,\n  Carter-Wegman one-time MAC.\n\n<p></p></li>\n<li><b>Authenticated encryption.</b> Chosen-ciphertext attack\n  security, ciphertext integrity, encrypt-and-authenticate,\n  authenticate-then-encrypt, encrypt-then-authenticate, padding oracle\n  example, GCM.\n\n<p></p></li>\n<li><b>Secure hash functions.</b> One-way functions, collision\n  resistance, padding, Merkle-Damg\u00e5rd construction, sponge\n  function, duplex construct, entropy pool, SHA standards.\n\n<p></p></li>\n<li><b>Applications of secure hash functions.</b> HMAC, stream\n  authentication, Merkle tree, commitment protocols, block chains,\n  Bitcoin.\n\n<p></p></li>\n<li><b>Key distribution problem.</b> Needham-Schroeder protocol,\n  Kerberos, hardware-security modules, public-key encryption schemes,\n  CPA and CCA security for asymmetric encryption.\n\n<p></p></li>\n<li><b>Number theory, finite groups and fields.</b> Modular\n  arithmetic, Euclid\u2019s algorithm, inversion, groups, rings, fields,\n  GF(<span class=\"MATH\">2<sup>n</sup></span>), subgroup order, cyclic groups, Euler\u2019s theorem, Chinese\n  remainder theorem, modular roots, quadratic residues, modular\n  exponentiation, easy and difficult problems. [2 lectures]\n\n<p></p></li>\n<li><b>Discrete logarithm problem.</b> Baby-step-giant-step algorithm,\n  computational and decision Diffie-Hellman problem, DH key exchange,\n  ElGamal encryption, hybrid cryptography, Schnorr groups,\n  elliptic-curve systems, key sizes. [2 lectures]\n\n<p></p></li>\n<li><b>Trapdoor permutations.</b> Security definition, turning one\n  into a public-key encryption scheme, RSA, attacks on \u201ctextbook\u201d\n  RSA, RSA as a trapdoor permutation, optimal asymmetric encryption\n  padding, common factor attacks.\n\n<p></p></li>\n<li><b>Digital signatures.</b> One-time signatures, RSA signatures,\n  Schnorr identification scheme, ElGamal signatures, DSA, PS3 hack,\n  certificates, PKI.\n\n<p></p></li>\n\nObjectives\nBy the end of the course students should\n\n\n<li>be familiar with commonly used standardized cryptographic\n  building blocks;\n\n<p></p></li>\n<li>be able to match application requirements with concrete security\n  definitions and identify their absence in naive schemes;\n\n<p></p></li>\n<li>understand various adversarial capabilities and basic attack\n  algorithms and how they affect key sizes;\n\n<p></p></li>\n<li>understand and compare the finite groups most commonly used with\n  discrete-logarithm schemes;\n\n<p></p></li>\n<li>understand the basic number theory underlying the most common\n  public-key schemes, and some efficient implementation techniques.\n\n<p></p></li>\n\nRecommended reading\nKatz, J., Lindell, Y. (2015). <em>Introduction to modern cryptography</em>. Chapman &amp; Hall/CRC (2nd ed.).\n\n\n", "course_name": "Cryptography", "course_code": "Crypto", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/Crypto", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "ECommerce": {"supervisions": 2, "lectures": 8, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-E-Commerce.html", "description": "\n\n\nAims\nLectures\n<li><b>The history of electronic commerce.</b> Mail order; EDI;\n  web-based businesses, credit card processing, PKI, identity and\n  other hot topics.\n\n<p></p></li>\n<li><b>Network economics.</b> Real and virtual networks, supply-side\n  <em>versus</em> demand-side scale economies, Metcalfe\u2019s law, the dominant\n  firm model, the differentiated pricing model Data Protection Act,\n  Distance Selling regulations, business models.\n\n<p></p></li>\n<li><b>Web site design.</b> Stock and price control; domain names,\n  common mistakes, dynamic pages, transition diagrams, content\n  management systems, multiple targets.\n\n<p></p></li>\n<li><b>Web site implementation.</b> Merchant systems, system design\n  and sizing, enterprise integration, payment mechanisms, CRM and help\n  desks. Personalisation and internationalisation.\n\n<p></p></li>\n<li><b>The law and electronic commerce.</b> Contract and tort;\n  copyright; binding actions; liabilities and remedies. Legislation:\n  RIP; Data Protection; EU Directives on Distance Selling and\n  Electronic Signatures.\n\n<p></p></li>\n<li><b>Putting it into practice.</b> Search engine interaction,\n  driving and analysing traffic; dynamic pricing models. Integration\n  with traditional media. Logs and audit, data mining modelling the\n  user. collaborative filtering and affinity marketing brand value,\n  building communities, typical behaviour.\n\n<p></p></li>\n<li><b>Finance.</b> How business plans are put together. Funding\n  Internet ventures; the recent hysteria; maximising shareholder\n  value. Future trends.\n\n<p></p></li>\n<li><b>UK and International Internet Regulation.</b> Data Protection\n  Act and US Privacy laws; HIPAA, Sarbanes-Oxley, Security Breach\n  Disclosure, RIP Act 2000, Electronic Communications Act 2000,\n  Patriot Act, Privacy Directives, data retention; specific issues:\n  deep linking, Inlining, brand misuse, phishing.\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should know how to apply their\ncomputer science skills to the conduct of e-commerce with some\nunderstanding of the legal, security, commercial, economic, marketing\nand infrastructure issues involved.\n\nRecommended reading\nShapiro, C. &amp; Varian, H. (1998). <em>Information rules</em>. Harvard Business School Press.\n\nAdditional reading:\n\nStandage, T. (1999). <em>The Victorian Internet</em>. Phoenix Press.\nKlemperer, P. (2004). <em>Auctions: theory and practice</em>. Princeton Paperback ISBN 0-691-11925-2.\n\n\n", "course_name": "E-Commerce", "course_code": "ECommerce", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/ECommerce", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "MLBayInfer": {"supervisions": 4, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-MachineLearningandBayesianInference.html", "description": "\n\n\nAims\nLectures\n<li><b>Introduction to learning and inference.</b> Supervised,\nunsupervised, semi-supervised and reinforcement learning. Bayesian\ninference in general. What the naive Bayes method actually does.\nReview of backpropagation. Other kinds of learning and inference.\n[1 lecture]\n\n<p></p></li>\n<li><b>How to classify optimally.</b> Treating learning\nprobabilistically. Bayesian decision theory and Bayes optimal\nclassification. Likelihood\nfunctions and priors. Bayes theorem as applied to supervised learning.\nThe maximum likelihood and maximum <em>a posteriori</em> hypotheses. What does\nthis teach us about the backpropagation algorithm? [2 lectures]\n\n<p></p></li>\n<li><b>Linear classifiers I.</b> Supervised learning via error\nminimization. Iterative reweighted least squares.  The maximum margin\nclassifier. [2 lectures]\n\n<p></p></li>\n<li><b>Gaussian processes.</b> Learning and inference\n  for regression using Gaussian process models.  [2 lectures]\n\n<p></p></li>\n<li><b>Support vector machines (SVMs).</b> The kernel trick. Problem\nformulation. Constrained optimization and the dual problem. SVM\nalgorithm. [2 lectures]\n\n<p></p></li>\n<li><b>Practical issues.</b> Hyperparameters. Measuring performance.\n  Cross-validation. Experimental methods. [1 lecture]\n\n<p></p></li>\n<li><b>Linear classifiers II.</b> The Bayesian approach to neural networks.\n[1 lecture]\n\n<p></p></li>\n<li><b>Unsupervised learning I.</b> The <span class=\"MATH\"><i>k</i></span>-means algorithm. Clustering\n  as a maximum likelihood problem. [1 lecture]\n\n<p></p></li>\n<li><b>Unsupervised learning II.</b> The EM algorithm and its\napplication to clustering. [1 lecture]\n\n<p></p></li>\n<li><b>Bayesian networks I.</b> Representing uncertain knowledge using\nBayesian networks. Conditional independence. Exact inference in\nBayesian networks. [2 lectures]\n\n<p></p></li>\n<li><b>Bayesian networks II.</b> Markov random fields. Approximate\n  inference. Markov chain Monte Carlo methods. [1 lecture]\n\n<p></p></li>\n\nObjectives\nAt the end of this course students should:\n\n\n<li>Understand how learning and inference can be captured within a\nprobabilistic framework, and know how probability theory can be\napplied in practice as a means of handling uncertainty in AI systems.\n\n<p></p></li>\n<li>Understand several algorithms for machine\nlearning and apply those methods in practice with proper regard for\ngood experimental practice.\n</li>\n\nRecommended reading\nIf you are going to buy a single book for this course I recommend:\n\n* Bishop, C.M. (2006). <span class=\"textit\">Pattern recognition and machine learning</span>.\nSpringer.\n<br/>The course text for Artificial Intelligence I:\n\nRussell, S. &amp; Norvig, P. (2010). <span class=\"textit\">Artificial intelligence: a\nmodern approach</span>. Prentice\u00a0Hall (3rd ed.).\n<br/>covers some relevant material but often in insufficient detail. Similarly:\n\nMitchell, T.M. (1997). <span class=\"textit\">Machine Learning</span>. McGraw-Hill.\n<br/>gives a gentle introduction to some of the course material, but only\nan introduction.\n\nRecently a few new books have appeared that cover a lot of\nrelevant ground well. For example:\n<br/>Barber, D. (2012). <span class=\"textit\">Bayesian Reasoning and Machine Learning</span>.\nCambridge University Press.\n<br/>Flach, P. (2012). <span class=\"textit\">Machine Learning: The Art and Science of Algorithms\nthat Make Sense of Data</span>. Cambridge University Press.\n<br/>Murphy, K.P. (2012). <span class=\"textit\">Machine Learning: A Probabilistic Perspective</span>.\nMIT Press.\n<br/>\n", "course_name": "Machine Learning and Bayesian Inference", "course_code": "MLBayInfer", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/MLBayInfer", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "MobSensSys": {"supervisions": 3, "lectures": 12, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-MobileandSensorSystems.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Introduction to Mobile Systems. MAC Layer concepts.</b>\nExamples of mobile systems, differences with non mobile systems. \nIntroduction to MAC layer protocols of wireless and mobile systems.\n\n<p></p></li>\n<li><b>Mobile Infrastructure Communication and Opportunistic\nNetworking.</b>  Description of common communication architectures and\nprotocols for mobile and introduction to models of opportunistic\nnetworking.\n\n<p></p></li>\n<li><b>Introduction to Sensor Systems, MAC Layer concepts and\nInternet of Things.</b>  Sensor systems challenges and applications.\nConcepts related to duty cycling and energy preservation protocols.\nEmerging concepts and communications protocols for the Internet of\nThings.\n\n<p></p></li>\n<li><b>Sensor Systems Routing Protocols.</b>\nCommunication protocols, data aggregation and dissemination in sensor \nnetworks.\n\n<p></p></li>\n<li><b>Machine Learning for Mobile Systems and Sensor Data</b>\nMobile and wearable sensing. Machine Learning on Sensor Data.  On-device\nMachine Learning.\n\n<p></p></li>\n<li><b>Mobile Sensing: Systems Considerations</b>\nConsiderations of energy preservation.  Local computation vs cloud computation.\nStorage and Latency.\n\n<p></p></li>\n<li><b>Privacy in Mobile and Sensor Systems.</b>\nConcepts of mobile and sensor systems privacy. Privacy and sensor based\nactivity inference. Mobility prediction and privacy.\n\n<p></p></li>\n<li><b>Localization I.</b> Basic concepts.\nProximity, trilateration, triangulation, ToA, TDoA. Examples including\nultrasonic, mobile networks, UWB.\n\n<p></p></li>\n<li><b>Localization II.</b> GNSS, RSS fingerprinting/WiFi positioning.\n\n<p></p></li>\n<li><b>Tracking.</b> Kalman filtering, particle filtering, Pedestrian\nDead-Reckoning as an example.\n\n<p></p></li>\n<li><b>Mobile/Wearable health sensing.</b>  Health as a key driver for\nwearables. High availability and low fidelity (wearable) vs low\navailability and high fidelity (clinical). Sensing challenges (low\npower, noisy, poor contact). PPG case study. False positives and the\nbase rate fallacy.\n\n<p></p></li>\n<li><b>Robots and Drones</b> Concepts related to control, communication and\ncoordination of robotic systems.\n\n<p></p></li>\n\nObjectives\nOn completing the course, students should be able to\n\n\n<li>describe similarities and differences between standard\n distributed systems and mobile and sensor systems;\n\n<p></p></li>\n<li>explain the fundamental tradeoffs related to energy limitations\n and communication needs in these systems;\n\n<p></p></li>\n<li>argue for and against different mobile and sensor systems\narchitectures and protocols.\n\n<p></p></li>\n<li>Understand typical error sources for sensing and be aware of\ntechniques to minimise them.\n\n<p></p></li>\n<li>put concepts into context of current applications of mobile and sensor\nsystems as described in the course.\n\n<p></p></li>\n\nRecommended reading\nThe course is based mainly on research papers cited in each lecture. The following books, however, contain some of the more traditional concepts.\n\n* Schiller, J. (2003). <em>Mobile communications</em>. Pearson (2nd ed.).\n<br/>* Karl, H. &amp; Willig, A. (2005). <em>Protocols and architectures for wireless sensor networks</em>. Wiley.\n<br/>Agrawal, D. &amp; Zheng, Q. (2006). <em>Introduction to wireless and mobile systems</em>. Thomson.\n\n\n", "course_name": "Mobile and Sensor Systems", "course_code": "MobSensSys", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/MobSensSys", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "OptComp": {"supervisions": 4, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-OptimisingCompilers.html", "description": "\n\n\nAims\nLectures\n<li><b>Introduction and motivation.</b>\nOutline of an optimising compiler.\nOptimisation partitioned: <i>analysis</i> shows a property holds\nwhich enables a <em>transformation</em>.\nThe flow graph; representation of programming concepts including argument\nand result passing.\nThe phase-order problem.\n\n<p></p></li>\n<li><b>Kinds of optimisation.</b>\nLocal optimisation: peephole optimisation, instruction scheduling.\nGlobal optimisation: common sub-expressions, code motion.\nInterprocedural optimisation.\nThe call graph.\n\n<p></p></li>\n<li><b>Classical dataflow analysis.</b>\nGraph algorithms, <em>live</em> and <em>avail</em> sets.\nRegister allocation by register colouring.\nCommon sub-expression elimination.\nSpilling to memory; treatment of CSE-introduced temporaries.\nData flow anomalies.\nStatic Single Assignment (SSA) form.\n\n<p></p></li>\n<li><b>Higher-level optimisations.</b>\nAbstract interpretation,  Strictness analysis.\nConstraint-based analysis, Control flow analysis for lambda-calculus.\nRule-based inference of program properties,\nTypes and effect systems.\nPoints-to and alias analysis.\n\n<p></p></li>\n<li><b>Target-dependent optimisations.</b>\nInstruction selection.\nInstruction scheduling and its phase-order problem.\n\n<p></p></li>\n<li><b>Decompilation.</b>\nLegal/ethical issues.\nSome basic ideas, control flow and type reconstruction.\n</li>\n\nObjectives\nAt the end of the course students should\n\n<li>be able to explain program analyses as dataflow equations on a\nflowgraph;\n\n<p></p></li>\n<li>know various techniques for high-level optimisation of programs\nat the abstract syntax level;\n\n<p></p></li>\n<li>understand how code may be re-scheduled to improve execution speed;\n\n<p></p></li>\n<li>know the basic ideas of decompilation.\n\n<p></p></li>\n\nRecommended reading\n* Nielson, F., Nielson, H.R. &amp; Hankin, C.L. (1999).  <em>Principles of program analysis</em>. Springer. Good on part A and part B.\n<br/>Appel, A. (1997). <em>Modern compiler implementation in Java/C/ML</em>  (3 editions).\n<br/>Muchnick, S. (1997). <em>Advanced compiler design and implementation</em>.  Morgan Kaufmann.\n<br/>Wilhelm, R. (1995). <em>Compiler design</em>. Addison-Wesley.\n<br/>Aho, A.V., Sethi, R. &amp; Ullman, J.D. (2007). <em>Compilers:  principles, techniques and tools</em>. Addison-Wesley (2nd ed.).\n\n", "course_name": "Optimising Compilers", "course_code": "OptComp", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/OptComp", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "QuantComp": {"supervisions": 4, "lectures": 16, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-QuantumComputing.html", "description": "\n\n\nAims\nLectures\n\n<li><b>Bits and qubits.</b>\nIntroduction to quantum states and measurements with motivating examples. Comparison with discrete classical states.\n\n<p></p></li>\n<li><b>Linear algebra.</b>\nReview of linear algebra: vector spaces, linear operators, Dirac\nnotation, the tensor product. \n\n<p></p></li>\n<li><b>The postulates of quantum mechanics.</b>\nPostulates of quantum mechanics, incl. evolution and measurement.\n\n<p></p></li>\n<li><b>Important concepts in quantum mechanics.</b>\nEntanglement, distinguishing orthogonal and non-orthogonal quantum states, no-cloning and no signalling.\n\n<p></p></li>\n<li><b>The quantum circuit model.</b>\nThe circuit model of quantum computation. Quantum gates and circuits. Universality of the quantum circuit model, and efficient simulation of arbitrary two-qubit gates with a standard universal set of gates.\n\n<p></p></li>\n<li><b>Some applications of quantum information.</b>\nApplications of quantum information (other than quantum computation):\nquantum key distribution, superdense coding and quantum teleportation.\n\n<p></p></li>\n<li><b>Deutsch-Jozsa algorithm.</b>\nIntroducing Deutsch\u2019s problem and Deutsch\u2019s algorithm leading onto its generalisation, the Deutsch-Jozsa algorithm.\n\n<p></p></li>\n<li><b>Quantum search.</b>\nGrover\u2019s search algorithm: analysis and lower bounds.\n\n<p></p></li>\n<li><b>Quantum Fourier Transform &amp; Quantum Phase Estimation.</b>\nDefinition of the Quantum Fourier Transform (QFT), and efficient representation thereof as a quantum circuit. Application of the QFT to enable Quantum Phase Estimation (QPE).\n\n<p></p></li>\n<li><b>Application 1 of QFT / QPE: Factoring.</b>\n Shor\u2019s algorithm: reduction of factoring to period finding and then using the QFT for period finding.\n\n<p></p></li>\n<li><b>Application 2 of QFT / QPE: Quantum Chemistry.</b>\nEfficient simulation of quantum systems, and applications to real-world problems in quantum chemistry. \n\n<p></p></li>\n<li><b>Quantum complexity.</b>\nQuantum complexity classes and their relationship to classical complexity.  Comparison with probabilistic computation.\n\n<p></p></li>\n<li><b>Quantum error correction.</b>\nIntroducing the concept of quantum error correction required for the following lecture on fault-tolerance.\n\n<p></p></li>\n<li><b>Fault tolerant quantum computing.</b>\nElements of fault tolerant computing; the threshold theorem for efficient suppression of errors. \n\n<p></p></li>\n<li><b>Adiabatic quantum computing.</b>\nThe quantum adiabatic theorem, and adiabatic optimisation. Quantum annealing and D-Wave.\n\n<p></p></li>\n<li><b>Case studies in near-term quantum computation.</b>\nExamples of state-of-the-art quantum algorithms and computers, including superconducting and networked quantum computers.\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should:\n\n\n<li>understand the quantum model of computation and the basic principles of quantum mechanics;\n\n<p></p></li>\n<li>be familiar with basic quantum algorithms and their analysis;\n\n<p></p></li>\n<li>be familiar with basic quantum protocols such as teleportation and superdense coding;\n\n<p></p></li>\n<li>see how the quantum model relates to classical models of deterministic and probabilistic computation.\n\n<p></p></li>\n<li>appreciate the importance of efficient error-suppression if quantum computation is to yield an advantage over classical computation.\n\n<p></p></li>\n<li>gain a general understanding of the important topics in near-term quantum computing, including adiabatic quantum computing.\n\n<p></p></li>\n\nRecommended reading\n<span class=\"textbf\">Books:</span>\nKaye P., Laflamme R., Mosca M. (2007). <em>An Introduction to Quantum Computing</em>. Oxford University Press.\n<br/>Nielsen M.A., Chuang I.L. (2010). <em>Quantum Computation and Quantum Information</em>. Cambridge University Press.\n<br/>Mermin N.D. (2007). <em>Quantum Computer Science: An Introduction</em>. Cambridge University Press.\n<br/>Hirvensalo M. (2001). <em>Quantum Computing</em>. Springer.\n<br/>McGeoch, C. (2014). <em>Adiabatic Quantum Computation and Quantum Annealing Theory and Practice</em>. Morgan &amp; Claypool. <tt><a href=\"https://ieeexplore.ieee.org/document/7055969\" name=\"tex2html21\">https://ieeexplore.ieee.org/document/7055969</a></tt>\n\n<span class=\"textbf\">Papers:</span>\nBraunstein S.L. (2003). <em>Quantum computation tutorial</em>. Available at: <tt><a href=\"https://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf\" name=\"tex2html22\">https://www-users.cs.york.ac.uk/~schmuel/comp/comp_best.pdf</a></tt>\n<br/>Aharonov D., Quantum computation [arXiv:quant-ph/9812037]\n<br/>Steane A., Quantum computing [arXiv:quant-ph/9708022]\n<br/>Albash T., Adiabatic Quantum Computing <tt><a href=\"https://arxiv.org/pdf/1611.04471.pdf\" name=\"tex2html23\">https://arxiv.org/pdf/1611.04471.pdf</a></tt>\n<br/>McCardle S. <span class=\"textit\">et al</span>, Quantum computational chemistry <tt><a href=\"https://arxiv.org/abs/1808.10402\" name=\"tex2html24\">https://arxiv.org/abs/1808.10402</a></tt>\n\n<span class=\"textbf\">Other lecture notes:</span>\nUmesh Vazirani (UC Berkeley):\n<tt><a href=\"http://www-inst.eecs.berkeley.edu/~cs191/sp12/\" name=\"tex2html25\">http://www-inst.eecs.berkeley.edu/~cs191/sp12/</a></tt>\n<br/>John Preskill (Caltech):\n<tt><a href=\"http://www.theory.caltech.edu/people/preskill/ph229/\" name=\"tex2html26\">http://www.theory.caltech.edu/people/preskill/ph229/</a></tt>\n<br/>Andrew Childs (University of Maryland):\n<tt><a href=\"http://cs.umd.edu/~amchilds/qa/\" name=\"tex2html27\">http://cs.umd.edu/~amchilds/qa/</a></tt>\n<br/>John Watrous (University of Waterloo):\n<tt><a href=\"https://cs.uwaterloo.ca/~watrous/TQI/\" name=\"tex2html28\">https://cs.uwaterloo.ca/~watrous/TQI/</a></tt>\n\n", "course_name": "Quantum Computing", "course_code": "QuantComp", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/QuantComp", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": true, "easter": false}, "AdvAlgo": {"supervisions": 3, "lectures": 12, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-AdvancedAlgorithms.html", "description": "\n\n\nAims\nLectures\n\n\n<li><b>Sorting Networks.</b> Zero-one principle. Merging Network, Bitonic\nSorter. Counting Networks. [CLRS2, Chapter 27]\n\n<p>\n</p></li>\n<li><b>Linear Programming.</b> Definitions and Applications. Formulating\nLinear Programs. The Simplex Algorithm. Finding Initial Solutions.\n[CLRS3, Chapter 29]\n\n<p>\n</p></li>\n<li><b>Approximation Algorithms.</b> (Fully) Polynomial-Time Approximation\nSchemes. Design Techniques. Applications: Vertex Cover, Subset-Sum, Parallel\nMachine Scheduling, Travelling Salesman Problem (including a practical\ndemonstration how to solve a TSP instance exactly using linear programming),\nHardness of Approximation. [CLRS3, Chapter 35]\n\n<p>\n</p></li>\n<li><b>Randomised Approximation Algorithms.</b> Randomised Approximation\nSchemes. Linearity of Expectations and Randomised Rounding of Linear Programs.\nApplications: MAX3-SAT problem, Weighted Vertex Cover, Weighted Set Cover.\nSummary: MAX-SAT problem and discussion of various approximation algorithms. \n[CLRS3, Chapter 35]. \n\n<p>\n</p></li>\n\nObjectives\n\nAt the end of the course students should\n\n\n\n<li>have an understanding of algorithm design for\nparallel computers;\n\n<p>\n</p></li>\n<li>be able to formulate, analyse and solve linear programs;\n\n<p>\n</p></li>\n<li>have learned a variety of tools to design efficient (approximation) algorithms.\n\n<p>\n</p></li>\n\nRecommended reading\n\n* Cormen, T.H., Leiserson, C.D., Rivest, R.L. &amp; Stein,\nC. (2009). <em>Introduction to Algorithms</em>. MIT Press (3rd ed.). ISBN\n978-0-262-53305-8\n<br/>\n\n\n", "course_name": "Advanced Algorithms", "course_code": "AdvAlgo", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/AdvAlgo", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": false, "easter": true}, "BusSeminrs": {"supervisions": null, "lectures": 8, "prerequisite_for": [], "past_exam_questions": null, "description": "\n\n\nAims\nLectures\nEight lectures by eight different entrepreneurs.\n\nObjectives\nAt the end of the course students should have a better knowledge of the\npleasures and pitfalls of starting a high tech company.\n\nRecommended reading\nLang, J. (2001). <em>The high-tech entrepreneur\u2019s handbook: how to start and run a high-tech company</em>. FT.COM/Prentice Hall.\n<br/>Maurya, A. (2012). <em>Running Lean: Iterate from Plan A to a Plan That Works</em>. O\u2019Reilly.\n<br/>Osterwalder, A. &amp; Pigneur, Y. (2010). <em>Business Model Generation: A Handbook for Visionaires, Game Changers, and Challengers</em>. Wiley.\n<br/>Kim, W. &amp; Mauborgne, R. (2005). <em>Blue Ocean Strategy</em>.  Harvard Business School Press.\n\nSee also the additional reading list on the Business Studies web page.\n\n\n", "course_name": "Business Studies Seminars", "course_code": "BusSeminrs", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/BusSeminrs", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": false, "easter": true}, "HLog+ModC": {"supervisions": 3, "lectures": 12, "prerequisite_for": [], "past_exam_questions": "https://www.cl.cam.ac.uk/teaching/exams/pastpapers/t-HoareLogicandModelChecking.html", "description": "\n\n\nAims\nThe first aim is to introduce Hoare logic for a simple imperative language\nand then to show how it can be used to formally specify programs\n(along with discussion of soundness and completeness),\nand also how to use it in a mechanised program verifier.\n\nThe second aim is to introduce model checking: to show how temporal\nmodels can be used to represent systems, how temporal logic can\ndescribe the behaviour of systems, and finally to introduce\nmodel-checking algorithms to determine whether properties hold, and to\nfind counter-examples. \n\nCurrent research trends also will be outlined.\n\nLectures\n<li><b>Part 1: Hoare logic.</b>  \nFormal versus informal methods. Specification using preconditions and\npostconditions.\n\n<p></p></li>\n<li><b>Axioms and rules of inference.</b>  \nHoare logic for a simple language with assignments, sequences,\nconditionals and while-loops.  Syntax-directedness.\n\n<p></p></li>\n<li><b>Loops and invariants.</b>  \nVarious examples illustrating loop invariants and how they can be\nfound.\n\n<p></p></li>\n<li><b>Partial and total correctness.</b>  \nHoare logic for proving termination. Variants.\n\n<p></p></li>\n<li><b>Semantics and metatheory</b>\nMathematical interpretation of Hoare logic.\nSemantics and soundness of Hoare logic.\n\n<p></p></li>\n<li><b>Separation logic</b>\nSeparation logic as a resource-aware reinterpretation of Hoare logic\nto deal with aliasing in programs with pointers. \n\n<p></p></li>\n<li><b>Part 2: Model checking.</b>\nModels. Representation of state spaces. Reachable states. \n\n<p></p></li>\n<li><b>Temporal logic</b>.\nLinear and branching time. Temporal operators. Path quantifiers.\nCTL, LTL, and CTL*.  \n\n<p></p></li>\n<li><b>Model checking.</b>\nSimple algorithms for verifying that temporal properties hold.\n\n<p></p></li>\n<li><b>Applications and more recent developments</b>\nSimple software and hardware examples.\nCEGAR (counter-example guided abstraction refinement).\n\n<p></p></li>\n\nObjectives\nAt the end of the course students should\n\n\n<li>be able to prove simple programs correct by hand and implement a\n  simple program verifier;\n\n<p></p></li>\n<li>be familiar with the theory and use of separation logic;\n\n<p></p></li>\n<li>be able to write simple models and specify them using temporal logic;\n\n<p></p></li>\n<li>be familiar with the core ideas of model checking, and be able to\n  implement a simple model checker.\n\n<p></p></li>\n\nRecommended reading\nHuth, M. &amp; Ryan M. (2004). <em>Logic in Computer Science: Modelling and Reasoning about Systems</em>. Cambridge University Press (2nd ed.).\n\n\n\n", "course_name": "Hoare Logic and Model Checking", "course_code": "HLog+ModC", "course_url": "https://www.cl.cam.ac.uk/teaching/1920/HLog+ModC", "lecturers": [], "year": "1920", "tripos_part": "2", "michaelmas": false, "lent": false, "easter": true}}