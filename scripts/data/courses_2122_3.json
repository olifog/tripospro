{"L352": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Advanced Graphics covers topics related to processing,\n  perception and display of images. The focus of the course is on\n  the algorithms behind new emerging display technologies, such as\n  virtual reality, augmented reality, and high dynamic range\n  displays. It complements two computer graphics courses,\n  Introduction to Graphics and Further Graphics, by introducing\n  problems that became the part of graphics pipeline: tone-mapping,\n  post-processing, displays and models of visual perception.</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>GP-GPU</strong>: scheduling and thread mapping,\n    reductions.</li>\n<li><strong>Advanced image processing</strong>: edge-stopping\n    filters, pyramids, optimization-based image processing.</li>\n<li><strong>Beyond 2D</strong>: stereo rendering and light\n    fields.</li>\n<li><strong>Models of visual perception</strong>: visual\n    system, brightness perception, detection and discrimination,\n    contrast sensitivity function, contrast constancy, perceptually\n    uniform spaces, depth perception.</li>\n<li><strong>High Dynamic Range and tone mapping</strong>:\n    dynamic range, display model, methods of tone-mapping.</li>\n<li><strong>Display technologies</strong>: 2D displays, 3D\n    displays, temporal display characteristic, HDR displays.</li>\n<li><strong>Virtual and Augmented Reality</strong>: display\n    technologies, VR rendering, orientation tracking, pose\n    tracking, perceptual considerations, panoramic imaging.</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should be able to:</p>\n<ul>\n<li>implement real-time image processing methods on a GPU\n    (OpenCL);</li>\n<li>design and implement a tone-mapping algorithm;</li>\n<li>describe the limitations of display technologies (dynamic\n    range, brightness, visual comfort, VR simulation sickness) and\n    how they can be addressed using computational methods\n    (tone-mapping, HDR displays);</li>\n<li>describe the limitations of the visual system and how those\n    limitation can be exploited in computer graphics and image\n    processing.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Two practical exercises, worth 50% of the marks.</li>\n<li>One test, worth 50% of the marks</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Hainich, R. and Bimber, O. (2016) <em>Displays: Fundamentals\n  and Applications</em>. CRC Press (2nd ed.).<br/>\n  Boreskov, A. and Shikin, E. (2013) <em>Computer Graphics: From\n  Pixels to Programmable Graphics Hardware</em>. CRC Press.<br/>\n  Reinhard, E., et. al. (2010) <em>High Dynamic Range Imaging:\n  Acquisition, Display, and Image-Based Lighting</em>. Morgan\n  Kaufmann (2nd ed.).</p>\n", "course_name": "Advanced Graphics and Image Processing", "course_code": "L352", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L352", "lecturers": ["rkm38"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R265": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims to provide students with an introduction to a\n  range of advanced topics in computer architecture. It will\n  explore the current and future challenges facing the architects\n  of modern computers. These will also be used to illustrate the\n  many different influences and trade-offs involved in computer\n  architecture.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should:</p>\n<ul>\n<li>understand the challenges of designing and verifying modern\n    microprocessors</li>\n<li>be familiar with recent research themes and emerging\n    challenges</li>\n<li>appreciate the complex trade-offs at the heart of computer\n    architecture</li>\n</ul>\n<h2>Syllabus</h2>\n<p>Each seminar will focus on a different topic. The proposed\n  topics are listed below but there may be some minor changes to\n  this:</p>\n<ul>\n<li>Trends in computer architecture</li>\n<li>State-of-the-art microprocessor design</li>\n<li>Memory system design</li>\n<li>Hardware reliability</li>\n<li>Specification, verification and test</li>\n<li>Hardware security (2)</li>\n<li>HW accelerators and accelerators for machine learning</li>\n</ul>\n<p>Each two hour seminar will include three student presentations\n  (15mins) questions (5mins) and a broader discussion of the topics\n  (around 30mins). The last part of the seminar will include a\n  short scene setting lecture (around 20mins) to introduce the\n  following week's topic.</p>\n<h2>Assessment</h2>\n<p>Each week students will compare and contrast two of the main\n  papers and submit a written summary and review in advance of each\n  seminar (except when presenting).</p>\n<p>Students will be expected to give a number of 15 minute\n  presentations.</p>\n<p>Essays and presentations will be marked out of 10. After\n  dropping the lowest mark, the remaining marks will be scaled to\n  give a final score out of 100.</p>\n<p>Students will give at least one presentation during the\n  course. They will not be required to submit an essay during the\n  weeks they are presenting.</p>\n<p>Each presentation will focus on a single paper from the\n  reading list. Marks will be awarded for clarity and the\n  communication of the paper's key ideas, an analysis of the work's\n  strengths and weaknesses and the work\u2019s relationship to related\n  work and broader trends and constraints.</p>\n<h2>Recommended prerequisite reading</h2>\n<p>Patterson, D. A., Hennessy, J. L. (2017). <em>Computer\n  organization and design: The Hardware/software interface RISC-V\n  edition</em> Morgan Kaufmann. ISBN 978-0-12-812275-4.</p>\n<p>Hennessy, J. and Patterson, D. (2012). <em>Computer\n  architecture: a quantitative approach</em>. Elsevier (5th ed.)\n  ISBN 9780123838728. (the 3rd and 4th editions are also\n  relevant)</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Advanced Topics in Computer Architecture", "course_code": "R265", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R265", "lecturers": ["rdm34", "swm11", "tmj32"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L18": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide an introduction to how reasoning\n  can be automated. In particular, the course will introduce\n  students to fundamental techniques for designing automated\n  reasoners, provide some experience of how they work and how to\n  use them, and present advanced applications of theorem proving\n  for solving problems via automated reasoning.</p>\n<h2>Syllabus</h2>\n<p>Topics <strong><em>may</em></strong> include:</p>\n<ul>\n<li>Introduction to automated reasoning and history of\n    automated theorem provers.</li>\n<li>Brief review of mathematical logic: representations,\n    propositional, predicate logic, semantics.</li>\n<li>Representing mathematical knowledge using logic.</li>\n<li>Proof and correctness: formalization of proof, inference\n    rules and resolution, unification, equational reasoning,\n    combinatorial explosion, search algorithms.</li>\n<li>Guidance techniques: rewrite rules, human proofs, decision\n    procedures, meta-level inference.</li>\n<li>Inductive theorem proving, heuristic guidance, rippling,\n    proof planning.</li>\n<li>Applied uses of automated reasoning: diagrammatic\n    reasoning, ontology/semantic web.</li>\n<li>Student presentations of reviews/rational reconstructions\n    of topics in automated reasoning.</li>\n</ul>\n<p>Note that some content may vary, and the number of lectures\n  per topic is provisional; the final plan will depend on the\n  students' background and the number of students taking the\n  course.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be able to represent mathematical and other knowledge using\n    logical formalism;</li>\n<li>understand the history of formalizing mathematical\n    knowledge;</li>\n<li>know and understand the advantages and limitations of the\n    main approaches and techniques in automated reasoning of\n    mathematical knowledge;</li>\n<li>be able to apply different automated reasoning techniques\n    to new problems;</li>\n<li>be able to locate, read, understand, and present a research\n    paper from the field;</li>\n<li>be familiar with current research in a number of aspects of\n    the field;</li>\n<li>be able to critically analyze and evaluate a piece of\n    research.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Coursework may consist of two practical exercises.</p>\n<p>First, students will be able to select one of a list of topics\n  in automated reasoning and carry out a literature survey of\n  state-of-the-art research on this topic.</p>\n<p>Second, students will be able to select a research paper from\n  a list of papers describing one of state-of-the-art reasoning\n  systems, do a review (an in-depth analysis) of the work described\n  in the paper, and give a short presentation about it to the rest\n  of the class.</p>\n<h2>Practical work</h2>\n<p>Exercises will be given to students during lectures that will\n  enable students to practice and apply principles discussed in the\n  lectures. Solutions to exercises should be submitted, but will\n  not be assessed. Feedback will be provided as appropriate.</p>\n<h2>Assessment</h2>\n<p>The assessment will be done by the lecturer. Full details will\n  follow in September preceding the start of Michaelmas term:</p>\n<ul>\n<li>A literature survey 35%;</li>\n<li>A presentation of an in-depth analysis of a selected\n    research paper and reading group participation 15%; and</li>\n<li>A written test of up to 90 minutes in duration 50%<br/>\n<em>(Note: If students are unable to be in Cambridge due to\n    COVID-19 restrictions, the test may take place online or be\n    replaced with another form of assessment)</em></li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Bundy, A. (1983). <em>The computer modelling of mathematical\n  reasoning</em>. London: Academic Press (2nd ed.). Out of print,\n  but available on-line from: <a href=\"http://www.inf.ed.ac.uk/teaching/courses/ar/book/book-postcript/\">\n  http://www.inf.ed.ac.uk/teaching/courses/ar/book/book-postcript/</a></p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Automated Reasoning", "course_code": "L18", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L18", "lecturers": ["mj201"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L108": {"supervisions": 0, "prerequisite_for": ["L118"], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Category theory provides a unified treatment of mathematical\n  properties and constructions that can be expressed in terms of\n  'morphisms' between structures. It gives a precise framework for\n  comparing one branch of mathematics (organized as a category)\n  with another and for the transfer of problems in one area to\n  another. Since its origins in the 1940s motivated by connections\n  between algebra and geometry, category theory has been applied to\n  diverse fields, including computer science, logic and\n  linguistics. This course introduces the basic notions of category\n  theory: adjunction, natural transformation, functor and category.\n  We will use category theory to organize and develop the kinds of\n  structure that arise in models and semantics for logics and\n  programming languages.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction; some history. Definition of category. The\n    category of sets and functions.</li>\n<li>Commutative diagrams. Examples of categories: preorders and\n    monotone functions; monoids and monoid homomorphisms; a\n    preorder as a category; a monoid as a category. Definition of\n    isomorphism. Informal notion of a 'category-theoretic'\n    property.</li>\n<li>Terminal objects. The opposite of a category and the\n    duality principle. Initial objects. Free monoids as initial\n    objects.</li>\n<li>Binary products and coproducts. Cartesian categories.</li>\n<li>Exponential objects: in the category of sets and in\n    general. Cartesian closed categories: definition and\n    examples.</li>\n<li>Intuitionistic Propositional Logic (IPL) in Natural\n    Deduction style. Semantics of IPL in a cartesian closed\n    preorder.</li>\n<li>Simply Typed Lambda Calculus (STLC). The typing relation.\n    Semantics of STLC types and terms in a cartesian closed\n    category (ccc). The internal language of a ccc. The\n    Curry-Howard-Lawvere correspondence.</li>\n<li>Functors. Contravariance. Identity and composition for\n    functors.</li>\n<li>Size: small categories and locally small categories. The\n    category of small categories. Finite products of\n    categories.</li>\n<li>Natural transformations. Functor categories. The category\n    of small categories is cartesian closed.</li>\n<li>Hom functors. Natural isomorphisms. Adjunctions. Examples\n    of adjoint functors. Theorem characterising the existence of\n    right (respectively left) adjoints in terms of a universal\n    property.</li>\n<li>Dependent types. Dependent product sets and dependent\n    function sets as adjoint functors. Equivalence of categories.\n    Example: the category of I-indexed sets and functions is\n    equivalent to the slice category Set/I.</li>\n<li>Presheaves. The Yoneda Lemma. Categories of presheaves are\n    cartesian closed.</li>\n<li>Monads. Modelling notions of computation as monads. Moggi's\n    computational lambda calculus.</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with some of the basic notions of category\n    theory and its connections with logic and programming language\n    semantics</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>a graded exercise sheet (25% of the final mark), and</li>\n<li>a take-home test (75%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Awodey, S. (2010). <em>Category theory</em>. Oxford University\n  Press (2nd ed.).</p>\n<p>Crole, R. L. (1994). <em>Categories for types</em>. Cambridge\n  University Press.</p>\n<p>Lambek, J. and Scott, P. J. (1986). <em>Introduction to higher\n  order categorical logic</em>. Cambridge University Press.</p>\n<p>Pitts, A. M. (2000). <em>Categorical Logic</em>. Chapter 2 of\n  S. Abramsky, D. M. Gabbay and T. S. E. Maibaum (Eds) Handbook of\n  Logic in Computer Science, Volume 5. Oxford University Press.\n  (Draft copy available <a href=\"http://www.cl.cam.ac.uk/~amp12/papers/catl/catl.pdf\">here</a>.)</p>\n<h2>Class Size</h2>\n<p>This module can accommodate upto 15 Part II students plus 15\n  MPhil / Part III students.</p>\n", "course_name": "Category Theory", "course_code": "L108", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L108", "lecturers": ["amp12"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L314": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course teaches the basic signal-processing principles\n  necessary to understand many modern high-tech systems, with\n  application examples focussing on audio processing, image coding,\n  communication systems and software-defined radio. Students will\n  gain practical experience from numerical experiments in\n  programming assignments (in MATLAB, NumPy or Julia).</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Signals and systems.</strong>\u00a0Discrete\n    sequences and systems: types and properties. Amplitude, phase,\n    frequency, modulation, decibels, root-mean square. Linear\n    time-invariant systems, convolution. Some examples from\n    electronics, optics and acoustics.</li>\n<li><strong>Phasors.</strong>\u00a0Eigen functions of linear\n    time-invariant systems. Review of complex arithmetic. Phasors\n    as orthogonal base functions.</li>\n<li><strong>Fourier transform.</strong>\u00a0Forms and\n    properties of the Fourier transform. Convolution theorem. Rect\n    and sinc.</li>\n<li><strong>Dirac\u2019s delta function.</strong>\u00a0Fourier\n    representation of sine waves, impulse combs in the time and\n    frequency domain. Amplitude-modulation in the frequency\n    domain.</li>\n<li><strong>Discrete sequences and\n    spectra.</strong>\u00a0Sampling of continuous signals, periodic\n    signals, aliasing, interpolation, sampling and reconstruction,\n    sample-rate conversion, oversampling, spectral inversion.</li>\n<li><strong>Discrete Fourier\n    transform.</strong>\u00a0Continuous\u00a0<em>versus</em>\u00a0discrete\n    Fourier transform, symmetry, linearity, FFT, real-valued FFT,\n    FFT-based convolution, zero padding, FFT-based resampling,\n    deconvolution exercise.</li>\n<li><strong>Spectral estimation.</strong>\u00a0Short-time\n    Fourier transform, leakage and scalloping phenomena, windowing,\n    zero padding. Audio and voice examples. DTFM exercise.</li>\n<li><strong>Finite impulse-response\n    filters.</strong>\u00a0Properties of filters, implementation\n    forms, window-based FIR design, use of frequency-inversion to\n    obtain high-pass filters, use of modulation to obtain band-pass\n    filters.</li>\n<li><strong>Infinite impulse-response\n    filters.</strong>\u00a0Sequences as\n    polynomials,\u00a0<em>z</em>-transform, zeros and poles, some\n    analog IIR design techniques (Butterworth, Chebyshev I/II,\n    elliptic filters, second-order cascade form).</li>\n<li><strong>Band-pass signals.</strong>\u00a0Band-pass sampling\n    and reconstruction, IQ up and down conversion, superheterodyne\n    receivers, software-defined radio front-ends, IQ representation\n    of AM and FM signals and their demodulation.</li>\n<li><strong>Digital\n    communication.</strong>\u00a0Pulse-amplitude modulation.\n    Matched-filter detector. Pulse shapes, inter-symbol\n    interference, equalization. IQ representation of ASK, BSK, PSK,\n    QAM and FSK signals.\u00a0[2 hours]</li>\n<li><strong>Random sequences and noise.</strong>\u00a0Random\n    variables, stationary and ergodic processes, autocorrelation,\n    cross-correlation, deterministic cross-correlation sequences,\n    filtered random sequences, white noise, periodic\n    averaging.</li>\n<li><strong>Correlation coding.</strong>\u00a0Entropy, delta\n    coding, linear prediction,\n    dependence\u00a0<em>versus</em>\u00a0correlation, random\n    vectors, covariance, decorrelation, matrix diagonalization,\n    eigen decomposition, Karhunen-Lo\u00e8ve transform, principal\n    component analysis. Relation to orthogonal transform coding\n    using fixed basis vectors, such as DCT.</li>\n<li><strong>Lossy versus lossless\n    compression.</strong>\u00a0What information is discarded by\n    human senses and can be eliminated by encoders? Perceptual\n    scales, audio masking, spatial resolution, colour coordinates,\n    some demonstration experiments.</li>\n<li><strong>Quantization, image coding\n    standards.</strong>\u00a0Uniform and logarithmic quantization,\n    A/\u00b5-law coding, dithering, JPEG.</li>\n</ul>\n<h2>Objectives</h2>\n<ul>\n<li>apply basic properties of time-invariant linear\n    systems;</li>\n<li>understand sampling, aliasing, convolution, filtering, the\n    pitfalls of spectral estimation;</li>\n<li>explain the above in time and frequency domain\n    representations;</li>\n<li>use filter-design software;</li>\n<li>visualize and discuss digital filters in\n    the\u00a0<em>z</em>-domain;</li>\n<li>use the FFT for convolution, deconvolution, filtering;</li>\n<li>implement, apply and evaluate simple DSP applications;</li>\n<li>familiarity with a number of signal-processing concepts\n    used in digital communication systems</li>\n</ul>\n<h2>Assessment - Part II students</h2>\n<ul>\n<li>Three homework programming assignments, each comprising 20%\n    of the mark</li>\n<li>Written take-home assignment, comprising 40% of the total\n    mark.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Lyons, R.G. (2010).\u00a0<em>Understanding digital signal\n  processing.</em>\u00a0Prentice Hall (3rd ed.).<br/>\n  Oppenheim, A.V. and Schafer, R.W. (2007).\u00a0<em>Discrete-time\n  digital signal processing.</em>\u00a0Prentice Hall (3rd ed.).<br/>\n  Stein, J. (2000).\u00a0<em>Digital signal processing \u2013 a computer\n  science perspective.</em>\u00a0Wiley.</p>\n", "course_name": "Digital Signal Processing", "course_code": "L314", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L314", "lecturers": ["aoy20"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L312": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This aim of this course is an introduction to computer music,\n  including applications in generative composition, audio\n  interaction, sonification, game sound and other non-speech audio.\n  The basic principles of music information retrieval and\n  musicological corpus analysis will be covered. Finally, the\n  course will conclude with an overview of current research topics\n  as addressed at venues such as NIME, ICLC, ICCM.</p>\n<h2>Lectures</h2>\n<p>Part 1: Digital signal processing (Lecturer: Dr A. Yontem)</p>\n<p>Lectures 1-8 of the DSP course. This course teaches the basic\n  signal-processing principles necessary to understand many modern\n  high-tech systems, with audio, voice and communication examples.\n  Students will gain practical experience from numerical\n  experiments in MATLAB-based programming assignments.</p>\n<p>Part 2: Computer music (Lecturers: Professor A. Blackwell and\n  Dr M.\u00a0Gotham)</p>\n<ul>\n<li><strong>Perception</strong>: pitch (chroma, temperament),\n    timbre, rhythmic entrainment, spatialisation</li>\n<li><strong>Synthesis methods</strong>: sampling, wavetable,\n    FM, granular synthesis, physical modelling</li>\n<li><strong>Machine listening</strong>: contemporary approaches\n    to source separation, beat tracking, pitch estimation,\n    transcription</li>\n<li><strong>Engineering</strong>: Audio processing tools and\n    architectures, incl DAWs, UGens, SuperCollider</li>\n<li><strong>Musicological analysis</strong>: sound objects,\n    pitch and harmony, structure, orchestration, genre and\n    ethnomusicology</li>\n<li><strong>Audio interfaces</strong>: Sonification, audio\n    display and non-speech audio interaction, new interfaces for\n    music interaction</li>\n<li><strong>Composition</strong>: Algorithmic composition,\n    generative music, game soundtracks, and live programming</li>\n<li><strong>Student-led session</strong>: research reviews,\n    performance outlines</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>Understand the application of digital signal processing\n    methods to the production of structured sounds;</li>\n<li>Be able to apply principles of human perception and\n    interaction to simple musical and non-speech audio\n    applications.</li>\n</ul>\n<h2>Assessment</h2>\n<ol>\n<li>A critical review of a self-selected piece of research in\n    Computer Music or Audio Interaction</li>\n<li>A research essay \u2013 proposal, synthesis or original\n    argument</li>\n<li>Public performance / demonstration of original work with\n    jury assessment (additional timetabled session)</li>\n</ol>\n<p>Publication venues for research selection should be a full\n  paper published at a conference such as ICMC, NIME, ISMIR, ICLC\n  or ICAD:</p>\n<ul>\n<li><a href=\"http://www.computermusic.org/\">International\n    Computer Music Association</a></li>\n<li><a href=\"http://www.nime.org/\">New Interfaces for Musical\n    Expression</a></li>\n<li><a href=\"http://ismir.net/conferences.html\">International\n    Society of Music Information Retrieval</a></li>\n<li><a href=\"https://iclc.livecodenetwork.org/\">International\n    Conference on Live Coding</a></li>\n<li><a href=\"https://icad.org/\">International Conference on\n    Auditory Displays</a></li>\n</ul>\n<h2>Recommended Reading</h2>\n<p><span style=\"font-size:11pt\"><span style=\"font-family:Calibri,sans-serif\">Collins, N. (2009). Introduction\n  to Computer Music. WileyCollins, N. (2009). Introduction to\n  Computer Music. Wiley</span></span></p>\n", "course_name": "Digital Signal Processing with Computer Music", "course_code": "L312", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L312", "lecturers": ["afb21", "aoy20"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R47": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This reading group course examines foundations and current\n  research into distributed ledger (blockchain) technologies and\n  their applications. Students will read, review, and present\n  seminal research papers in this area. Once completed, students\n  should be able to integrate blockchain technologies into their\n  own research and gain familiarity with a range of research\n  skills.</p>\n<h2>Lectures</h2>\n<ol>\n<li>Introduction</li>\n<li>Consensus protocols</li>\n<li>Bitcoin and its variants</li>\n<li>Ethereum, smart contracts, and other permissionless\n    DLTs</li>\n<li>Hybrid and permissioned DLTs</li>\n<li>Applications</li>\n</ol>\n<p>The final format of the course will depend on the enrolment.\n  Each week, three or four class participants will critically\n  introduce papers using 15-20 minute conference-style\n  presentations. Each presentation will be followed by about 10\n  minutes of questions, then a guided discussion by all present.\n  Slides will be used for presentation.</p>\n<p>Students will give one or more presentations each term,\n  depending on the number of class participants. Each student will\n  also submit a paper review each week for one of the three papers\n  presented except for the week they will be making their own\n  presentation. Each review will follow a template and be up to\n  1,000 words. As a result, each student will produce 6-7 reviews\n  and at least one presentation, potentially two, depending on the\n  number of participants. All participants are expected to attend\n  and participate in every class; the instructor must be notified\n  of any absences in advance.</p>\n<h2>Learning objectives</h2>\n<p>There are two broad objectives: to acquire familiarity with a\n  body of work in the area of distributed ledgers and to learn some\n  specific research skills:</p>\n<ol>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf\" target=\"_blank\">How to read a paper</a></li>\n<li><a href=\"http://pages.cs.wisc.edu/~markhill/the_task_of_the_referee.pdf\" target=\"_blank\">How to review a paper</a></li>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Evaluating_a_research_paper\" target=\"_blank\">How to analyze a paper\u2019s strengths and\n    weaknesses</a></li>\n<li><a href=\"http://www-net.cs.umass.edu/kurose/talks/top_10_tips_for_writing_a_paper.ppt\" target=\"_blank\">Written</a> and <a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Giving_and_attending_talks\" target=\"_blank\">oral</a> presentation skills</li>\n</ol>\n<h2>Assessment</h2>\n<p>You are expected to read all assigned papers and submit a\n  paper review each week. Each review must either follow the\n  provided review form [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:b:/g/personal/sk818_cam_ac_uk/ESpwszt8Nr5PtSF5b9GGH4EBT0WLI_z-bj-jPnRd5i2YYw?e=xrDeau\" target=\"_blank\">PDF</a>] [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:u:/g/personal/sk818_cam_ac_uk/Ecpv3poyjuVKgbvC8fL48jIBRnAGTdVLBFKvMPf2RJjYrw?e=rnWfGE\" target=\"_blank\">Latex source</a>] or may be a PDF copy of your\n  presentation slide deck for those papers you are asked to\n  present. Each \u201creview\u201d is worth 10% of your total mark, and is\n  marked out of 100 with 60 a passing grade. Marks will be awarded\n  and penalties for late submission applied according to <a href=\"http://www.cl.cam.ac.uk/teaching/exams/acs_assessment.html\" target=\"_blank\">ACS Assessment Guidelines</a>. No extra marks are\n  available for submitting more than one review in any week.</p>\n<ul>\n<li>One paper review per week for 7 weeks (10% each) \u2013 [max\n    1200 words]</li>\n<li>Presentation (10%)</li>\n<li>Summative essay critically exploring one blockchain\n    technology in detail (20%) \u2013 [max 2500 words]</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p>Narayanan, A. , Bonneau, J., Felten, E., Miller, A. and\n  Goldfeder, S. (2016). <em>Bitcoin and Cryptocurrency\n  Technologies: A Comprehensive Introduction</em>. Princeton\n  University Press.<br/>\n  (2016 Draft available here: <a href=\"https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf\" target=\"_blank\">https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf</a>)</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Distributed Ledger Technologies: Foundations and Applications", "course_code": "R47", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R47", "lecturers": ["sk818"], "lectures": null, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L21": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module introduces students to interactive theorem proving\n  using Isabelle. It includes techniques for specifying formal\n  models of software and hardware systems and for deriving\n  properties of these models.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction to higher-order logic and Isabelle/HOL. [2\n    lectures]</li>\n<li>Recursive datatypes and functions: modelling them in logic,\n    reasoning about them. [2 lectures]</li>\n<li>Reasoning in predicate logic and typed set theory. [2\n    lectures]</li>\n<li>Inductive Definitions. [1 lecture]</li>\n<li>Introduction to hardware verification: combinational and\n    sequential circuits, registers, etc. [1 lecture]</li>\n<li>Modelling operational semantics definitions and proving\n    properties. [2 lectures]</li>\n<li>Structured proofs. [2 lectures]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>possess good skills in the use of Isabelle;</li>\n<li>be able to specify inductive definitions and perform proofs\n    by induction;</li>\n<li>be able to define and use abstract models in\n    verification;</li>\n<li>be able to express a variety of specifications in\n    higher-order logic.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Each candidate will undertake two small formalisations, which\n  will serve as the basis for assessment.</p>\n<h2>Practical work</h2>\n<p>Four supervised practical sessions will allow students to\n  develop skills.</p>\n<p><em><strong>NOTE: If these module is run\n  remotely,\u00a0students\u00a0will require\u00a0a fairly high\n  specification laptop or PC\u00a0(minimum 8 GB)</strong></em></p>\n<h2>Assessment</h2>\n<p>Each student must undertake two small verification projects,\n  each consisting of a practical write-up accompanied by an\n  Isabelle theory file. These will be started during the practical\n  sessions but will probably be completed on the student\u2019s own\n  time. The projects will assess the extent to which each candidate\n  has absorbed the syllabus and developed practical skills. The\n  lecturer will set and mark the assessments. The mark will be\n  reported as a percentage.</p>\n<h2>Recommended reading</h2>\n<p>Nipkow, T., Paulson, L.C. and Wenzel, M. (2002). <em>A proof\n  assistant for higher-order logic</em>. Springer LNCS 2283.<br/>\n  Krauss, A. <em>Defining recursive functions in\n  Isabelle/HOL</em>.<br/>\n  Nipkow, T. (). <em>Programming and Proving in\n  Isabelle/HOL</em>.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Interactive Formal Verification", "course_code": "L21", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L21", "lecturers": ["lp15"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L95": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide a brief introduction to\n  linguistics for computer scientists and then goes on to cover\n  some of the core tasks in natural language processing (NLP),\n  focussing on statistical parsing of sentences to yield syntactic\n  and semantic representations. We will look at how to evaluate\n  parsers and see how well state-of-the-art tools perform given\n  current techniques.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Linguistics for NLP - morphology, syntax, semantics,\n    pragmatics (of English) [6 sessions]</li>\n<li>Parsing - grammars, treebanks, representations and\n    evaluation, statistical parse ranking [8 sessions]</li>\n<li>Interpretation - compositional semantics and entailment,\n    pragmatic inference [2 sessions]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the basic properties of human languages and be\n    familiar with descriptive and theoretical frameworks for\n    handling these properties;</li>\n<li>understand the design of tools for NLP tasks such as\n    parsing and be able to apply them to text and evaluate their\n    performance;</li>\n<li>understand some of the basic principles of the\n    representation of linguistic meaning and interpretative\n    inference.</li>\n</ul>\n<h2>Practical work</h2>\n<ul>\n<li>Week 6: Download and apply a PSG-based parser to a\n    designated text. Evaluate the performance of the tools\n    quantitatively and qualitatively.</li>\n<li>Week 8: Download and apply two parsers to a designated\n    text. Evaluate the performance of the tools quantitatively and\n    qualitatively.</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>There will be four ticked, take-home assignments in the\n    first half of the term. Each assignment is worth 5% of the\n    final mark.</li>\n<li>An assessed practical report based on the practicals\n    described above. The practical report will consist of a\n    description of the work done of not more than 5000 words. It\n    will contribute 80% of the final mark.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Jurafsky, D. and Martin, J. (2008). <em>Speech and Language\n  Processing</em>. Prentice-Hall (2nd ed.). (See also 3rd ed.\n  available online.)</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Introduction to Natural Language Syntax and Parsing", "course_code": "L95", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L95", "lecturers": ["ejb1"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L310": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course teaches the foundations of autonomous mobile\n  robots, covering topics such as perception, motion control, and\n  planning. It also teaches algorithmic strategies that enable the\n  coordination of multi-robot systems and robot swarms. The course\n  will feature several practical sessions with hands-on robot\n  programming. The students will undertake mini-projects, which\n  will be formally evaluated through a report and presentation.</p>\n<h2>Lectures</h2>\n<p>Lectures (2 lectures per week):</p>\n<ol>\n<li>Introduction (A. Prorok) -- Oct. 7<br/>\n    a. Why study robotics?<br/>\n    b. The basics of mobile autonomy<br/>\n    c. History of robotics research</li>\n<li>Architectures (A. Prorok) -- Oct. 14<br/>\n    a. Autonomy and sensor-actuator loops<br/>\n    b. Reactive vs deliberative decision-making (and control)<br/>\n    c. Control architectures</li>\n<li>Introduction to kinematics (F. Forni and F. Iida) -- Oct.\n    21<br/>\n    a. Motion models; robots with non-holonomic constraints<br/>\n    b. Kinematics; forward and inverse kinematics<br/>\n    c. Open-loop vs closed-loop control; intro to PID control.</li>\n<li>Introduction to dynamics (F. Iida and F. Forni) -- Oct.\n    28<br/>\n    a. Dynamics models<br/>\n    b. Open-loop and closed-loop control<br/>\n    c. PID control applied to dynamic systems.</li>\n<li>Perception and Localization (R. Harle) -- Nov. 4<br/>\n    a. Sensors and sensor models, odometry<br/>\n    b. Maximum likelihood estimation and sensor fusion<br/>\n    c. Noise and belief representation<br/>\n    d. Bayes rule, Bayes filter, Particle Filter, KF<br/>\n    e. Grid localization and map representations</li>\n<li>Navigation and Planning (A. Prorok) -- Nov. 11<br/>\n    a. Basic concepts<br/>\n    b. Reactive navigation (without a roadmap)<br/>\n    c. Deliberative planning (with a roadmap)<br/>\n    d. Planning in multi-robot systems</li>\n<li>Multi-Robot Systems (A. Prorok) -- Nov.18<br/>\n    a. Introduction to Multi-Robot Systems (MRS)<br/>\n    b. Centralized vs decentralized architectures<br/>\n    c. Collective movement (formations, flocking)<br/>\n    d. Task assignment</li>\n<li>Introduction to Advanced Robotics (A. Prorok) -- Nov.\n    25<br/>\n    a. Introduction to reinforcement learning methods<br/>\n    b. Model-based vs model-free approaches<br/>\n    c. Open robotics problems</li>\n</ol>\n<p>Pre-recorded material is available here:<br/>\n<a href=\"https://www.youtube.com/playlist?list=PLaTKfS3-bDpDyOwrxLcQRGxY9XJw33ANo\">\n  https://www.youtube.com/playlist?list=PLaTKfS3-bDpDyOwrxLcQRGxY9XJw33ANo</a></p>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>understand how to control a mobile robot;</li>\n<li>understand how a robot perceives its environment;</li>\n<li>understand how a robot plans actions (navigation\n    paths);</li>\n<li>know paradigms of coordination in systems of multiple\n    robots;</li>\n<li>know classical multi-robot problems and their solution\n    methods;</li>\n<li>Know how to use ROS (Robot Operating System, <a href=\"http://www.ros.org\" id=\"tex2html21\" name=\"tex2html21\">http://www.ros.org</a>).</li>\n</ul>\n<h2>Assessment - Part II Students:</h2>\n<p>For undergraduate students, the assignments will be 100%\n  coursework and consist of two elements: (1) experimental work\n  using a robot simulator and real robots, and (2) theory /\n  understanding. The exercises will require data collection and\n  analysis. The balance between practice and theory will depend on\n  the exercise topic. Each student will submit a written report.\n  Students will be expected to be able to demonstrate any results\n  reported in their hand-in.<br/>\n  Assessment: Each assignment will compose 45% of the final mark;\n  the remaining 10% of the mark will be determined by the student's\n  performance in a 1-on-1 presentation with either the lecturer or\n  a senior assessor. The mark for each assignment will be\n  determined in part by the score achieved in the written report,\n  and in part by the performance of the student during a\n  questioning session. The lecturers will hold an in-person\n  questioning session with each student to discuss their\n  submissions. Submissions are non-anonymous.</p>\n<p>Assignment 1:\u00a0Covers material from Lectures 1-4<br/>\n  Assignment 2:\u00a0Covers material from Lectures 5-8</p>\n<p>Presentation: One-on-one presentation of assignments.</p>\n<h2>Recommended reading</h2>\n<p>Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D. (2004).\n  <em>Autonomous mobile robots</em>. MIT Press.<br/>\n  Thrun, S., Wolfram B. and Dieter F. (2005). <em>Probabilistic\n  robotics</em>. MIT Press.<br/>\n  Mondada, F. and Mordechai B. (2018) <em>Elements of\n  Robotics</em>. Springer<br/>\n  Siciliano, B. and Khatib, O. (2016) <em>Springer handbook of\n  robotics</em>. Springer.<br/>\n  Mesbahi, M. and Egerstedt, M. (2010) <em>Graph theoretic methods\n  in multiagent networks</em>. Princeton University Press.</p>\n", "course_name": "Introduction to Robotics", "course_code": "L310", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L310", "lecturers": ["asp45", "rkh23"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L50": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Systems research refers to the study of a broad range of\n  behaviours arising from complex system design, including:\n  resource sharing and scheduling; interactions between hardware\n  and software; network topology, protocol and device design and\n  implementation; low-level operating systems; Interconnect,\n  storage and more. This module will:</p>\n<ol>\n<li>Teach performance characteristics and performance\n    measurement methodology and practice through profiling\n    experiments;</li>\n<li>Expose students to real-world systems artefacts evident\n    through different measurement tools;</li>\n<li>Develop scientific writing skills through a series of\n    laboratory reports;</li>\n<li>Provide research skills for characterization and modelling\n    of systems and networks using measurements.</li>\n</ol>\n<h2>Prerequisites</h2>\n<p>It is strongly recommended that students have previously (and\n  successfully) completed an undergraduate networking course -- or\n  have equivalent experience through project or open-source\n  work.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction to performance measurements, performance\n    characteristics [1 lecture]</li>\n<li>Performance measurements tools and techniques [2 lectures +\n    2 lab sessions]</li>\n<li>Reproducible experiments [1 lecture + 1 lab session]</li>\n<li>Common pitfalls in measurements [1 lecture]</li>\n<li>Device and system characterisation [1 lecture + 2 lab\n    sessions]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Describe the objectives of measurements, and what they can\n    achieve;</li>\n<li>Characterise and model a system using measurements;</li>\n<li>Perform reproducible measurements experiments;</li>\n<li>Evaluate the performance of a system using\n    measurements;</li>\n<li>Operate measurements tools and be aware of their\n    limitations;</li>\n<li>Detect anomalies in the network and avoid common\n    measurements pitfalls;</li>\n<li>Write system-style performance evaluations.</li>\n</ul>\n<h2>Practical work</h2>\n<p>Five 2-hour in-classroom labs will ask students to develop and\n  use skills in performance measurements as applied to real-world\n  systems and networking artefacts. Results from these labs (and\n  follow-up work by students outside of the classroom) will by the\n  primary input to lab reports.</p>\n<p>The first three labs will provide an introduction and hands on\n  experience with measurement tools and measurements methodologies,\n  while the last two labs will focus on practical measurements and\n  evaluation of specific platforms. Students may find it useful to\n  work in pairs within the lab, but must prepare lab reports\n  independently. The module lecturer will give a short introductory\n  at the start of each lab, and instructors will be on-hand\n  throughout labs to provide assistance.</p>\n<p>Lab participation is not directly included in the final mark,\n  but lab work is a key input to lab reports that are assessed.\n  Guided lab experiments resulting in practical write ups.</p>\n<p><em><strong>NOTE: <em>This module has a large practical\n  element. Students who cannot be in Cambridge due to COVID-19\n  restrictions, may not be able to take this\n  module</em></strong></em></p>\n<h2>Assessment</h2>\n<p>Each student will write two lab reports. The first lab report\n  will summarise the experiments done in the first three labs\n  (20%). The second will be a lab report (5000 words) summarising\n  the evaluation of a device or a system (80%).</p>\n<h2>Recommended reading</h2>\n<p>The following list provides some background to the course\n  materials, but is not mandatory. A reading list, including\n  research papers, will be provided in the course materials.</p>\n<ul>\n<li>George Varghese. Network algorithmics. Chapman and\n    Hall/CRC, 2010.</li>\n<li>Mark Crovella and Balachander Krishnamurthy. Internet\n    measurement: infrastructure, traffic and applications. John\n    Wiley and Sons, Inc., 2006.</li>\n<li>Brendan Gregg. Systems Performance: Enterprise and the\n    Cloud, Prentice Hall Press, Upper Saddle River, NJ, USA,\n    October 2013.</li>\n<li>Raj Jain, The Art of Computer Systems Performance Analysis:\n    Techniques for Experimental Design, Measurement, Simulation,\n    and Modeling, Wiley - Interscience, New York, NY, USA, April\n    1991.</li>\n</ul>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Introduction to networking and systems measurements", "course_code": "L50", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L50", "lecturers": ["awm22"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R244": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module provides an introduction to large-scale data\n  processing, optimisation, and the impact on computer system's\n  architecture. Large-scale distributed applications with high\n  volume data processing such as training of machine learning will\n  grow ever more in importance. Supporting the design and\n  implementation of robust, secure, and heterogeneous large-scale\n  distributed systems is essential. To deal with distributed\n  systems with a large and complex parameter space, tuning and\n  optimising computer systems is becoming an important and complex\n  task, which also deals with the characteristics of input data and\n  algorithms used in the applications. Algorithm designers are\n  often unaware of the constraints imposed by systems and the best\n  way to consider these when designing algorithms with massive\n  volume of data. On the other hand, computer systems often miss\n  advances in algorithm design that can be used to cut down\n  processing time and scale up systems in terms of the size of the\n  problem they can address. Integrating machine learning approaches\n  (e.g. Bayesian Optimisation, Reinforcement Learning) for system\n  optimisation will be explored in this course.</p>\n<h2>Syllabus</h2>\n<p>This course provides perspectives on large-scale data\n  processing, including data-flow programming, graph data\n  processing, probabilistic programming and computer\n  system\u00a0optimisation, especially using machine learning\n  approaches, thus providing a solid basis to work on\u00a0the next\n  generation of distributed systems.</p>\n<p>The module consists of 8 sessions, with 5 sessions on specific\n  aspects of large-scale data processing\u00a0research. Each\n  session discusses 3-4 papers, led by the assigned students. One\n  session is a hands-on\u00a0tutorial on MapReduce using data flow\n  programming of\u00a0Deep Neural Networks training\u00a0using\n  Google TensorFlow also Bayersian Optimisation basics. The first\n  session advises on how to\u00a0read/review a paper together with\n  a brief introduction on different perspectives in large-scale\n  data<br/>\n  processing and optimisation. The last session is dedicated to the\n  student presentation of opensource\u00a0project studies.</p>\n<ol>\n<li>Introduction to large-scale data processing and\n    optimisation</li>\n<li>Data flow programming: Map/Reduce to TensorFlow</li>\n<li>Large-scale graph data processing: storage, processing\n    model and parallel processing</li>\n<li>Map/Reduce and Deep Neural Network using TensorFlow\n    hands-on tutorial</li>\n<li>Probabilistic Programming</li>\n<li>Many Aspects of Optimisation in\u00a0Computer Systems</li>\n<li>Optimisation of Computer Systems using ML</li>\n<li>Presentation of Open Source Project Study</li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Understand key concepts of scalable data processing\n    approaches in future computer systems.</li>\n<li>Obtain a clear understanding of building distributed\n    systems using data centric programming and large-scale data\n    processing.</li>\n<li>Understand a large and complex parameter space in computer\n    system's optimisation and applicability of Machine Learning\n    approach.</li>\n</ul>\n<h2>Coursework</h2>\n<h3>Reading Club:</h3>\n<ul>\n<li>The preparation for the reading club will involve 1-3\n    papers every week. At each session, around 3-4 papers are\n    selected under the given topic, and the students present their\n    review work.</li>\n<li>Hands-on tutorial session of data flow programming\n    including writing an application of processing streaming in\n    Twitter data and/or Deep Neural Networks using Google\n    TensorFlow using cluster computing.</li>\n</ul>\n<h3>Reports</h3>\n<p>The following three reports are required, which could be\n  extended from the assignment of the reading club, within the\n  scope of data centric systems.</p>\n<ol>\n<li>Review report on a full length paper (max 1800 words)\n      <ul>\n<li>Describe the contribution of the paper in depth with\n        criticisms</li>\n<li>Crystallise the significant novelty in contrast to\n        other related work</li>\n<li>Suggestions for future work</li>\n</ul>\n</li>\n<li>Survey report on sub-topic in large-scale data processing\n    and optimisation (max 2000 words)\n      <ul>\n<li>Pick up to 5 papers as core papers in the survey\n        scope</li>\n<li>Read the above and expand reading through related\n        work</li>\n<li>Comprehend the view and finish an own survey paper</li>\n</ul>\n</li>\n<li>Project study and exploration of a prototype (max 2500\n    words)\n      <ul>\n<li>What is the significance of the project in the research\n        domain?</li>\n<li>Compare with similar and succeeding projects</li>\n<li>Demonstrate the project by exploring its prototype</li>\n</ul>\n</li>\n</ol>\n<p>Reports 1 and 2 should be handed in by the end of 5th week and\n  7th week of the course. Report 3 should be handed in by the end\n  of the Michaelmas Term.</p>\n<h2>Assessment</h2>\n<p>The final grade for the course will be provided as a\n  percentage, and the assessment will consist of two parts:</p>\n<ol>\n<li>25%: for reading club (participation, presentation)</li>\n<li>75%: for the three reports:\n      <ul>\n<li>15%: Intensive review report</li>\n<li>25%: Survey report</li>\n<li>35%: Project study</li>\n</ul>\n</li>\n</ol>\n<h2>Recommended reading</h2>\n<ol>\n<li>M. Abadi et al. TensorFlow: A System for Large-Scale\n    Machine Learning, OSDI, 2016.</li>\n<li>D. Aken et al.: Automatic Database Management System Tuning\n    Through Large-scale Machine Learning, SIGMOD, 2017.</li>\n<li>J. Ansel et al. Opentuner: an extensible framework for\n    program autotuning. PACT, 2014.</li>\n<li>V. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building\n    Auto-Tuners with Structured Bayesian Optimization, WWW,\n    2017.</li>\n<li>J. Dean et al. Large scale distributed deep networks. NIPS,\n    2012.</li>\n<li>G. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N.\n    Leiser, G. Czajkowski. Pregel: A System for Large-Scale Graph\n    Processing, SIGMOD, 2010.</li>\n<li>A. Mirhoseini et al. Device Placement Optimization with\n    Reinforcement Learning, ICML, 2017.</li>\n<li>D. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M.\n    Abadi. Naiad: A Timely Dataflow System, SOSP, 2013.</li>\n<li>M. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki:\n    RLgraph: Modular Computation Graphs for Deep Reinforcement\n    Learning, SysML, 2019.</li>\n<li>Z. Jia, O. Padon, J. Thomas, T. Warszawski, M.\n    Zaharia,\u00a0 A. Aiken: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/jia_SOSP_2019.pdf\" style=\"color:#0563c1; text-decoration:underline\" target=\"_blank\">TASO: Optimizing Deep Learning Computation with\n    Automated Generation of Graph Substitutions</a>: SOSP,\n    2019.</li>\n<li>H. Mao et al.: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/mao_OR_2019.pdf\" style=\"color:#0563c1; text-decoration:underline\" target=\"_blank\">Park: An Open Platform for Learning-Augmented Computer\n    Systems</a>, OpenReview, 2019.</li>\n</ol>\n<p>A complete list can be found on the course material web page.\n  See also 2019-2020 course material on the previous course\n  <a href=\"https://www.cl.cam.ac.uk/teaching/1920/R244/materials.html\" target=\"_blank\">Large-Scale Data Processing and\n  Optimisation</a>.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Large-scale data processing and optimisation", "course_code": "R244", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R244", "lecturers": ["ey204"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L48": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The module \u201cMachine Learning and the Physical World\u201d is\n  focused on machine learning systems that interact directly with\n  the real world. Building artificial systems that interact with\n  the physical world have significantly different challenges\n  compared to the purely digital domain. In the real world data is\n  scares, often uncertain and decisions can have costly and\n  irreversible consequences. However, we also have the benefit of\n  centuries of scientific knowledge that we can draw from. This\n  module will provide the methodological background to machine\n  learning applied in this scenario. We will study how we can build\n  models with a principled treatment of uncertainty, allowing us to\n  leverage prior knowledge and provide decisions that can be\n  interrogated.</p>\n<p>There are three principle points about machine learning in the\n  real world that will concern us.</p>\n<ol>\n<li>We often have a mechanistic understanding of the real world\n    which we should be able to bootstrap to make decisions. For\n    example, equations from physics or an understanding of\n    economics.</li>\n<li>Real world decisions have consequences which may have\n    costs, and often these cost functions need to be assimilated\n    into our machine learning system.</li>\n<li>The real world is surprising, it does things that you do\n    not expect and accounting for these challenges requires us to\n    build more robust and or interpretable systems.</li>\n</ol>\n<p>Decision making in the real world hasn\u2019t begun only with the\n  advent of machine learning technologies. There are other domains\n  which take these areas seriously, physics, environmental\n  scientists, econometricians, statisticians, operational\n  researchers. This course identifies how machine learning can\n  contribute and become a tool within these fields. It will equip\n  you with an understanding of methodologies based on uncertainty\n  and decision making functions for delivering on these\n  challenges.</p>\n<h2>Objectives</h2>\n<p>You will gain detailed knowledge of</p>\n<ul>\n<li>surrogate models and uncertainty</li>\n<li>surrogate-based optimization</li>\n<li>sensitivity analysis</li>\n<li>experimental design</li>\n</ul>\n<p>You will gain knowledge of</p>\n<ul>\n<li>counterfactual analysis</li>\n<li>surrogate-based quadrature</li>\n</ul>\n<h2>Schedule</h2>\n<p>Week 1:\u00a0Introduction to the unit and foundation of\n  probabilistic modelling</p>\n<p>Week 2:\u00a0Gaussian processes and probablistic inference</p>\n<p>Week 3:\u00a0Simulation and Sequential decision making under\n  uncertainty</p>\n<p>Week 4:\u00a0Emulation and Experimental Design</p>\n<p>Week 5:\u00a0Sensitivity Analysis and Multifidelity\n  Modelling</p>\n<p>Week 6-8:\u00a0Case studies of applications and Projects</p>\n<h2>Practical work</h2>\n<p>During the first five weeks of the unit we will provide a\n  weekly worksheet that will focus on implementation and practical\n  exploration of the material covered in the lectures. The\n  worksheets will allow you to build up a these methods without\n  relying on extensive external libraries. You are free to use any\n  programming language of choice however we highly recommended the\n  use of =Python=.</p>\n<h2>Assessment</h2>\n<p>This unit will be assessed using a group project. Each group\n  will work on an application of uncertainty that covers the\n  material of the first 5 weeks of lectures in the unit. Each group\n  will submit a report which will form the basis of the assessment.\n  In addition to the report each group will also attend a short\n  oral examination based on the material covered both in the report\n  and the taught material.</p>\n<h2>Recommended Reading</h2>\n<p>Rasmussen, C. E. and Williams, C. K. I. (2006). <em>Gaussian\n  Processes for Machine Learning</em>. MIT Press</p>\n<p>Bishop, C. (2006). <em>Pattern recognition and machine\n  learning</em>. Springer.<br/>\n<a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\" target=\"_blank\">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>\n<p>Laplace, P. S. (1902). <em>A Philosophical Essay on\n  Probabilities</em>. John Wiley &amp; Sons.<br/>\n<a href=\"https://archive.org/details/philosophicaless00lapliala\" target=\"_blank\">https://archive.org/details/philosophicaless00lapliala</a></p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Machine Learning and the Physical World", "course_code": "L48", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L48", "lecturers": ["ndl21"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L101": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide an introduction to machine\n  learning with specific application to tasks such as document\n  classification, spam email filtering, language modelling,\n  part-of-speech tagging, and named entity and event recognition\n  for textual information extraction. We will cover supervised,\n  weakly-supervised and unsupervised approaches using generative\n  and discriminative linear and non-linear classifiers, such as\n  Naive Bayes, Perceptron, Multi-Layer Perceptron, Logistic\n  Regression, clustering / dimensionality-reduction methods, such\n  as latent Dirichlet allocation and neural word embeddings.</p>\n<h2>Syllabus</h2>\n<p>Classification by machine learning: classification, types of\n  classifier, generative vs. discriminative models,\n  (un-/semi-)supervised training.</p>\n<p>Document Classification: by topic, sentiment, spam content,\n  etc, bag-of-words, word embeddings, feature selection /\n  induction.</p>\n<p>Structured prediction: sequence tagging, graph parsing,\n  incremental language generation with recurrent neural\n  networks.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the issues involved in applying machine learning\n    approaches to a range of language processing applications;</li>\n<li>understand the theory underlying a number of machine\n    learning approaches that have been applied to language\n    processing, including: Naive Bayes, Perceptron, Logistic\n    Regression, and Multi-Layer Perceptron;</li>\n<li>understand some applications and specific tasks including:\n    document topic classification and clustering, SPAM filtering,\n    PoS tagging, named entity recognition, event extraction,\n    language modelling and word embeddings.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Students will be expected to undertake reading for assigned\n  lectures and seminars. Each student will give a 20 minute\n  presentation of one paper.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Students will receive one tick worth 5% for attendance at\n    seminar sessions, reading of assigned material, and\n    satisfactory contribution during seminars.</li>\n<li>Students will receive a second tick worth 5% for a\n    satisfactory presentation of an assigned paper.</li>\n<li>students will undertake a small project to be agreed with\n    the lecturers and write a project report of not more than 5000\n    words. The report will be due around the beginning of the Lent\n    Term (see academic calendar for precise date), will be assessed\n    by the lecturers, and will account for 90% of the module\n    marks.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Bishop, C. (2006). <em>Pattern recognition and machine\n  learning</em>. Springer. (Chaps: 1, 2, 4-9, 13).</p>\n<p>Jurafsky, D. &amp; Martin, J. (2008). <em>Speech and language\n  processing</em>. Prentice Hall (2nd ed.). (Chaps: 4-6, 22)(see\n  also 3rd ed. draft, online).</p>\n<p>Manning, C., Raghavan, P. &amp; Schutze, H. (2008).\n  <em>Introduction to information retrieval</em>. Cambridge\n  University Press. (Chaps: 12-18).</p>\n<p>Goodfellow et al, DL (Chaps 6-12).</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Machine Learning for Language Processing", "course_code": "L101", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L101", "lecturers": ["av308", "ejb1"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L335": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims at introducing the theoretical foundations\n  and practical techniques for machine perception, the capability\n  of computers to interpret data resulting from sensor\n  measurements. The course will teach the fundamentals and modern\n  techniques of machine perception, i.e. reconstructing the real\n  world starting from sensor measurements with a focus on machine\n  perception for visual data. The topics covered will be\n  image/geometry representations for machine perception, semantic\n  segmentation, object detection and recognition, geometry capture,\n  appearance modeling and acquisition, motion detection and\n  estimation, human-in-the-loop machine perception, select topics\n  in applied machine perception.</p>\n<p>Machine perception/computer vision is a rapidly expanding area\n  of research with real-world applications. An understanding of\n  machine perception is also important for robotics, interactive\n  graphics (especially AR/VR), applied machine learning, and\n  several other fields and industries. This course will provide a\n  fundamental understanding of and practical experience with the\n  relevant techniques.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Students will understand the theoretical underpinnings of\n    the modern machine perception techniques for reconstructing\n    models of reality starting from an incomplete and imperfect\n    view of reality.</li>\n<li>Students will be able to apply machine perception theory to\n    solve practical problems, e.g. classification of images,\n    geometry capture.</li>\n<li>Students will gain an understanding of which machine\n    perception techniques are appropriate for different tasks and\n    scenarios.</li>\n<li>Students will have hands-on experience with some of these\n    techniques via developing a functional machine perception\n    system in their projects.</li>\n<li>Students will have practical experience with the current\n    prominent machine perception frameworks.</li>\n</ul>\n<h2>Syllabus</h2>\n<ul>\n<li>The fundamentals of machine learning for machine\n    perception</li>\n<li>Deep neural networks and frameworks for machine\n    perception</li>\n<li>Semantic segmentation of objects and humans</li>\n<li>Object detection and recognition</li>\n<li>Motion estimation, tracking and recognition</li>\n<li>3D geometry capture</li>\n<li>Appearance modeling and acquisition</li>\n<li>Select topics in applied machine perception</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>A practical exercise, worth 20% of the mark. This will\n    cover the basics and theory of machine perception and some of\n    the practical techniques the students will likely use in their\n    projects. This is individual work. No GPU hours will be needed\n    for the practical work.</li>\n</ul>\n<ul>\n<li>A machine perception project worth 80% of the marks:\n      <ul>\n<li>Course projects will be selected by the students\n        following the possible project\u00a0themes proposed by the\n        lecturer,\u00a0and will be checked by the lecturer for\n        appropriateness.\u00a0</li>\n</ul>\n<ul>\n<li>The students will form groups of 2-3 to design,\n        implement, report, and present a project to tackle a given\n        task in machine perception.</li>\n</ul>\n<ul>\n<li>The final mark will be composed of an\n        implementation/report mark (60%) and a presentation mark\n        (20%). Each team member will be evaluated based on her/his\n        contribution.</li>\n</ul>\n<ul>\n<li>Each project will have extensions to be completed only\n        by the ACS students. Each student will write a different\n        part of the report, whose author will be clearly marked.\n        Each student will further summarise her/his contributions\n        to the project in the same report.</li>\n</ul>\n</li>\n</ul>\n<h2>Recommended Reading List</h2>\n<ul>\n<li><em>Computer Vision: Algorithms and Applications, Richard\n    Szeliski, Springer, 2010.</em></li>\n<li><em>Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron\n    Courville, MIT Press, 2016.</em></li>\n<li><em>Machine Learning and Visual Perception, Baochang Zhang,\n    De Gruyter, 2020.</em></li>\n</ul>\n", "course_name": "Machine Visual Perception", "course_code": "L335", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L335", "lecturers": ["cpt23"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L304": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>In recent years multiprocessors have become ubiquitous, but\n  building reliable concurrent systems with good performance\n  remains very challenging. The aim of this module is to introduce\n  some of the theory and the practice of concurrent programming,\n  from hardware memory models and the design of high-level\n  programming languages to the correctness and performance\n  properties of concurrent algorithms.</p>\n<h2>Lectures</h2>\n<p>Part 1: Introduction and relaxed-memory concurrency [Professor\n  P. Sewell]</p>\n<ul>\n<li><strong>Introduction</strong>. Sequential consistency,\n    atomicity, basic concurrent problems. [1 block]</li>\n<li><strong>Concurrency on real multiprocessors</strong>: the\n    relaxed memory model(s) for x86, ARM, and IBM Power, and\n    theoretical tools for reasoning about x86-TSO programs. [2\n    blocks]</li>\n<li><strong>High-level languages</strong>. An introduction to\n    C/C++11 and Java shared-memory concurrency. [1 block]</li>\n</ul>\n<p>Part 2: Concurrent algorithms [Dr T. Harris]</p>\n<ul>\n<li><strong>Concurrent programming</strong>. Simple algorithms\n    (readers/writers, stacks, queues) and correctness criteria\n    (linearisability and progress properties). Advanced\n    synchronisation patterns (e.g. some of the following:\n    optimistic and lazy list algorithms, hash tables,\n    double-checked locking, RCU, hazard pointers), with discussion\n    of performance and on the interaction between algorithm design\n    and the underlying relaxed memory models. [3 blocks]</li>\n<li><strong>Research topics</strong>, likely to include one\n    hour on transactional memory and one guest lecture. [1\n    block]</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>have a good understanding of the semantics of concurrent\n    programs, both at the multprocessor level and the C/Java\n    programming language level;</li>\n<li>have a good understanding of some key concurrent\n    algorithms, with practical experience.</li>\n</ul>\n<h2>Assessment</h2>\n<p>Two assignments each worth 50%</p>\n<h2>Recommended reading</h2>\n<p>Herlihy, M. and Shavit, N. (2008). <em>The art of\n  multiprocessor programming</em>. Morgan Kaufmann.</p>\n", "course_name": "Multicore Semantics and Programming", "course_code": "L304", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L304", "lecturers": ["pes20", "tlh20"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L90": {"supervisions": 0, "prerequisite_for": ["L95"], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course introduces the fundamental techniques of natural\n  language processing. It aims to explain the potential and the\n  main limitations of these techniques. Some current research\n  issues are introduced and some current and potential applications\n  discussed and evaluated. Students will also be introduced to\n  practical experimentation in natural language processing.</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Introduction.</strong> Brief history of NLP\n    research, some current applications, components of NLP\n    systems.</li>\n<li><strong>Finite-state techniques.</strong> Inflectional and\n    derivational morphology, finite-state automata in NLP,\n    finite-state transducers.</li>\n<li><strong>Prediction and part-of-speech tagging.</strong>\n    Corpora, simple N-grams, word prediction, stochastic tagging,\n    evaluating system performance.</li>\n<li><strong>Context-free grammars and parsing.</strong>\n    Generative grammar, context-free grammars, parsing with\n    context-free grammars, weights and probabilities. Some\n    limitations of context-free grammars.</li>\n<li><strong>Dependency structures.</strong> English as an\n    outlier. Universal dependencies. Introduction to dependency\n    parsing.</li>\n<li><strong>Compositional semantics.</strong> Logical\n    representations. Compositional semantics and lambda calculus.\n    Inference and robust entailment. Negation.</li>\n<li><strong>Lexical semantics.</strong> Semantic relations,\n    WordNet, word senses.</li>\n<li><strong>Distributional semantics.</strong> Representing\n    lexical meaning with distributions. Similarity metrics.</li>\n<li><strong>Distributional semantics and deep\n    learning.</strong> Embeddings. Grounding. Multimodal systems\n    and visual question answering.</li>\n<li><strong>Discourse processing.</strong> Anaphora resolution,\n    summarization.</li>\n<li><strong>Language generation and regeneration.</strong>\n    Generation and regeneration. Components of a generation system.\n    Generation of referring expressions.</li>\n<li><strong>Recent NLP research.</strong> Some recent NLP\n    research.</li>\n<li><strong>Practical on information extraction.</strong>\n    Students will build named entity recognition systems which will\n    be trained and evaluated on supplied data. The system will be\n    built from existing components, but students will be expected\n    to compare approaches and some programming will be required for\n    this.</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>be able to discuss the current and likely future\n    performance of several NLP applications;</li>\n<li>be able to describe briefly a fundamental technique for\n    processing language for several subtasks, such as morphological\n    processing, parsing, word sense disambiguation etc.;</li>\n<li>understand how these techniques draw on and relate to other\n    areas of computer science.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Assignment 1 - 10% of marks</li>\n<li>Assignment 2 -\u00a0 10% of marks</li>\n<li>Assignment 3 -\u00a0 80% of marks</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>* Jurafsky, D. and Martin, J. (2008) <em>Speech and language\n  processing</em>. Prentice Hall.</p>\n", "course_name": "Natural Language Processing", "course_code": "L90", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L90", "lecturers": ["sht25", "apc38", "gete2"], "lectures": 18, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R02": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide the world with more network\n  architects. The 2011-2012 version was oriented around the\n  evolution of IP to support new services like multicast, mobility,\n  multihoming, pub/sub and, in general, data oriented networking.\n  The course is a <em>paper reading</em> which puts the onus on the\n  student to do the work.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>IPng [2 lectures, Jon Crowcroft]</li>\n<li>New Architectures [2 lectures, Jon Crowcroft]</li>\n<li>Multicast [2 lectures, Jon Crowcroft]</li>\n<li>Content Distribution and Content Centric Networks [2\n    lectures, Jon Crowcroft]</li>\n<li>Resource Pooling [2 lectures, Jon Crowcroft]</li>\n<li>Green Networking [2 lectures, Jon Crowcroft]</li>\n<li>Alternative Router Implementions [2 lectures, Jon\n    Crowcroft]</li>\n<li>Data Center Networks [2 Lectures, Jon Crowcroft]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should be able to:</p>\n<ul>\n<li>contribute to new network system designs;</li>\n<li>engineer evolutionary changes in network systems;</li>\n<li>identify and repair architectural design flaws in networked\n    systems;</li>\n<li>see that there are no perfect solutions (aside from\n    academic ones) for routing, addressing, naming;</li>\n<li>understand tradeoffs in modularisation and other pressures\n    on clean software systems implementation, and see how the world\n    is changing the proper choices in protocol layering, or non\n    layered or cross-layered.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Assessment is through three graded essays (each chosen\n  individually from a number of suggested or student-chosen\n  topics), as follows:</p>\n<ol>\n<li>Analysis of two different architectures for a particular\n    scenario in terms of cost/performance tradeoffs for some\n    functionality and design dimension, for example:\n      <ul>\n<li>ATM \u2013 e.g. for hardware <em>versus</em> software\n        tradeoff</li>\n<li>IP \u2013 e.g. for mobility, multi-homing, multicast,\n        multipath</li>\n<li>3GPP \u2013 e.g. for plain complexity <em>versus</em>\n        complicatedness</li>\n</ul>\n</li>\n<li>A discursive essay on a specific communications systems\n    component, in a particular context, such as <em>ad hoc</em>\n    routing, or wireless sensor networks.</li>\n<li>A bespoke network design for a narrow, well specified\n    specialised target scenario, for example:\n      <ul>\n<li>A customer baggage tracking network for an\n        airport.</li>\n<li>in-flight entertainment system.</li>\n<li>in-car network for monitoring and control.</li>\n<li>inter-car sensor/control network for automatic\n        highways.</li>\n</ul>\n</li>\n</ol>\n<h2>Practical work</h2>\n<p>This course does not feature any implementation work due to\n  time constraints.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Three 1,200-word essays (worth 25% each), and</li>\n<li>an annotated bibliography (25%).</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Pre-course reading:</p>\n<p>Keshav, S. (1997). <em>An engineering approach to computer\n  networking</em>. Addison-Wesley (1st ed.). ISBN 0201634422<br/>\n  Peterson, L.L. and Davie, B.S. (2007). <em>Computer networks: a\n  systems approach</em>. Morgan Kaufmann (4th ed.).</p>\n<p>Design patterns:</p>\n<p>Day, John (2007). <em>Patterns in network architecture: a\n  return to fundamentals</em>. Prentice Hall.</p>\n<p>Example systems:</p>\n<p>Krishnamurthy, B. and Rexford, J. (2001). <em>Web protocols\n  and practice: HTTP/1.1, Networking protocols, caching, and\n  traffic measurement</em>. Addison-Wesley.</p>\n<p>Economics and networks:</p>\n<p>Frank, Robert H. (2008). <em>The economic naturalist: why\n  economics explains almost everything</em>.</p>\n<p>Papers:</p>\n<p>Certainly, a collection of papers (see <a href=\"http://ccr.sigcomm.org/\">ACM CCR</a> which publishes notable\n  network researchers' favourite ten papers every 6 months or\n  so).</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Network Architectures", "course_code": "R02", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R02", "lecturers": ["jac22"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L46": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course will examine the emerging principles and\n  methodologies that underpin scalable and efficient machine\n  learning systems. Primarily, the course will focus on an exciting\n  cross-section of algorithms and system techniques that are used\n  to support the training and inference of machine learning models\n  under a spectrum of computing systems that range from constrained\n  embedded systems up to large-scale distributed systems. It will\n  also touch upon the new engineering practices that are developing\n  in support of such systems at scale. When needed to appreciate\n  issues of scalability and efficiency, the course will drill down\n  to certain aspects of computer architecture, systems software and\n  distributed systems and explore how these interact with the usage\n  and deployment of state-of-the-art machine learning.</p>\n<h2>Syllabus</h2>\n<p>Topics covered may include the following, with confirmation a\n  month before the course begins:</p>\n<ul>\n<li>System Performance Trade-offs</li>\n<li>Distributed Learning Algorithms\u00a0</li>\n<li>Model Compression\u00a0</li>\n<li>Deep Learning Compilers\u00a0</li>\n<li>Frameworks and Run-times\u00a0</li>\n<li>Scalable Inference Serving\u00a0</li>\n<li>Development Practices\u00a0</li>\n<li>Automated Machine Learning\u00a0</li>\n<li>Federated Learning\u00a0</li>\n</ul>\n<p>Primarily, topics are covered with conventional lectures.\n  However, where appropriate, material will be delivered through\n  hands-on lab tutorials. Lab tutorials will make use of hardware\n  including ARM microcontrollers and multi-GPU machines to explore\n  forms of efficient machine learning (any necessary equipment will\n  be provided to students)</p>\n<h2>Assessment</h2>\n<p>Each student will be assessed on 3 labs which will be worth\n  30% of their grade. They will also undertake a written project\n  report which will be worth\u00a070% of the grade. This report\n  will detail an investigation into a particular aspect of machine\n  learning systems, this report will be made available\n  publicly.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Principles of Machine Learning Systems", "course_code": "L46", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L46", "lecturers": [], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "LE49": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Many machine learning methods can be interpreted as \"propose a\n  probabilistic model, and fit it to the dataset\". This module will\n  study three such methods in depth: neural networks including\n  classifiers, sequence models, autoencoders, and adversarial\n  training; ranking; and document topic modelling. The common\n  themes will be how to formulate a model, how to fit it, how to\n  evaluate it, and how to reason about uncertainty.</p>\n<h2>Syllabus</h2>\n<p>The course will comprise three parts:</p>\n<ol>\n<li>Neural networks as\u00a0probabilistic models</li>\n<li>Probabilistic ranking</li>\n<li>Topic modelling.</li>\n</ol>\n<p>Some of the course will be taught by the Department of\n  Computer Science and Technology and some will be\u00a0taught via\n  lectures from the Department of Engineering. Please refer to the\n  'Course Materials' tab\u00a0for more details.</p>\n<h2>Assessment</h2>\n<p>There are four pieces of coursework. The first three are\n  structured exercises designed to reinforce the lectures. The\n  fourth is an open-ended investigation of a topic that you chose\n  from a small list, drawing on the main themes of the lecture\n  course.</p>\n<ul>\n<li>Three\u00a0exercises, one for each topic\u00a0(10%\n    each)</li>\n<li>Investigative project (70%)</li>\n</ul>\n<p>The investigative project should be written up in the style of\n  a NIPS or ICML conference paper, 8 pages plus one for\n  references.</p>\n<h2>Further Information</h2>\n<p>Many of the lectures for this module are delivered by\n  the\u00a0Department of Engineering at their Trumpington Street\n  site. Students wishing to take this module should note that the\n  Department of Engineering is about 2 miles from the Computer\n  Laboratory.</p>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Probabilistic Machine Learning", "course_code": "LE49", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/LE49", "lecturers": ["djw1005"], "lectures": 18, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R252": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The objectives of this course is to expose you to one of the\n  most active contemporary research\u00a0directions within machine\n  learning: the theory of deep learning (DL). While the first wave\n  of\u00a0modern DL has focussed on empirical breakthroughs and\n  ever more complex techniques, the\u00a0attention is now shifting\n  to building a solid mathematical understanding of why these\n  techniques<br/>\n  work so well in the first place. The purpose of this course is to\n  review this recent progress\u00a0through a mixture of reading\n  group sessions and invited talks by leading researchers in\n  the\u00a0topic, and prepare you to embark on a PhD in modern deep\n  learning research. Compared to\u00a0typical, non-mathematical\n  courses on deep learning, this advanced module will appeal to\n  those\u00a0who have strong foundations in mathematics and\n  theoretical computer science. In a way, this\u00a0course is our\n  answer to the question \u201cWhat should the world\u2019s best computer\n  science students\u00a0know about deep learning in 2021?\u201d</p>\n<h2>Learning Outcomes</h2>\n<p>This course should prepare the best students to start a PhD in\n  the theory and mathematics of\u00a0deep learning, and to start\n  formulating their own hypotheses in this space. You will\n  be\u00a0introduced to a range of empirical and mathematical tools\n  developed in recent years for the\u00a0study of deep learning\n  behaviour, and you will build an awareness of the main open\n  questions\u00a0and current lines of attack. At the end of the\n  course you will:</p>\n<ol>\n<li>be able to explain why classical learning theory is\n    insufficient to describe the\u00a0phenomenon of generalization\n    in DL</li>\n<li>be able to design and interpret empirical studies aimed at\n    understanding generalization</li>\n<li>be able to explain the role of overparameterization: be\n    able to use deep linear models as\u00a0a model to study\n    implicit regularisation of gradient-based learning</li>\n<li>be able to state PAC-Bayes and Information-theoretic\n    bounds, and apply them to DL</li>\n<li>be able to explain the connection between Gaussian\n    processes and neural networks,\u00a0and will be able to study\n    learning dynamics in the neural tangent kernel (NTK)\n    regime.</li>\n<li>be able to formulate your own hypotheses about DL and\n    choose tools to prove/test them</li>\n<li>leverage your deeper theoretical understanding to produce\n    more robust, rigorous and\u00a0reproducible solutions to\n    practical machine learning problems.</li>\n</ol>\n<h2>Syllabus</h2>\n<p>Each week we'll have two or three student-lead presentations\n  about a research paper chosen from a reading list. Occasionally,\n  we'll include invited guest lectures by top researchers in the\n  field. The reading list follows the weekly breakdown below:</p>\n<p>Week 1: Introduction to the topic<br/>\n  Week 2: Empirical Studies of Deep Learning Phenomena<br/>\n  Week 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena<br/>\n  Week 4: Implicit Regularization in Deep Linear Models<br/>\n  Week 5: Approximation Theory<br/>\n  Week 6: Networks in the Infinite Width Limit<br/>\n  Week 7: PAC-Bayes and Information Theoretic Bounds for SGD<br/>\n  Week 8: Discussion and Coursework Spotlight Session</p>\n<h2>Assessment</h2>\n<p>Students will be assessed on the following basis:</p>\n<ol>\n<li>20% for presentation/content contributed to the module:\n    Each student will have an\u00a0opportunity to present one of\n    the recommended papers during Weeks 1-7 (30 minute\u00a0slot +\n    10 mins Q&amp;A). For the presentation, students should aim to\n    communicate the core\u00a0ideas behind the paper, and clearly\n    present the results, conclusions, and future\u00a0directions.\n    Where possible, students are encouraged to comment on how the\n    work itself\u00a0fits into broader research goals.</li>\n<li>10% for active participation (regular attendance and\n    contribution to discussions during\u00a0the Q&amp;A\n    sessions).</li>\n<li>70% for a coursework report, with a word limit of 4000.\n    Either (1) an original research\u00a0proposal/report with a\n    hypothesis, review of related literature, and ideally\n    preliminary\u00a0findings, or, (2) reproduction and ideally\n    extension of an existing relevant paper.<br/>\n    Coursework reports are marked in line with general ACS\n    guidelines, reports receiving\u00a0top marks will have have\n    demonstrable research value (contain an original\n    research\u00a0idea, extension of existing work, or a thorough\n    reproduction effort which is valuable to\u00a0the research\n    community).</li>\n</ol>\n<h2>Relationship with related modules</h2>\n<p>This course can be considered as an advanced follow-up to the\n  Part IIB course on Deep Neural\u00a0Networks. That course\n  introduces some high level\u00a0concepts that this course\n  significantly expands on.</p>\n<p>This module complements L48: Machine Learning in the Physical\n  World and L46: Principles of\u00a0Machine Learning Systems, which\n  focus on applications and hardware/systems aspects of\n  ML\u00a0respectively.</p>\n<h2>Recommended reading</h2>\n<ul>\n<li><a href=\"https://www.pnas.org/cc/arthur-m-sackler-colloquium-on-the-science-of-deep-learning\">\n    PNAS Colloquium on the Science of Deep Learning</a></li>\n<li><a href=\"https://mml-book.github.io/\">Mathematics of\n    Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng\n    Soon Ong.</a></li>\n<li><a href=\"https://probml.github.io/pml-book/book1.html\">Probabilistic\n    Machine Learning: An Introduction book by Kevin Murphy</a></li>\n<li><a href=\"https://mjt.cs.illinois.edu/dlt/\">Matus\n    Telgarsky's lecture notes on deep learning\n    theory</a>\u00a0</li>\n</ul>\n<p>These are in addition to the papers which will be discussed in\n  the lectures.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Theory of Deep Learning", "course_code": "R252", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R252", "lecturers": [], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L41": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p><em>Systems research</em> refers to the study of a broad range\n  of behaviours arising from complex system design, including:\n  low-level operating systems; resource sharing and scheduling;\n  interactions between hardware and software; network-protocol\n  design and implementation; separation of mutually distrusting\n  parties on a common platform; and control of distributed-system\n  behaviours such as concurrency and data replication. This module\n  will:</p>\n<ol>\n<li>Teach systems-analysis methodology and practice through\n    tracing and performance profiling experiments;</li>\n<li>Expose students to real-world systems artefacts such as\n    I/O, IPC,and network-stack implementations, and consider their\n    hardware-software interactions with CPUs;</li>\n<li>Develop scientific experimentation, analysis and\n    presentation skills through a series of laboratory assignments;\n    and</li>\n<li>Assign a selection of original research papers to give\n    insight into potential research topics and approaches.</li>\n</ol>\n<p>The teaching style will blend lectures and hands-on labs that\n  teach methodology, design principles, and practical skills.\n  Students will be taught about (and assessed via) a series of\n  lab\u00a0assignments based on practical work. The systems studied\n  are real, and all wires will be live.</p>\n<h2>Prerequisites</h2>\n<p>It is strongly recommended that students:</p>\n<ol>\n<li>Have previously (and successfully) completed an\n    undergraduate operating-system course\n     -- or have equivalent experience\n    through project or open-source work.</li>\n<li>Have reasonable comfort with the C and Python programming\n    languages. C is the primary implementation language for systems\n    that we will analyse, requiring reading fluency; userspace C\n    programs will also be written and extended as part of lab\n    exercises. Python will be used as our data-collection and\n    processing language, and provides useful tools for data\n    analysis and presentation.</li>\n<li>Review an undergraduate operating-system textbook (such as\n    the 'Dinosaur Book') to ensure that basic OS concepts such as\n    the <em>process model</em>, <em>inter-process\n    communication</em>, <em>filesystems</em>, <em>network\n    stacks</em>, and <em>virtual memory</em> are familiar.</li>\n</ol>\n<h2>Syllabus</h2>\n<p>The sessions are split up into three submodules:</p>\n<ol>\n<li>\n<strong>Introduction to kernels and kernel\n      tracing/analysis</strong>\n<p>The purpose of this submodule is to introduce students to\n      the structure of a contemporary operating system kernel\n      through tracing and profiling.</p>\n<ul>\n<li><strong>Lecture 1:</strong> Introduction: OSes and this\n        course\u00a0(1h)</li>\n<li><strong>Lecture 2:</strong> Kernels and Tracing\n        (1h)</li>\n<li><strong>Lecturelet 1:</strong> I/O Lab (30m)</li>\n<li><strong>Lab 1:</strong>\u00a0I/O (2x2h lab sessions, if\n        in person; otherwise short 1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 1 -\u00a0\n        I/O\u00a0</strong></li>\n</ul>\n</li>\n<li>\n<strong>Processors, processes, and threads</strong>\n<p>This submodule introduces students to concrete\n      implications of the UNIX process model: processes and threads\n      in both userspace and kernelspace, the hardware foundations\n      for kernel and process isolation, system calls, and\n      traps.</p>\n<ul>\n<li><strong>Lecture 3:</strong> The Process Model - 1\n        (1h)</li>\n<li><strong>Lecture 4:</strong> The Process Model - 2\n        (1h)</li>\n<li><strong>Lecturelet 2:</strong> IPC Lab (30m)</li>\n<li><strong>Lab 2:</strong>\u00a0IPC (2x2h lab sessions, if\n        in person; otherwise short 1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 2 - IPC\n        \u00a0</strong></li>\n</ul>\n</li>\n<li>\n<strong>TCP/IP</strong>\n<p>This submodule introduces students to a contemporary,\n      multithreaded, multiprocessing network stack, with a\n      particular interest in the TCP protocol. Labs will consider\n      both the behaviour of a single TCP connection, exploring the\n      TCP state machine, socket-buffer interactions with flow\n      control, and TCP congestion control. Students will use\n      DUMMYNET to simulate network latency and explore how TCP slow\n      start and congestion avoidance respond to network conditions.\n      The second marked lab assignment will be written.</p>\n<ul>\n<li><strong>Lecture 5:</strong> The Network Stack (1)\n        (1h)</li>\n<li><strong>Lecture 6:</strong> The Network Stack (2)\n        (1h)</li>\n<li><strong>Lecturelet 3:</strong> TCP/IP Lab (30m)</li>\n<li><strong>Lab 3:</strong> TCP/IP (2x2h lab sessions, if\n        in person; otherwise short1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 3 -\u00a0 TCP\n        /IP</strong></li>\n</ul>\n</li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Have a good understanding of high-level OS kernel\n    structure</li>\n<li>Gained insight into hardware-software interactions for\n    compute and I/O</li>\n<li>Have practical skills in system tracing and performance\n    analysis</li>\n<li>Have been exposed to research ideas in system structure and\n    behaviour</li>\n<li>Have learned how to perform systems-style performance\n    evaluations</li>\n<li>Have learned how to present systems evaluation results</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>Lab assignment 1 (10% of total mark)</li>\n<li>Lab assignment 2 (45% of total mark)</li>\n<li>Lab assignment 3\u00a0(45% of total mark)</li>\n</ul>\n<h2>Recommended reading</h2>\n<h3>Primary module texts</h3>\n<p>Course texts provide instruction on statistics,\n  operating-system design and implementation, and system tracing.\n  You will be asked to read selected chapters from these, but will\n  likely find other content in them useful as you proceed with the\n  labs.</p>\n<p>Marshall Kirk McKusick, George V. Neville-Neil, and Robert N.\n  M. Watson. <em>The Design and Implementation of the FreeBSD\n  Operating System, 2nd Edition</em>, Pearson Education, Boston,\n  MA, USA, September 2014.</p>\n<p>Brendan Gregg and Jim Mauro. <em>DTrace: Dynamic Tracing in\n  Oracle Solaris, Mac OS X and FreeBSD</em>, Prentice Hall Press,\n  Upper Saddle River, NJ, USA, April 2011.</p>\n<h3>Additional texts</h3>\n<p>Abraham Silberschatz, Peter Baer Galvin, and Greg Gagne,\n  <em>Operating System Concepts, Eighth Edition</em>, John Wiley\n  and Sons, Inc., New York, NY, USA, July 2008.</p>\n<p>Brendan Gregg. <em>Systems Performance: Enterprise and the\n  Cloud</em>, Prentice Hall Press, Upper Saddle River, NJ, USA,\n  October 2013.</p>\n<h3>Research-paper readings</h3>\n<p>Research-paper readings will be announced as the terms\n  proceed, but will likely include original papers on BPF, DTrace,\n  OS scheduling, OS scalability, network stacks, and systems\n  modelling.</p>\n", "course_name": "Advanced Operating Systems", "course_code": "L41", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L41", "lecturers": ["rnw24"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L32": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims and Objectives</h2>\n<p>This module aims to extend the knowledge and skills of\n  students in designing and developing autonomous machines and\n  researching robotics-related topics. Beyond the Introduction to\n  Robotics course given in Michaelmas Term, the Advanced Robotics\n  course will focus on more advanced topics such as Robot Learning,\n  Underactuated Robot Control, Soft Robotics, Human-Robot\n  Interaction, and Multi-Agent Systems, which are not covered in\n  the introductory course.</p>\n<h2>Lectures</h2>\n<ol>\n<li>Introduction\n      <ul>\n<li>Course overview</li>\n<li>History and landscape of robotics</li>\n<li>basic knowledge and theories (kinematics, dynamics,\n        planning/search)</li>\n</ul>\n</li>\n<li>Underactuated Robotics\n      <ul>\n<li>Problem formulation and modelling</li>\n<li>Control approaches of underactuated systems</li>\n<li>Case studies</li>\n</ul>\n</li>\n<li>Robot Learning and Adaptation\n      <ul>\n<li>Model-based learning approaches</li>\n<li>Model-free learning approaches</li>\n<li>Optimization methods and case studies</li>\n</ul>\n</li>\n<li>Soft Robotics (2L; F Iida)\n      <ul>\n<li>Soft material/body robot modelling</li>\n<li>Soft actuators and sensors</li>\n<li>Control and learning of soft robots</li>\n</ul>\n</li>\n<li>5. Human-Robot Interaction 1\n      <ul>\n<li>Introduction to human-robot interaction</li>\n<li>Theoretical frameworks (spatial, nonverbal, verbal\n        interactions)</li>\n<li>Research methods, applications, robots in society</li>\n</ul>\n</li>\n<li>Distributed Robotics, Multi-Agent Systems\n      <ul>\n<li>Planning and control in multi-robot systems</li>\n<li>Methods for learning coordination and cooperation in\n        multi-agent systems</li>\n</ul>\n</li>\n<li>Coursework Presentations</li>\n</ol>\n<h2>Assessment</h2>\n<p>The assessment will be 100% coursework and consist of three\n  elements:</p>\n<ol>\n<li>first individual written report (30%)</li>\n<li>intermediate group project presentation (20%)</li>\n<li>final individual written report (50%)</li>\n</ol>\n<p>The first report is about theoretical questions on the topics\n  of advanced robotics.<br/>\n  The project will be conducted in groups of 2-3 students, and the\n  topics should be either or both simulation/hardware. The\n  intermediate presentation will be delivered by the groups.<br/>\n  The final report is expected to be a professional presentation\n  about the project, extended from the intermediate\n  presentation.</p>\n<h2>Recommended reading</h2>\n<ul>\n<li>Ronald C. Arkin 1949. Behavior-Based Robotics / Ronald C.\n    Arkin. Cambridge, Mass. : MIT Press, c1998.; 1998.</li>\n<li>Bruno Siciliano 1959; Oussama Khatib editor., eds. Springer\n    Handbook of Robotics / Edited by Bruno Siciliano, Oussama\n    Khatib. 2nd Edition. Cham : Springer International Publishing,\n    2016.; 2016.</li>\n<li>Rolf Pfeifer. Understanding Intelligence / Rolf Pfeifer and\n    Christian Scheier ; with Figures by Alex Riegler and Cartoons\n    by Isabelle Follath. (Christian Scheier, ed.). MIT Press;\n    1999.</li>\n<li>Fantoni, Isabelle, Lozano, Rogelio, Non-linear Control for\n    Underactuated Mechanical Systems, Springer, 2002</li>\n</ul>\n", "course_name": "Advanced Robotics", "course_code": "L32", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L32", "lecturers": ["asp45", "hg410"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L118": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Teaching</h2>\n<p>The teaching style will be largely based on lectures, but\n  supported by a practical component\u00a0where students will learn\n  to use a proof assistant for higher category theory.</p>\n<h2>Aims</h2>\n<p>The module will introduce advanced topics in category theory.\n  The aim is to train students to\u00a0engage and start modern\n  research on the mathematical foundations of higher\n  categories,\u00a0the graphical calculus, logical systems,\n  programming languages, type theories, and their\u00a0applications\n  in theoretical computer science, both classical and quantum.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Be familiar with the techniques of compositional category\n    theory.</li>\n<li>Have a strong understanding of basic categorical semantic\n    models.</li>\n<li>Have glimpsed current research in higher categorical\n    structures.</li>\n</ul>\n<h2>Syllabus</h2>\n<ul>\n<li>Monoidal categories and the graphical calculus (Lectures 1\n    and 2)</li>\n<li>Coherence, higher categories (Lectures 3 and 4)</li>\n<li>Linearity, superposition (Lecture 5)</li>\n<li>Duality, quantum entanglement (Lecture 6)</li>\n<li>Monoids, Frobenius algebras, and bialgebras (Lectures 7 and\n    8)</li>\n<li>Monoidal models of dualities (Lecture 9)</li>\n<li>Presheaves and profunctors (Lectures 10 and 11)</li>\n<li>Combinatorial structures (Lecture 12)</li>\n<li>Polynomial functors (Lectures 13 and 14)</li>\n<li>Models of linear logic (Lectures 15 and 16)</li>\n</ul>\n<h2>Practical</h2>\n<p>There will be 4 practical sessions where students will be\n  guided to use the proof assistant\u00a0homotopy.io. These would\n  take place in the computer room, but could be delivered\n  over\u00a0Zoom if required, since the tool is web-based and can\n  be easily accessed from any location.<br/>\n  Once students gain an understanding of the tool, they will choose\n  problems to attempt from\u00a0a long list of suggestions, of\n  varying difficulty, from easy bookwork to\n  research-level.\u00a0<br/>\n  At the end of the course, students will be required to submit 5\n  homotopy.io project\u00a0workspaces, demonstrating their best\n  work using the system. At least 3 of these workspaces\u00a0should\n  be attempts on research-level problems. The practical portfolio\n  will be graded and\u00a0form part of the assessment of the\n  course.<br/>\n  No special computing resources are required, the tool runs\n  adequately on an ordinary\u00a0laptop.</p>\n<h2>Classes</h2>\n<p>There will be 4 exercise sheets for homework, and 4 classes\n  taught by a TA from the\u00a0lecturers\u2019 research group. These\n  exercise sheets will be graded and form part of\n  the\u00a0assessment of the course.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Graded exercise sheets (70%)</li>\n<li>Practical portfolio (30%)</li>\n</ul>\n<h2>Reading List</h2>\n<p>Chris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory:\n  An Introduction\u201d, Oxford\u00a0University Press</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Advanced Topics in Category Theory", "course_code": "L118", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L118", "lecturers": ["mpf23", "jv258"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R01": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module will attempt to provide an overview of \u201csystems\u201d\n  research. This is a very broad field which has existed for over\n  50 years and which has historically included areas such as\n  operating systems, database systems, file systems, distributed\n  systems and networking, to name but a few. The course will thus\n  necessarily cover only a tiny subset of the field.</p>\n<p>Many good ideas in systems research are the result of\n  discussing and debating previous work. A primary aim of this\n  course therefore will be to educate students in the art of\n  <em>critical thinking</em>: the ability to argue for and/or\n  against a particular approach or idea. This will be done by\n  having students read and critique a set of papers each week. In\n  addition, each week will include presentations from a number of\n  participants which aim to advocate or criticise each piece of\n  work.</p>\n<h2>Syllabus</h2>\n<p>The syllabus for this course will vary from year to year so as\n  to cover a mixture of older and more contemporary systems papers.\n  Contemporary papers will be generally selected from the past 5\n  years, primarily drawn from high quality conferences such as\n  SOSP, OSDI, ASPLOS, FAST, NSDI and EuroSys. Example topics might\n  include:</p>\n<ul>\n<li><em>Systems Research and System Design</em>,</li>\n<li><em>OS Structure and Virtual Memory</em>,</li>\n<li><em>Systems Virtualisation</em>,</li>\n<li><em>Consensus</em>,</li>\n<li><em>Scheduling</em>,</li>\n<li><em>Privacy</em>,</li>\n<li><em>Data Intensive Computing</em>, and</li>\n<li><em>Bugs</em>.</li>\n</ul>\n<p>The reading each week will involve a load equivalent to 3 full\n  length papers. Students will be expected to read these in detail\n  and prepare a written summary and review. In addition, each week\n  will contain one or more short presentations by students for each\n  paper. The types of presentation will include:</p>\n<ul>\n<li><strong>Overview</strong>: a balanced presentation of the\n    paper, covering both positive and negative aspects.</li>\n<li><strong>Advocacy</strong>: a positive spin on the paper,\n    aiming to convince others of its value.</li>\n<li><strong>Criticism</strong>: a negative take on the paper,\n    focusing on its weak spots and omissions.</li>\n</ul>\n<p>These presentation roles will be assigned in advance,\n  regardless of the <em>soi disant</em> absolute merit of the paper\n  or the preference of the student. Furthermore, all students \u2013\n  regardless of any assigned presentation role in a given week \u2013\n  will be expected to participate in the class by asking questions\n  and generally entering into the debate.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should have a broad\n  understanding of some key papers and concepts in computer systems\n  research, as well as an appreciation of how to argue for or\n  against any particular idea.</p>\n<h2>Coursework and practical work</h2>\n<p>Coursework will be the production of the weekly paper reviews.\n  Practical work will be presenting papers as appropriate, as well\n  as ongoing participation in the class.</p>\n<h2>Assessment</h2>\n<p>Assessment consists of:</p>\n<ul>\n<li>One essay per week for 7 weeks (10% each)</li>\n<li>Presentation (20%)</li>\n<li>Participation in class over the term (10%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Most of the reading for this course will be in the form of the\n  selected papers each week. However, the following may be useful\n  background reading to refresh your knowledge from undergraduate\n  courses:</p>\n<p>Silberschatz, A., Peterson, J.L. and Galvin, P.C. (2005).\n  <em>Operating systems concepts</em>. Addison-Wesley (7th\n  ed.).</p>\n<p>Tanenbaum, A.S. (2008). <em>Modern Operating Systems</em>.\n  Prentice-Hall (3rd ed.).</p>\n<p>Bacon, J. and Harris, T. (2003). <em>Operating systems</em>.\n  Addison-Wesley (3rd ed.).</p>\n<p>Anderson, T. and Dahlin, M. (2014). <em>Operating Systems:\n  Principles and Practice</em>. Recursive Books (2nd ed.).</p>\n<p>Hennessy, J. and Patterson, D. (2006). <em>Computer\n  architecture: a quantitative approach</em>. Elsevier (4th ed.).\n  ISBN\u00a0978-0-12-370490-0.</p>\n<p>Kleppmann M (2016) <em>Designing Data-Intensive\n  Applications</em>, O'Reilly (1st ed.)</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Advanced Topics in Computer Systems", "course_code": "R01", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R01", "lecturers": ["rmm1002"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R255": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course explores current research topics in machine\n  learning\u00a0in sufficient depth that, at the end of the course,\n  participants will be in a position to contribute to research on\n  their chosen topics. Each topic will be introduced with a lecture\n  which, building on the material covered in the prerequisite\n  courses, will make the current research literature accessible.\n  Each lecture will be followed by up to three seminar sessions\n  which will typically be run as a reading group with student\n  presentations on recent papers from the literature followed by a\n  discussion.</p>\n<h2>Structure</h2>\n<p>Each student will attend 3 topics and each topic's sessions\n  will be spread over 5 contact hours. Students will be expected to\n  undertake readings for their selected topics. There will be some\n  group work.</p>\n<p>There will be a briefing session in Michaelmas term.</p>\n<h2>Syllabus</h2>\n<p>Students choose exactly\n  three\u00a0<strong>topics</strong>\u00a0in preferential order\n  from a list to be published in Michaelmas term. Students are\n  assessed on one of these topics which may\u00a0not necessarily be\n  their first choice topic.</p>\n<p>Topics to be offered in 2021-22:</p>\n<ol>\n<li>Imitation learning\u00a0<em>Dr A. Vlachos</em></li>\n<li>Applications of Machine Learning to Psychiatry <em>Dr S.\n    Morgan</em></li>\n<li>Federated Learning\u00a0<em>Dr N.\u00a0Lane</em></li>\n<li>Reinforcement learning\u00a0<em>Dr A.\u00a0Prorok</em></li>\n<li>Machine Learning of self-organizing structures: from\n    textures to developmental biology\u00a0<em>Dr\n    B.\u00a0Dumitrascu</em></li>\n<li>Causal Inference <em>Dr F.\u00a0Husz\u00e1r</em></li>\n<li>Bias in datasets\u00a0<em>Dr M.\u00a0Tomalin</em></li>\n<li>Probabilistic Numerics: Computation as statistical\n    inference\u00a0<em>Dr C. H.\u00a0Ek</em></li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be in a strong position to contribute to the research\n    topics covered;</li>\n<li>understand the fundamental methods (algorithms, data\n    analysis, specific tasks) underlying each topic;</li>\n<li>and be familiar with recent research papers and advances in\n    the field.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Students will work in groups to give a presentation on\n  assigned papers. Each topic will typically consist of one\n  preliminary lecture followed by 3 reading and discussion\n  sessions. A typical topic can accommodate up to 9 students\n  presenting papers. There will be at least 10 minutes general\n  discussion per session.</p>\n<p>Full coursework details will be published by October.</p>\n<h2>Assessment</h2>\n<p>Coursework will be marked by the topic leaders and second\n  marked by the module conveners.</p>\n<ul>\n<li>Participation/attendance in\u00a0three topics, 10%</li>\n<li>Presentation (for one of the chosen topics), 20%</li>\n<li>Topic coursework (for one of the chosen\u00a0topics),\n    70%</li>\n</ul>\n<p>Individual topic coursework will be published late Michaelmas\n  term.</p>\n<p><em>Please note that students will be assessed on one of their\n  three chosen topics\u00a0but this may not\n  be\u00a0their\u00a0first choice</em>.</p>\n<h2>Recommended reading</h2>\n<p>To be confirmed.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Advanced topics in machine learning", "course_code": "R255", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R255", "lecturers": ["mj201", "av308"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R254": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course examines major topics relating to cybercrime from\n  an interdisciplinary perspective. These include offence types and\n  techniques, targets, victimisation, social and financial cost,\n  criminal marketplaces, offenders, detection and prevention, and\n  regulation and policing. The course outlines: key debates in\n  cybercrime research; how crime is committed using computer\n  systems; and provides an understanding of how cybercrime is\n  regulated, policed, detected, and prevented.</p>\n<h2>Syllabus</h2>\n<p>The course will consist of eight two-hour sessions\n  covering:</p>\n<ul>\n<li>Tools and techniques of cybercrime</li>\n<li>Cybercrime victimisation</li>\n<li>Costs and harms of cybercrime</li>\n<li>Criminal marketplaces</li>\n<li>Cybercrime offenders and offender pathways</li>\n<li>Cybercrime prevention (situational and social\n    approaches)</li>\n<li>Regulation and policy</li>\n<li>Cybercrime and the criminal justice system</li>\n</ul>\n<p>\u00a0</p>\n<p>All participants are expected to attend and participate in\n  every class, and to read the specified papers beforehand. The\n  instructor must be notified of any absences in advance.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Have a broad knowledge of the key themes, debates, theory,\n    and research in relation to cybercrime;</li>\n<li>Have developed further skills in critical analysis;</li>\n<li>Have developed skills in presenting a case study,\n    critically evaluating current issues, and writing about\n    cybercrime;</li>\n<li>Have a sound understanding of strategies to combat and\n    prevent cybercrime;</li>\n<li>Understand the ethical and practical challenges in\n    conducting cybercrime research.</li>\n</ul>\n<h2>Assessment</h2>\n<p>You will assessed by 4 essays each worth 25% of the total\n  marks</p>\n<h2>Recommended reading</h2>\n<p>Please see Course Materials for recommended reading for each\n  session.</p>\n<p>\u00a0</p>\n", "course_name": "Cybercrime", "course_code": "R254", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R254", "lecturers": ["ah793", "rja14", "rnc1"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "P230": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is an advanced course in human-computer interaction, with\n  a specialist focus on intelligent user interfaces and interaction\n  with machine-learning and artificial intelligence technologies.\n  The format will be largely Practical, with students working in\n  pairs to carry out a mini-project involving empirical research\n  investigation. These studies will investigate human interaction\n  with some kind of model-based system for planning,\n  decision-making, automation etc. Possible study formats might\n  include: System evaluation, Field observation, Hypothesis testing\n  experiment, Design intervention or Corpus analysis, following set\n  examples from recent research publications. Project work will be\n  formally evaluated through a report and presentation.</p>\n<h2>Lectures</h2>\n<p>(note that Lectures 2-7 also include one hour class discussion\n  of practical work)<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Current research\n  themes in intelligent user interfaces<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Program\n  synthesis<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Mixed initiative\n  interaction<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Interpretability /\n  explainable AI<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Labelling as a\n  fundamental problem<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Machine learning\n  risks and bias<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Visualisation and\n  visual analytics<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Student research\n  presentations</p>\n<h2>Objectives</h2>\n<p>By the end of the course students should:<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be familiar with\n  current state of the art in intelligent interactive systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 understand the human\n  factors that are most critical in the design of such systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to evaluate\n  evidence for and against the utility of novel systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 have experience of\n  conducting user studies meeting the quality criteria of this\n  field<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to write up\n  and present user research in a professional manner</p>\n<h2>Assessment</h2>\n<p>Assignment 1: six incremental submissions which\u00a0together\n  contribute 20% to the final module mark.</p>\n<p>Assignment 2: Final report - 80% of the final module mark</p>\n<h2>Recommended reading</h2>\n<p>Brad A. Myers and Richard McDaniel (2000). <a href=\"http://web.media.mit.edu/~lieber/Your-Wish/03-Myers.pdf\">Demonstrational\n  Interfaces: Sometimes You Need a Little Intelligence, Sometimes\n  You Need a Lot</a>.<br/>\n  \u00a0</p>\n", "course_name": "Interaction with Machine Learning", "course_code": "P230", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/P230", "lecturers": ["afb21"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L98": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is a lecture-style course that introduces students to\n  various aspects of the semantics of\u00a0Natural Languages\n  (mainly English):</p>\n<ul>\n<li>Lexical Semantics, with an emphasis on theory and\n    phenomenology (4 sessions)</li>\n<li>Compositional Semantics (9 sessions)</li>\n<li>Discourse and pragmatics-related aspects of semantics (3\n    sessions)</li>\n</ul>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Give an operational definition of what is meant by\n    \u201cmeaning\u201d (for instance, above and\u00a0beyond syntax);</li>\n<li>Name the types of phenomena in language that require\n    semantic consideration, in terms\u00a0of lexical, compositional\n    and discourse/pragmatic aspects, in other words, argue\n    why\u00a0semantics is important;</li>\n<li>Demonstrate an understanding of the basics of various\n    semantic representations,\u00a0including logic-based and\n    graph-based semantic representations, their properties,\n    how\u00a0they are used and why they are important, and how they\n    are different from syntactic\u00a0representations;</li>\n<li>Know how such semantic representations are derived during\n    or after parsing, and how\u00a0they can be analysed and mapped\n    to surface strings;</li>\n<li>Understand applications of semantic representations e.g.\n    reasoning, validation, and\u00a0methods how these are\n    approached.</li>\n<li>When designing NL tasks that clearly require semantic\n    processing (e.g.\u00a0knowledge-based QA), to be aware of and\n    reuse semantic representations and\u00a0algorithms when\n    designing the task, rather than reinventing the wheel.</li>\n</ul>\n<h2>Practical advantages of this course for NLP students</h2>\n<ul>\n<li>Knowledge of underlying semantic effects helps improve NLP\n    evaluation, for instance by\u00a0providing more meaningful\n    error analysis. You will be able to link particular errors\n    to\u00a0design decisions inside your system.</li>\n<li>You will learn methods for better benchmarking of your\n    system, whatever the task may\u00a0be. Supervised ML systems\n    (in particular black-box systems such as Deep Learning)\n    are\u00a0only as clever as the datasets they are based on. In\n    this course, you will learn to design\u00a0datasets so that\n    they are harder to trick without real understanding, or\n    critique existing\u00a0datasets.</li>\n<li>You will be able to design tests for ML systems that better\n    pinpoint which aspects of\u00a0language an end-to-end system\n    has \u201cunderstood\u201d.</li>\n<li>You will learn to detect ambiguity and ill-formed semantics\n    in human-human\u00a0communication. This can serve to write more\n    clearly and logically.</li>\n<li>You will learn about decomposing complex semantics-reliant\n    tasks sensibly so that you\u00a0can reuse the techniques\n    underlying semantic analyzers in a modular way. In this\n    way,\u00a0rather than being forced to treat complex tasks in an\n    end-to-end manner, you will be able\u00a0to profit from partial\n    explanations and a better error analysis already built into\n    the\u00a0system.</li>\n</ul>\n<h2>Syllabus</h2>\n<ol>\n<li>Word senses</li>\n<li>WSD algorithms</li>\n<li>Semantic roles</li>\n<li>Semantics of modifiers (adjectives, noun-noun relations,\n    sentiment)</li>\n<li>Truth conditions and types</li>\n<li>Syntax--semantics transparency and categorial grammar</li>\n<li>Graph-based meaning representations</li>\n<li>Meaning representations in natural language generation</li>\n<li>Scope-related issues</li>\n<li>Compositional distributional models</li>\n<li>Discourse processing</li>\n<li>Topic structure and Focus structure</li>\n<li>Reasoning</li>\n<li>Semantics of comparatives</li>\n<li>Presuppositions and Gricean Maxims</li>\n<li>Figurative language</li>\n</ol>\n<h2>Assessment</h2>\n<ul>\n<li>Two short tasks / exercises (10% each)</li>\n<li>Take-home test\u00a0(80%)</li>\n</ul>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Introduction to Computational Semantics", "course_code": "L98", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L98", "lecturers": ["ws390", "sht25"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L45": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims and Objectives</h2>\n<p>Most of the patterns we see in nature are elegantly\n  representable using the language of graph structures. Prominent\n  examples include molecules (represented as graphs of atoms and\n  bonds), social networks and transportation networks. Several\n  already-impacted application areas include traffic forecasting,\n  drug discovery, social network analysis and recommender systems.\n  Further, some of the most successful domains of application for\n  machine learning in previous years---images, text and speech\n  processing---can be seen as special cases of graph representation\n  learning, and consequently there has been significant exchange of\n  information between these areas. The module will provide the\n  students the capability to analyse graph-structured data in an\n  effective way, and position graph representation learning in a\n  proper context with related fields. The main aim of the course is\n  to enable students to make direct contributions to the field of\n  graph representation learning, thoroughly assimilate the key\n  concepts in the area, and draw relevant connections to various\n  other fields (such as NLP, Fourier Analysis and Probabilistic\n  Graphical Models). We assume only a basic background in machine\n  learning with deep neural networks. The course aims to empower\n  the students to discover new ideas in this area in future\n  years.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Fundamentals of processing data on graphs, as well as\n    impactful application areas for graph representation\n    learning</li>\n<li>Theoretical principles of graph learning: permutation\n    invariance and equivariance</li>\n<li>The three \"flavours\" of spatial graph neural networks\n    (GNNs) (convolutional, attentional, message passing) and their\n    relative merits</li>\n<li>Relevant connections of graph representation learning to\n    various other fields (such as NLP, Fourier Analysis and\n    Probabilistic Graphical Models)</li>\n<li>The \"bigger picture\" of GNNs within a broader geometric\n    deep learning framework.</li>\n</ul>\n<h2>Lectures</h2>\n<p>The lectures will cover the following topics:</p>\n<ul>\n<li>Why study data on graphs? Success stories: drug screening,\n    travel time estimation, recommender systems. Fundamentals of\n    graph data processing: network science, spectral clustering,\n    node embeddings.</li>\n<li>Inductive biases, with a brief look at CNNs. Permutation\n    invariance and equivariance on sets and graphs. The principal\n    tasks of node, edge and graph classification. Neural networks\n    for point clouds: Deep Sets, PointNet; universal approximation\n    properties.</li>\n<li>The three flavours of spatial GNNs: convolutional,\n    attentional, message passing. Prominent examples: ChebyNets,\n    GCN, SGC, MoNet, GAT, GaAN, IN, MPNN, GraphNets. Tradeoffs of\n    using different GNN variants.</li>\n<li>Revisiting node embeddings, and their link to conv-GNNs.\n    Self-supervised learning with GNNs: random-walk objectives\n    (VGAE), contrastive (DGI, GRACE), bootstrapping (BGRL), feature\n    masking. The effectiveness of randomly initialised GNNs.</li>\n<li>Revisiting Deep Sets: how to apply GNNs when there is no\n    graph? Links to natural language processing---Transformers as a\n    special case of attentional GNNs. Methodologies for latent\n    graph inference: DGCNN, NRI, DGM, PGN.</li>\n<li>Expressive power of graph neural networks: the\n    Weisfeiler-Lehman hierarchy. GINs as a maximally expressive\n    GNN. Links between GNNs and graph algorithms.</li>\n<li>The bigger picture of learning with invariances and\n    symmetries: geometric deep learning. Worked examples: Circulant\n    matrices on grids, the discrete Fourier transform, and\n    convolutional networks on spheres. Graph Fourier transform and\n    the Laplacian eigenbasis. Implications to spatial GNN\n    flavours.</li>\n</ul>\n<h2>Practicals</h2>\n<p>The practicals\u00a0are designed to complement the knowledge\n  learnt in lectures and teach students to derive additional\n  important results and architectures not directly shown in\n  lectures. The practicals will be given as a series of individual\n  exercises (each either code implementation or proof/derivation).\n  Each of these exercises can be individually assessed based on a\n  specified mark budget.</p>\n<p>Possible practical topics include the study of higher-order\n  GNNs and equivariant message passing.</p>\n<h2>Assessment</h2>\n<ul>\n<li>(70%) Mini-project (writeup) at the end of the course (in\n    the students' area of choice within graph representation\n    learning). The mini projects can be self-proposed, and will\n    consist of implementing and/or extending graph representation\n    learning models in the literature, applying them to publicly\n    available datasets. The writeup would be limited to 4,000 words\n    (in line with other modules);</li>\n<li>(30%) Practical work completion. Completing the exercises\n    specified in the practicals to a satisfactory standard. The\n    practical assessor should be satisfied that the student derived\n    their answers using insight gained from the course; coupled\n    with original thought, not by simple copy-pasting of relevant\n    related work. The students would submit code and a short\n    report, which would then be marked in line with the\n    predetermined mark budget for each practical item.</li>\n</ul>\n<p>The students will learn how to run advanced architectures on\n  GPU but no specific need for dedicated GPU resources. Practicals\n  will be made possible to do on CPU; if required, students can use\n  GPUs on publicly available free services (such as Colab) for\n  their mini-project work.</p>\n<h2>References</h2>\n<p>The course will be based on the following literature:</p>\n<ul>\n<li>\"Graph Representation Learning\", by Will Hamilton</li>\n<li>\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics,\n    and Gauges\", by Michael Bronstein, Joan Bruna, Taco Cohen and\n    Petar Veli\u010dkovi\u0107</li>\n<li>\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron\n    Courville.</li>\n</ul>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Representation Learning on Graphs and Networks", "course_code": "L45", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L45", "lecturers": ["pl219"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R260": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Data-driven technologies are increasingly the subject of\n  social commentary, political scrutiny and regulatory attention.\n  This module aims to develop a solid understanding of the\n  implications these concerns have on systems design and\n  development, and to encourage engineering practices that are more\n  responsible and socially-aware.</p>\n<p>Areas explored include the legal foundations in data\n  protection (GDPR), privacy, liability, human rights; issues of\n  tech-surveillance; the engineering aspects to rights (\"rights\n  engineering\"); algorithmic fairness, accountability and\n  transparency (the so-called 'FAT'); and the related implications\n  for technologies including cloud, machine learning and the\n  Internet of Things.</p>\n<p>This course provides students with a practical background\n  regarding how law, policy, power dynamics and societal concerns\n  interact with technology. This is to develop an awareness and\n  consideration of how systems can be designed and engineered to be\n  more accountable, legally compliant, and generally better for\n  society.</p>\n<h2>Syllabus</h2>\n<p>The course will consist of\u00a0eight seminars covering the\n  following topics:</p>\n<ul>\n<li>The foundations and interplays of tech-law</li>\n<li>Privacy, data protection (GDPR), techno-surveillance</li>\n<li>Issues brought by cloud, platforms, and mobile/app\n    ecosystems and\u00a0the Internet of Things</li>\n<li>Engineering for rights</li>\n<li>Algorithmic accountability (ML / automated decision\n    making)</li>\n</ul>\n<p>Seminars will be interactive and discussion-oriented, with\n  foundational materials presented in a lecture-like format.\n  Students will be allocated one or more reading materials to\n  present to the group. Coursework will also include short comment\n  pieces, and a research essay on relevant topics.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Appreciate the practical considerations and challenges of\n    engineering more compliant and accountable systems;</li>\n<li>Understand the key legal, regulatory and social influences\n    on technical design;</li>\n<li>Be familiar with the emerging\u00a0socio-technical and\n    'responsible technology' research\u00a0landscapes;</li>\n<li>Appreciate the ongoing legal, policy and societal debates\n    concerning emerging technology.</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>Presentation(s) of reading material (30%);</li>\n<li>Short comment pieces on topical issues (max 500 words)\n    (20%);</li>\n<li>A research essay (max 2500 words). Short presentations in\n    the final week (40%);</li>\n<li>General participation (10%).</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p>Specific reading materials will be set according to each\n  week\u2019s topic.</p>\n<p>For some general background and context, suggested reading\n  includes:</p>\n<p>Arbesman, Samuel. (2017) <em>Overcomplicated</em>,\n  Portfolio.</p>\n<p>Eubaks, Virginia. (2019) <em>Automating Inequality: How\n  High-Tech Tools Profile, Police, and Punish the Poor</em>,\n  Picador USA.</p>\n<p>Lessig, Lawrence (2006) <a href=\"http://codev2.cc/download+remix/Lessig-Codev2.pdf\" target=\"_blank\"><em>Code, and other laws of cyberspace</em></a>, Basic\n  Books</p>\n<p>Hildebrandt, Mireille. (2020) <a href=\"https://www.cohubicol.com/assets/uploads/law_for_computer_scientists.pdf\" target=\"_blank\"><em>Law for Computer Scientists</em></a>,\n  Oxford</p>\n<p>Schneier, Bruce (2012) <a href=\"https://www.schneier.com/books/liars_and_outliers/\" target=\"_blank\"><em>Liars and Outliers</em></a>, John Wiley &amp;\n  Sons</p>\n<p>O'Neil, Cathy. (2017) <em>Weapons of Math Destruction: How Big\n  Data Increases Inequality and Threatens Democracy</em>,\n  Penguin.</p>\n<p>Pasquale, Frank. (2016) <em>The Black Box Society: The Secret\n  Algorithms That Control Money and Information</em>. Harvard.</p>\n<p><em><a href=\"https://www.fatml.org/resources/principles-for-accountable-algorithms\" target=\"_blank\">Principles for Accountable\n  Algorithms</a></em></p>\n<p><em><a href=\"http://fra.europa.eu/en/publication/2018/handbook-european-data-protection-law\" target=\"_blank\">Handbook on European Data Protection Law\n  2018</a></em></p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Technology, law and society", "course_code": "R260", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/R260", "lecturers": ["js573"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L15": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide an introduction to topics in\n  complexity theory beyond that covered in the undergraduate course\n  and a grounding in research that connects this with methods from\n  logic. The topics covered in the last four lectures will focus on\n  current research and may vary from year to year.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Complexity theory\u2014a review of the major complexity classes\n    (space, time, nondeterministic, etc.) and their\n    interrelationships. [3 lectures]</li>\n<li>First-order and second-order logic: their expressive power\n    and computational complexity. [3 lectures]</li>\n<li>Lower bounds on expressive power: the use of games and\n    locality. [3 lectures]</li>\n<li>Fixed-point logics and descriptive complexity. [3\n    lectures]</li>\n<li>A selection of topics from the following [4 lectures]:\n      <ul>\n<li>finite-variable logics;</li>\n<li>complexity of constraint satisfaction problems;</li>\n<li>random structures;</li>\n<li>parameterized complexity;</li>\n<li>complexity of logical theories;</li>\n<li>logic and circuit complexity.</li>\n<li>logics of polynomial time computation.</li>\n</ul>\n</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with the basic relationship between the\n    expressive power of logic and computational complexity;</li>\n<li>be able to formulate simple game-based inexpressibility\n    arguments;</li>\n<li>be able to identify current research issues relating logic\n    to complexity.</li>\n</ul>\n<h2>Coursework and practical work</h2>\n<p>None.</p>\n<h2>Assessment</h2>\n<p>The assessment will be based on an essay completed during the\n  term, and a take-home test taken after the course, weighted as\n  follows:</p>\n<ul>\n<li>Essay: 30%</li>\n<li>Test: 70%</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Arora, S. and Barak, B. (2009). <em>Computational\n  complexity</em>. Cambridge University Press.<br/>\n  Gradel. E. <em>et al</em>. (2007). <em>Finite model theory and\n  its applications</em>. Springer.<br/>\n  Libkin, L. (2004). <em>Elements of finite model theory</em>.\n  Springer.<br/>\n  Immerman, N. (1999). <em>Descriptive complexity</em>.\n  Springer.<br/>\n  Ebbinghaus, H-D. and Flum, J. (1999). <em>Finite model\n  theory</em>. Springer.</p>\n<h2>Further Information</h2>\n<p>Due to COVID-19, the method of teaching for this module will\n  be adjusted to cater for physical distancing and students who are\n  working remotely. We will confirm precisely how the module will\n  be taught closer to the start of term.</p>\n", "course_name": "Topics in Logic and Complexity", "course_code": "L15", "course_url": "https://www.cl.cam.ac.uk/teaching/2122/L15", "lecturers": ["ad260"], "lectures": 16, "year": "2122", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}}