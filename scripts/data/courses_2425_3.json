{"R265": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims to provide students with an introduction to a\n  range of advanced topics in computer architecture. It will\n  explore the current and future challenges facing the architects\n  of modern computers. These will also be used to illustrate the\n  many different influences and trade-offs involved in computer\n  architecture.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should:</p>\n<ul>\n<li>understand the challenges of designing and verifying modern\n    microprocessors</li>\n<li>be familiar with recent research themes and emerging\n    challenges</li>\n<li>appreciate the complex trade-offs at the heart of computer\n    architecture</li>\n</ul>\n<h2>Syllabus</h2>\n<p>Each seminar will focus on a different topic. The proposed\n  topics are listed below but there may be some minor changes to\n  this:</p>\n<ul>\n<li>Trends in computer architecture</li>\n<li>State-of-the-art microprocessor design</li>\n<li>Memory system design</li>\n<li>Hardware reliability</li>\n<li>Specification, verification and test <i>(may be be replaced\n    with a different topic)</i></li>\n<li>Hardware security (2)</li>\n<li>HW accelerators and accelerators for machine learning</li>\n</ul>\n<p>Each two hour seminar will include three student presentations\n  (15mins) questions (5mins) and a broader discussion of the topics\n  (around 30mins). The last part of the seminar will include a\n  short scene setting lecture (around 20mins) to introduce the\n  following week's topic.</p>\n<h2>Assessment</h2>\n<p>Each week students will compare and contrast two of the main\n  papers and submit a written summary and review in advance of each\n  seminar (except when presenting).</p>\n<p>Students will be expected to give a number of 15 minute\n  presentations.</p>\n<p>Essays and presentations will be marked out of 10. After\n  dropping the lowest mark, the remaining marks will be scaled to\n  give a final score out of 100.</p>\n<p>Students will give at least one presentation during the\n  course. They will not be required to submit an essay during the\n  weeks they are presenting.</p>\n<p>Each presentation will focus on a single paper from the\n  reading list. Marks will be awarded for clarity and the\n  communication of the paper's key ideas, an analysis of the work's\n  strengths and weaknesses and the work\u2019s relationship to related\n  work and broader trends and constraints.</p>\n<h2>Recommended prerequisite reading</h2>\n<p>Patterson, D. A., Hennessy, J. L. (2017). <i>Computer\n  organization and design: The Hardware/software interface RISC-V\n  edition</i> Morgan Kaufmann. ISBN 978-0-12-812275-4.</p>\n<p>Hennessy, J. and Patterson, D. (2012). <i>Computer\n  architecture: a quantitative approach</i>. Elsevier (5th ed.)\n  ISBN 9780123838728. (the 3rd and 4th editions are also\n  relevant)</p>\n", "course_name": "Advanced Topics in Computer Architecture", "course_code": "R265", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R265", "lecturers": ["tmj32", "rdm34", "jdw57"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R277": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module explores various topics in programming languages\n  beyond the scope of undergraduate\u00a0courses. It aims to\n  introduce students to ideas, results and techniques found in the\n  literature and\u00a0prepare them for research in the field.</p>\n<h2>Syllabus and structure</h2>\n<p>The module consists of eight two-hour seminars, each on a\n  particular topic. Topics will vary from year\u00a0to year, but\n  may include, for example,</p>\n<ul>\n<li>Abstract interpretation</li>\n<li>Verified software</li>\n<li>Metaprogramming</li>\n<li>Behavioural types</li>\n<li>Program synthesis</li>\n<li>Verified compilation</li>\n<li>Partial evaluation</li>\n<li>Garbage collection</li>\n<li>Dependent types</li>\n<li>Automatic differentiation</li>\n<li>Delimited continuations</li>\n<li>Module systems</li>\n</ul>\n<p>There will be three papers assigned for each topic, which\n  students are expected to read before the\u00a0seminar.<br/>\n  Each seminar will include three 20 minute student presentations\n  (15 minutes + 5 minutes questions),\u00a0time for general\n  discussion of the topic, and a brief overview lecture for the\n  following week\u2019s topic.<br/>\n  Before each seminar, except in weeks in which they give\n  presentations, students will submit a short\u00a0essay about two\n  of the papers.</p>\n<h2><br/>\n  Objectives</h2>\n<p>On completion of this module, students should</p>\n<ul>\n<li>be able to identify some major themes in programming\n    language research</li>\n<li>be familiar with some classic papers and recent\n    advances</li>\n<li>have an understanding of techniques used in the field</li>\n</ul>\n<h2><br/>\n  Assessment</h2>\n<p>Assessment consists of:</p>\n<ul>\n<li>Presentation of one of the papers from the reading list\n    (typically once or twice in total for each\u00a0student,\n    depending on class numbers)</li>\n<li>One essay per week (except on the first week and on\n    presentation weeks)</li>\n</ul>\n<p>All essays and presentations carry equal numbers of marks.</p>\n<p>Essay marks are awarded for understanding, for insight and\n  analysis, and for writing quality. Essays\u00a0should be around\n  1500 words (with a lower limit of 1450 and upper limit of 1650).\n  Presentation marks\u00a0are awarded for clarity, for effective\n  communication, and for selection and organisation of topics.</p>\n<p>There will be seven submissions (essays or presentations) in\n  total and, as in other courses, the lowest\u00a0mark for each\n  student will be disregarded when computing the final mark.</p>\n<p>Marking, deadlines and extensions will be handled in\n  accordance with the <a href=\"https://www.cst.cam.ac.uk/teaching/exams/mphil-assessment\">MPhil\n  Assessment Guidelines</a>.</p>\n<h2>Recommended reading material and resources</h2>\n<p>Research and survey papers from programming language\n  conferences and journals (e.g. <a href=\"https://popl22.sigplan.org/series/POPL\">POPL</a>, <a href=\"https://pldi19.sigplan.org/series/pldi\">PLDI</a>,\u00a0<a href=\"https://dl.acm.org/journal/toplas\">TOPLAS</a>, <a href=\"https://www.nowpublishers.com/PGL\">FTPL</a>) will be assigned\n  each week. General background material may be found in:</p>\n<p>\u2022 <a href=\"https://www.cis.upenn.edu/~bcpierce/tapl/\">Types\n  and Programming Languages</a> (Benjamin C. Pierce)<br/>\n  \u00a0 \u00a0The MIT Press<br/>\n  \u00a0 \u00a0ISBN 0-262-16209-1</p>\n<p>\u2022 <a href=\"https://www.cis.upenn.edu/~bcpierce/attapl/\">Advanced Topics in\n  Types and Programming Languages</a> (ed. Benjamin C. Pierce)<br/>\n  \u00a0 \u00a0The MIT Press<br/>\n  \u00a0 \u00a0ISBN 0-262-16228-8</p>\n<p>\u2022 <a href=\"https://www.cs.cmu.edu/~rwh/pfpl/\">Practical\n  Foundations for Programming Languages</a> (Robert Harper)<br/>\n  \u00a0 \u00a0Cambridge University Press<br/>\n  \u00a0 \u00a09781107150300</p>\n", "course_name": "Advanced topics in programming languages", "course_code": "R277", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R277", "lecturers": ["jdy22"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L344": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Synopsis</h2>\n<p>Affective Artificial Intelligence (Affective AI) aims to imbue\n  machines with social and emotional intelligence (EQ). More\n  specifically, Affective AI aims to create artificially\n  intelligent systems and machines that can recognize, interpret,\n  process, and simulate human social signals and behaviours,\n  expressions, and emotions, to enhance human-AI interaction and\n  communication. \\r\\n\\r\\nTo achieve this goal, Affective AI draws\n  upon various scientific disciplines, including machine learning,\n  computer vision, speech / natural language / signal processing,\n  psychology and cognitive science, and ethics and social\n  sciences.</p>\n<h2>\u00a0</h2>\n<h2>Background &amp; Aims</h2>\n<p>Affective Artificial Intelligence (Affective AI) aims to imbue\n  machines with social and emotional intelligence (EQ). More\n  specifically, Affective AI aims to create artificially\n  intelligent systems and machines that can recognize, interpret,\n  process, and simulate human social signals and behaviours,\n  expressions, and emotions, to enhance human-AI interaction and\n  communication.\u00a0</p>\n<p>To achieve this goal, Affective AI draws upon various\n  scientific disciplines, including machine learning, computer\n  vision, speech / natural language / signal processing, psychology\n  and cognitive science, and ethics and social sciences.</p>\n<p>Affective AI has direct applications in and implications for\n  the design of innovative interactive technology (e.g.,\n  interaction with chat bots, virtual agents, robots), single and\n  multi-user smart environments ( e.g., in-car/ virtual / augmented\n  / mixed reality, serious games), public speaking and cognitive\n  training,\u00a0and clinical and biomedical studies (e.g., autism,\n  depression, pain).</p>\n<p>The aim of this module is to impart knowledge and ability\n  needed to make informed choices of models, data, and machine\n  learning techniques for sensing, recognition, and generation of\n  affective and social behaviour (e.g., smile, frown, head\n  nodding/shaking, agreement/disagreement) in order to create\n  Affectively intelligent AI systems, with a consideration for\n  various ethical issues (e.g., privacy, bias) arising from the\n  real-world deployment of these systems.</p>\n<p>\u00a0</p>\n<h2>Syllabus</h2>\n<p>The following list provides a representative list of\n  topics:</p>\n<ul style=\"list-style-type:square;\">\n<li>Introduction, definitions, and overview</li>\n<li>Theories from\u00a0various disciplines</li>\n<li>Sensing from multiple modalities (e.g., vision, audio, bio\n    signals, text)</li>\n<li>Data acquisition and annotation</li>\n<li>Signal processing / feature extraction</li>\n<li>Learning / prediction / recognition and evaluation</li>\n<li>Behaviour synthesis / generation (e.g., for embodied agents\n    / robots)</li>\n<li>Advanced topics and ethical considerations (e.g., bias and\n    fairness)</li>\n<li>Cross-disciplinary applications (via seminar presentations\n    and discussions)</li>\n<li>Guest lectures (diverse topics \u2013 changes each year)</li>\n<li>\n<p>Hands-on research and programming work (i.e., mini project\n      &amp; report)\u00a0</p>\n<p>\u00a0</p>\n</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students will:</p>\n<ol>\n<li>understand and demonstrate knowledge in\u00a0key\n    characteristics of affectively intelligent AI, which include:\n      <ul>\n<li>Recognition: How to equip Affective AI systems with\n        capabilities of analysing facial expressions, vocal\n        intonations, gestures, and other physiological signals to\n        infer human affective states?</li>\n<li>Generation: How to enable affectively intelligent AI\n        simulate expressions of emotions in machines, allowing them\n        to respond in a more human-like manner, such as virtual\n        agents and humanoid robots, showing empathy or\n        sympathy?</li>\n<li>Adaptation and Personalization: How to enable\n        affectively intelligent AI adapt system responses based on\n        users' affective states and/or needs, or tailor\n        interactions and experiences to individual users based on\n        their expressivity / emotional profiles, personalities, and\n        past interactions?</li>\n<li>Empathetic Communication: How to design Affective AI\n        systems to communicate with users in a way that\n        demonstrates empathy, understanding, and sensitivity to\n        their affective states and needs?</li>\n<li>Ethical and societal considerations: What are the\n        various human differences, ethical guidelines, and societal\n        impacts that need to be considered when designing and\n        deploying Affective AI to ensure that these systems respect\n        users' privacy, autonomy, and well-being?</li>\n</ul>\n</li>\n<li>comprehend and apply (appropriate) methods for collection,\n    analysis, representation, and evaluation of human affective and\n    communicative behavioural data.</li>\n<li>enhance programming skills for creating and implementing\n    (components of)\u00a0Affective AI systems.</li>\n<li>demonstrate critical thinking, analysis and synthesis while\n    deciding on 'when' and 'how' to incorporate\u00a0human affect\n    and social signals in a specific AI system context and gain\n    practical experience in proposing and justifying computational\n    solution(s) of suitable nature and scope.</li>\n</ol>\n<p>\u00a0</p>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Seminar presentation: 20%</li>\n<li>Participating in Q&amp;A and discussions: 10%</li>\n<li>Mid-term mini project report and presentation (as a team of\n    2): 10%\u00a0</li>\n<li>Final mini project report &amp; code (as a team of 2):\n    60%</li>\n</ul>\n<p>\u00a0</p>\n", "course_name": "Affective Artificial Intelligence", "course_code": "L344", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L344", "lecturers": ["hg410"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L108": {"supervisions": 0, "prerequisite_for": ["L118"], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Category theory provides a unified treatment of mathematical\n  properties and constructions that can be expressed in terms of\n  'morphisms' between structures. It gives a precise framework for\n  comparing one branch of mathematics (organized as a category)\n  with another and for the transfer of problems in one area to\n  another. Since its origins in the 1940s motivated by connections\n  between algebra and geometry, category theory has been applied to\n  diverse fields, including computer science, logic and\n  linguistics. This course introduces the basic notions of category\n  theory: adjunction, natural transformation, functor and category.\n  We will use category theory to organize and develop the kinds of\n  structure that arise in models and semantics for logics and\n  programming languages.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction; some history. Definition of category. The\n    category of sets and functions.</li>\n<li>Commutative diagrams. Examples of categories: preorders and\n    monotone functions; monoids and monoid homomorphisms; a\n    preorder as a category; a monoid as a category. Definition of\n    isomorphism. Informal notion of a 'category-theoretic'\n    property.</li>\n<li>Terminal objects. The opposite of a category and the\n    duality principle. Initial objects. Free monoids as initial\n    objects.</li>\n<li>Binary products and coproducts. Cartesian categories.</li>\n<li>Exponential objects: in the category of sets and in\n    general. Cartesian closed categories: definition and\n    examples.</li>\n<li>Intuitionistic Propositional Logic (IPL) in Natural\n    Deduction style. Semantics of IPL in a cartesian closed\n    preorder.</li>\n<li>Simply Typed Lambda Calculus (STLC). The typing relation.\n    Semantics of STLC types and terms in a cartesian closed\n    category (ccc). The internal language of a ccc. The\n    Curry-Howard-Lawvere correspondence.</li>\n<li>Functors. Contravariance. Identity and composition for\n    functors.</li>\n<li>Size: small categories and locally small categories. The\n    category of small categories. Finite products of\n    categories.</li>\n<li>Natural transformations. Functor categories. The category\n    of small categories is cartesian closed.</li>\n<li>Hom functors. Natural isomorphisms. Adjunctions. Examples\n    of adjoint functors. Theorem characterising the existence of\n    right (respectively left) adjoints in terms of a universal\n    property.</li>\n<li>Dependent types. Dependent product sets and dependent\n    function sets as adjoint functors. Equivalence of categories.\n    Example: the category of I-indexed sets and functions is\n    equivalent to the slice category Set/I.</li>\n<li>Presheaves. The Yoneda Lemma. Categories of presheaves are\n    cartesian closed.</li>\n<li>Monads. Modelling notions of computation as monads. Moggi's\n    computational lambda calculus.</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with some of the basic notions of category\n    theory and its connections with logic and programming language\n    semantics</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>a graded exercise sheet (25% of the final mark), and</li>\n<li>a take-home test (75%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Awodey, S. (2010). <em>Category theory</em>. Oxford University\n  Press (2nd ed.).</p>\n<p>Crole, R. L. (1994). <em>Categories for types</em>. Cambridge\n  University Press.</p>\n<p>Lambek, J. and Scott, P. J. (1986). <em>Introduction to higher\n  order categorical logic</em>. Cambridge University Press.</p>\n<p>Pitts, A. M. (2000). <em>Categorical Logic</em>. Chapter 2 of\n  S. Abramsky, D. M. Gabbay and T. S. E. Maibaum (Eds) Handbook of\n  Logic in Computer Science, Volume 5. Oxford University Press.\n  (Draft copy available <a href=\"http://www.cl.cam.ac.uk/~amp12/papers/catl/catl.pdf\">here</a>.)</p>\n<h2>Class Size</h2>\n<p>This module can accommodate upto 15 Part II students plus 15\n  MPhil / Part III students.</p>\n", "course_name": "Category Theory", "course_code": "L108", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L108", "lecturers": ["mpf23"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R160": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Our society is evolving towards digital payments and the\n  elimination of physical cash. Digital alternatives to cash\n  require trade-offs between various properties including\n  unforgeability, traceability, divisibility, transferability\n  without intermediaries, privacy, redeemability, control over the\n  money supply and many more, often in conflict with each other.\n  Since the 1980s, cryptographers have proposed a variety of clever\n  technical solutions addressing some of these issues but it is\n  only with the appearance of Bitcoin, blockchain and a plethora of\n  copycat cryptocurrencies that a new asset class has now emerged,\n  worth in excess of two trillion dollars (although plagued by\n  extreme volatility). Meanwhile, most major countries have been\n  planning the introduction of Central Bank Digital Currencies in\n  an attempt to retain control. This seminar-style interactive\n  module encourages the students to understand what's happening,\n  what the underlying problems and opportunities are (technical,\n  social and economic) and where we should go from here. We are at\n  a historical turning point where an enterprising candidate might\n  disrupt the status quo and truly make a difference.</p>\n<h2>Syllabus</h2>\n<ol>\n<li>Understanding money: from gold standard to digital\n    cash</li>\n<li>Intro to the Decentralised Finance (DeFi) ecosystem:\n    Bitcoin, blockchain, wallets, smart contracts</li>\n<li>Stablecoins and Central Bank Digital Currencies</li>\n<li>Decentralised Autonomous Organisations and Automated Market\n    Makers</li>\n<li>Yield Farming, Perpetual Futures and Maximal Extractable\n    Value</li>\n<li>Web3, Non Fungible Tokens</li>\n<li>Cryptocurrency crashes; crimes facilitated by\n    cryptocurrencies</li>\n<li>New areas of development. Where from here?</li>\n</ol>\n<p>The module is structured as a reading club with a high degree\n  of interactivity. Aside from the first and last session, which\n  are treated differently, the regular two-hour sessions have the\n  following structure (not necessarily in this order).</p>\n<ul style=\"list-style-type:disc;\">\n<li>Two half-hour debates on each of two papers</li>\n<li>Two rapid-fire presentations on open questions previously\n    assigned by the lecturers.</li>\n<li>Half an hour of presentation by the lecturers.</li>\n<li>About ten minutes of buffer that can account for switchover\n    time or be absorbed into the lecturers' presentation.</li>\n</ul>\n<p>The first session is scene-setting by the lecturers, with no\n  student presentations. The last session has no paper debates,\n  only one rapid-fire presentation on an open question from each of\n  the students.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students will gain an in-depth\n  understanding of digital money alternatives as well as many\n  modern applications and components of Decentralised Finance. By\n  comparing DeFi with current processes in traditional banking,\n  students will be able to discuss the merits and limitations of\n  these new technologies as well as to identify potential new areas\n  of development.</p>\n<p>Through having to write, present and defend very brief\n  extended abstracts, the students will improve their communication\n  skills and in particular the ability to distil and convey the\n  most important points forcefully and concisely.</p>\n<h2><br/>\n  Assessment</h2>\n<p>The module is an interactive reading club. Each student\n  produces four output triplets throughout the course. Each output\n  triplet consists of three parts: a 1200-word essay, a 200-word\n  extended abstract and a 7-minute presentation.</p>\n<p>The essay and abstract must be submitted in advance, using the\n  LaTeX templates provided. The abstract must consist of full\n  sentences, not bullet points, and must fit on a single slide.\n  During the presentation, the slide with the abstract will be\n  projected on screen as the only visual aid; the essay will not be\n  accessible to either presenter or audience. Reading out the slide\n  verbatim will obviously not count as a great presentation.\n  Presenters must be able to expand and defend their ideas live,\n  and answer questions from lecturers and audience.</p>\n<p>The essays, the abstracts and the presentations are graded\n  separately, each out of 10, and assigned weights of 60%, 20%,\n  20%, respectively, within the triplet.</p>\n<p>In a regular 2-hour session (all but the first and last) there\n  will be two paper slots and two investigation slots, plus a\n  lecturer slot, described next.</p>\n<p>Each paper slot (30 min) involves a \u201csupporter\u201d and a\n  \u201cdissenter\u201d for the paper, each contributing one output triplet,\n  and then a panel Q&amp;A where all the other participants quiz\n  the supporter and dissenter.</p>\n<p>Each investigation slot (10 min) involves a single student\n  contributing an output triplet on a given theme.</p>\n<p>Roles, papers and themes are assigned by the lecturers the\n  previous week.</p>\n<p>In the first session there will be no student presentations,\n  only lecturer-led exploration. In the last session there will be\n  12 investigation slots and no paper slots.</p>\n<p>Overall, each student will be supporter once, dissenter once\n  and investigator twice, for a total of 4 output triplets. The\n  output triplets of the last session will be weighed twice as much\n  as the others, i.e. 40% each, while the others 20% each.</p>\n<p>Attendance is required at all sessions. Missing a session in\n  which the candidate was due to present will result in the loss of\n  the corresponding presentation marks, while the written work will\n  still have to be submitted and will be marked as usual. Missing a\n  session in which the candidate was not due to present will result\n  in the loss of 3%.</p>\n<h2>Recommended reading</h2>\n<p>There isn't a textbook covering all the material in this\n  course. The following textbook, also adopted by R47 Distributed\n  Ledger Technologies (also freely available online), is a thorough\n  introduction to Bitcoin and Blockchain, but we will complement it\n  with research papers and industry white papers for each\n  lecture.</p>\n<p>Narayanan, A. , Bonneau, J., Felten, E., Miller, A. and\n  Goldfeder, S. (2016). Bitcoin and Cryptocurrency Technologies: A\n  Comprehensive Introduction. Princeton University Press.<br/>\n  (2016 Draft available here:\n  https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf)</p>\n", "course_name": "Digital Money and Decentralised Finance", "course_code": "R160", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R160", "lecturers": ["fms27"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L314": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course teaches basic signal-processing principles\n  necessary to understand many modern high-tech systems, with\n  examples from audio processing, image coding, radio\n  communication, radar, and software-defined radio. Students will\n  gain practical experience from numerical experiments in\n  programming assignments (in Julia, MATLAB or NumPy).</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Signals and systems.</strong>\u00a0Discrete\n    sequences and systems: types and properties. Amplitude, phase,\n    frequency, modulation, decibels, root-mean square. Linear\n    time-invariant systems, convolution. Some examples from\n    electronics, optics and acoustics.</li>\n<li><strong>Phasors.</strong>\u00a0Eigen functions of linear\n    time-invariant systems. Review of complex arithmetic. Phasors\n    as orthogonal base functions.</li>\n<li><strong>Fourier transform.</strong>\u00a0Forms and\n    properties of the Fourier transform. Convolution theorem. Rect\n    and sinc.</li>\n<li><strong>Dirac\u2019s delta function.</strong>\u00a0Fourier\n    representation of sine waves, impulse combs in the time and\n    frequency domain. Amplitude-modulation in the frequency\n    domain.</li>\n<li><strong>Discrete sequences and\n    spectra.</strong>\u00a0Sampling of continuous signals, periodic\n    signals, aliasing, interpolation, sampling and reconstruction,\n    sample-rate conversion, oversampling, spectral inversion.</li>\n<li><strong>Discrete Fourier\n    transform.</strong>\u00a0Continuous\u00a0<i>versus</i>\u00a0discrete\n    Fourier transform, symmetry, linearity, FFT, real-valued FFT,\n    FFT-based convolution, zero padding, FFT-based resampling,\n    deconvolution exercise.</li>\n<li><strong>Spectral estimation.</strong>\u00a0Short-time\n    Fourier transform, leakage and scalloping phenomena, windowing,\n    zero padding. Audio and voice examples. DTFM exercise.</li>\n<li><strong>Finite impulse-response\n    filters.</strong>\u00a0Properties of filters, implementation\n    forms, window-based FIR design, use of frequency-inversion to\n    obtain high-pass filters, use of modulation to obtain band-pass\n    filters.</li>\n<li><strong>Infinite impulse-response\n    filters.</strong>\u00a0Sequences as\n    polynomials,\u00a0<i>z</i>-transform, zeros and poles, some\n    analog IIR design techniques (Butterworth, Chebyshev I/II,\n    elliptic filters, second-order cascade form).</li>\n<li><strong>Band-pass signals.</strong>\u00a0Band-pass sampling\n    and reconstruction, IQ up and down conversion, superheterodyne\n    receivers, software-defined radio front-ends, IQ representation\n    of AM and FM signals and their demodulation.</li>\n<li><strong>Digital\n    communication.</strong>\u00a0Pulse-amplitude modulation.\n    Matched-filter detector. Pulse shapes, inter-symbol\n    interference, equalization. IQ representation of ASK, BSK, PSK,\n    QAM and FSK signals.\u00a0[2 hours]</li>\n<li><strong>Random sequences and noise.</strong>\u00a0Random\n    variables, stationary and ergodic processes, autocorrelation,\n    cross-correlation, deterministic cross-correlation sequences,\n    filtered random sequences, white noise, periodic\n    averaging.</li>\n<li><strong>Correlation coding.</strong>\u00a0Entropy, delta\n    coding, linear prediction,\n    dependence\u00a0<i>versus</i>\u00a0correlation, random vectors,\n    covariance, decorrelation, matrix diagonalization, eigen\n    decomposition, Karhunen-Lo\u00e8ve transform, principal component\n    analysis. Relation to orthogonal transform coding using fixed\n    basis vectors, such as DCT.</li>\n<li><strong>Lossy versus lossless\n    compression.</strong>\u00a0What information is discarded by\n    human senses and can be eliminated by encoders? Perceptual\n    scales, audio masking, spatial resolution, colour coordinates,\n    some demonstration experiments.</li>\n<li><strong>Quantization, image coding\n    standards.</strong>\u00a0Uniform and logarithmic quantization,\n    A/\u00b5-law coding, dithering, JPEG.</li>\n</ul>\n<h2>Objectives</h2>\n<ul>\n<li>apply basic properties of time-invariant linear\n    systems;</li>\n<li>understand sampling, aliasing, convolution, filtering, the\n    pitfalls of spectral estimation;</li>\n<li>explain the above in time and frequency domain\n    representations;</li>\n<li>use filter-design software;</li>\n<li>visualize and discuss digital filters in\n    the\u00a0<i>z</i>-domain;</li>\n<li>use the FFT for convolution, deconvolution, filtering;</li>\n<li>implement, apply and evaluate simple DSP applications;</li>\n<li>familiarity with a number of signal-processing concepts\n    used in digital communication systems</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Lyons, R.G. (2010).\u00a0<i>Understanding digital signal\n  processing.</i>\u00a0Prentice Hall (3rd ed.).<br/>\n  Oppenheim, A.V. and Schafer, R.W. (2007).\u00a0<i>Discrete-time\n  digital signal processing.</i>\u00a0Prentice Hall (3rd ed.).<br/>\n  Stein, J. (2000).\u00a0<i>Digital signal processing \u2013 a computer\n  science perspective.</i>\u00a0Wiley.</p>\n<h2>Class size</h2>\n<p>This module can accommodate a maximum of 24 students (16 Part\n  II students and 8 MPhil students)</p>\n<h2>Assessment - Part II students</h2>\n<ul>\n<li>Three homework programming assignments, each comprising 20%\n    of the mark</li>\n<li>Written test, comprising 40% of the total mark.</li>\n</ul>\n", "course_name": "Digital Signal Processing", "course_code": "L314", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L314", "lecturers": ["mgk25"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L98": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is a lecture-style course that introduces students to\n  various aspects of the semantics of\u00a0Natural Languages\n  (mainly English):</p>\n<ul>\n<li>Lexical Semantics, with an emphasis on theory and\n    phenomenology\u00a0</li>\n<li>Compositional Semantics</li>\n<li>Discourse and pragmatics-related aspects of semantics</li>\n</ul>\n<h2>Objectives</h2>\n<ul>\n<li>Give an operational definition of what is meant by\n    \u201cmeaning\u201d (for instance, above and\u00a0beyond syntax);</li>\n<li>Name the types of phenomena in language that require\n    semantic consideration, in terms\u00a0of lexical, compositional\n    and discourse/pragmatic aspects, in other words, argue\n    why\u00a0semantics is important;</li>\n<li>Demonstrate an understanding of the basics of various\n    semantic representations,\u00a0including logic-based and\n    graph-based semantic representations, their properties,\n    how\u00a0they are used and why they are important, and how they\n    are different from syntactic\u00a0representations;</li>\n<li>Know how such semantic representations are derived during\n    or after parsing, and how\u00a0they can be analysed and mapped\n    to surface strings;</li>\n<li>Understand applications of semantic representations e.g.\n    reasoning, validation, and\u00a0methods how these are\n    approached.</li>\n<li>When designing NL tasks that clearly require semantic\n    processing (e.g.\u00a0knowledge-based QA), to be aware of and\n    reuse semantic representations and\u00a0algorithms when\n    designing the task, rather than reinventing the wheel.</li>\n</ul>\n<h2>Practical advantages of this course for NLP students</h2>\n<ul>\n<li>Knowledge of underlying semantic effects helps improve NLP\n    evaluation, for instance by\u00a0providing more meaningful\n    error analysis. You will be able to link particular errors\n    to\u00a0design decisions inside your system.</li>\n<li>You will learn methods for better benchmarking of your\n    system, whatever the task may\u00a0be. Supervised ML systems\n    (in particular black-box systems such as Deep Learning)\n    are\u00a0only as clever as the datasets they are based on. In\n    this course, you will learn to design\u00a0datasets so that\n    they are harder to trick without real understanding, or\n    critique existing\u00a0datasets.</li>\n<li>You will be able to design tests for ML systems that better\n    pinpoint which aspects of\u00a0language an end-to-end system\n    has \u201cunderstood\u201d.</li>\n<li>You will learn to detect ambiguity and ill-formed semantics\n    in human-human\u00a0communication. This can serve to write more\n    clearly and logically.</li>\n<li>You will learn about decomposing complex semantics-reliant\n    tasks sensibly so that you\u00a0can reuse the techniques\n    underlying semantic analyzers in a modular way. In this\n    way,\u00a0rather than being forced to treat complex tasks in an\n    end-to-end manner, you will be able\u00a0to profit from partial\n    explanations and a better error analysis already built into\n    the\u00a0system.</li>\n</ul>\n<h2>Syllabus</h2>\n<ol>\n<li>Overview of the course</li>\n<li>Events and semantic role labelling</li>\n<li>Referentiality and coreference resolution</li>\n<li>Truth-conditional semantics</li>\n<li>Graph-based meaning representation</li>\n<li>Compositional semantics</li>\n<li>Context-free Graph Rewriting</li>\n<li>Surface realisation</li>\n<li>Negation and psychological approach to semantics</li>\n<li>Dynamic semantics</li>\n<li>Gricean pragmatics</li>\n<li>Vector space models</li>\n<li>Cross-modality</li>\n<li>Acquisition of semantics</li>\n<li>Diachronic change of semantics</li>\n<li>Summary of the course</li>\n</ol>\n<p>\u00a0</p>\n<h2>Assessment</h2>\n<p>5 take-home exercises worth 20% each:</p>\n<p>Take-home assignment 1 is given in Lecture 4 and due is\n  Lecture 6</p>\n<p style=\"margin-left:36.0pt;\">Students are given 10 English\n  sentences and asked to provide their semantic analysis according\n  to truth-conditions. Students will be expected to return 10\n  logical expressions as their answers. No word limit. Assessment\n  criteria: correctness of the 10 logical expressions; 2 points for\n  each logical expression.</p>\n<p>Take-home assignment 2 is given in Lecture 7 and due is\n  Lecture 9\u00a0</p>\n<p style=\"margin-left:36.0pt;\">Students are given 10 English\n  sentences and asked to provide their syntactico-semantic\n  derivations according to the compositionality principle. Students\n  will be expected to return derivation graphs as their answers. No\n  word limit. Assessment criteria: correctness of the 10 derivation\n  graphs; 2 points for each derivation.</p>\n<p>Take-home assignment 3 is given in Lecture 11 and due is\n  Lecture 13\u00a0</p>\n<p style=\"margin-left:36.0pt;\">All students are assigned with a\n  paper on modelling common ground in dialogue system. Students\n  will receive related but different papers. Each student will\n  write a review of their assigned paper, including a comprehensive\n  summary and their own thoughts. Word limit: 1000 words.\n  Assessment criteria: 15 points on whether a student understands\n  the paper correctly; 5 points on whether a student is able to\n  think critically.</p>\n<p>Take-home assignment 4 is given in Lecture 13 and due is\n  Lecture 15</p>\n<p style=\"margin-left:36.0pt;\">All students are assigned with a\n  paper on language-vision interaction. Students will receive\n  related but different papers. Each student will write a review of\n  their assigned paper, including a comprehensive summary and their\n  own thoughts. Word limit: 1000 words. Assessment criteria: 15\n  points on whether a student understands the paper correctly; 5\n  points on whether a student is able to think critically.</p>\n<p>Take-home assignment 5 is given in Lecture 14 and due is two\n  weeks later.</p>\n<p style=\"margin-left:36.0pt;\">Content: All students are assigned\n  with a paper on bootstrapping language acquisition. All students\n  will receive the same paper. Each student will write a review of\n  the paper, including a comprehensive summary and their own\n  thoughts. Word limit: 1000 words. Assessment criteria: 15 points\n  on whether a student understands the paper correctly; 5 points on\n  whether a student is able to think critically.</p>\n", "course_name": "Introduction to Computational Semantics", "course_code": "L98", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L98", "lecturers": ["ws390"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L95": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide a brief introduction to\n  linguistics for computer scientists and then goes on to cover\n  some of the core tasks in natural language processing (NLP),\n  focussing on statistical parsing of sentences to yield syntactic\n  representations. We will look at how to evaluate parsers and see\n  how well state-of-the-art tools perform given current\n  techniques.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Linguistics for NLP focusing on morphology and syntax</li>\n<li>Grammars and representations</li>\n<li>Statistical and neural parsing</li>\n<li>Treebanks and evaluation</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the basic properties of human languages and be\n    familiar with descriptive and theoretical frameworks for\n    handling these properties;</li>\n<li>understand the design of tools for NLP tasks such as\n    parsing and be able to apply them to text and evaluate their\n    performance;</li>\n</ul>\n<h2>Practical work</h2>\n<ul>\n<li>Week 1-7: There will be non-assessed practical exercises\n    between sessions and during sessions.</li>\n<li>Week 8: Download and apply two parsers to a designated\n    text. Evaluate the performance of the tools quantitatively and\n    qualitatively.</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>There will be one\u00a0presentation worth 10% of the final\n    mark; presentation topics will be allocated at the start of\n    term.</li>\n<li>An assessed practical report\u00a0of not more 8 pages in\n    ACL conference format. It will contribute 90% of the final\n    mark.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Jurafsky, D. and Martin, J. (2008). <em>Speech and Language\n  Processing</em>. Prentice-Hall (2nd ed.). (See also 3rd ed.\n  available online.)</p>\n", "course_name": "Introduction to Natural Language Syntax and Parsing", "course_code": "L95", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L95", "lecturers": ["pjb48"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L50": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Systems research refers to the study of a broad range of\n  behaviours arising from complex system design, including:\n  resource sharing and scheduling; interactions between hardware\n  and software; network topology, protocol and device design and\n  implementation; low-level operating systems; Interconnect,\n  storage and more. This module will:</p>\n<ol>\n<li>Teach performance characteristics and performance\n    measurement methodology and practice through profiling\n    experiments;</li>\n<li>Expose students to real-world systems artefacts evident\n    through different measurement tools;</li>\n<li>Develop scientific writing skills through a series of\n    laboratory reports;</li>\n<li>Provide research skills for characterization and modelling\n    of systems and networks using measurements.</li>\n</ol>\n<h2>Prerequisites</h2>\n<p>It is strongly recommended that students have previously (and\n  successfully) completed an undergraduate networking course -- or\n  have equivalent experience through project or open-source\n  work.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction to performance measurements, performance\n    characteristics [1 lecture]</li>\n<li>Performance measurements tools and techniques [2 lectures +\n    2 lab sessions]</li>\n<li>Reproducible experiments [1 lecture + 1 lab session]</li>\n<li>Common pitfalls in measurements [1 lecture]</li>\n<li>Device and system characterisation [1 lecture + 2 lab\n    sessions]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Describe the objectives of measurements, and what they can\n    achieve;</li>\n<li>Characterise and model a system using measurements;</li>\n<li>Perform reproducible measurements experiments;</li>\n<li>Evaluate the performance of a system using\n    measurements;</li>\n<li>Operate measurements tools and be aware of their\n    limitations;</li>\n<li>Detect anomalies in the network and avoid common\n    measurements pitfalls;</li>\n<li>Write system-style performance evaluations.</li>\n</ul>\n<h2>Practical work</h2>\n<p>Five 2-hour in-classroom labs will ask students to develop and\n  use skills in performance measurements as applied to real-world\n  systems and networking artefacts. Results from these labs (and\n  follow-up work by students outside of the classroom) will by the\n  primary input to lab reports.</p>\n<p>The first three labs will provide an introduction and hands on\n  experience with measurement tools and measurements methodologies,\n  while the last two labs will focus on practical measurements and\n  evaluation of specific platforms. Students may find it useful to\n  work in pairs within the lab, but must prepare lab reports\n  independently. The module lecturer will give a short introductory\n  at the start of each lab, and instructors will be on-hand\n  throughout labs to provide assistance.</p>\n<p>Lab participation is not directly included in the final mark,\n  but lab work is a key input to lab reports that are assessed.\n  Guided lab experiments resulting in practical write ups.</p>\n<h2>Assessment</h2>\n<p>Each student will write two lab reports. The first lab report\n  will summarise the experiments done in the first three labs\n  (20%). The second will be a lab report (5000 words) summarising\n  the evaluation of a device or a system (80%).</p>\n<h2>Recommended reading</h2>\n<p>The following list provides some background to the course\n  materials, but is not mandatory. A reading list, including\n  research papers, will be provided in the course materials.</p>\n<ul>\n<li>George Varghese. Network algorithmics. Chapman and\n    Hall/CRC, 2010.</li>\n<li>Mark Crovella and Balachander Krishnamurthy. Internet\n    measurement: infrastructure, traffic and applications. John\n    Wiley and Sons, Inc., 2006.</li>\n<li>Brendan Gregg. Systems Performance: Enterprise and the\n    Cloud, Prentice Hall Press, Upper Saddle River, NJ, USA,\n    October 2013.</li>\n<li>Raj Jain, The Art of Computer Systems Performance Analysis:\n    Techniques for Experimental Design, Measurement, Simulation,\n    and Modeling, Wiley - Interscience, New York, NY, USA, April\n    1991.</li>\n</ul>\n", "course_name": "Introduction to networking and systems measurements", "course_code": "L50", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L50", "lecturers": ["awm22"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R244": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module provides an introduction to large-scale data\n  processing, optimisation, and the impact on computer system's\n  architecture. Large-scale distributed applications with high\n  volume data processing such as training of machine learning will\n  grow ever more in importance. Supporting the design and\n  implementation of robust, secure, and heterogeneous large-scale\n  distributed systems is essential. To deal with distributed\n  systems with a large and complex parameter space, tuning and\n  optimising computer systems is becoming an important and complex\n  task, which also deals with the characteristics of input data and\n  algorithms used in the applications. Algorithm designers are\n  often unaware of the constraints imposed by systems and the best\n  way to consider these when designing algorithms with massive\n  volume of data. On the other hand, computer systems often miss\n  advances in algorithm design that can be used to cut down\n  processing time and scale up systems in terms of the size of the\n  problem they can address. Integrating machine learning approaches\n  (e.g. Bayesian Optimisation, Reinforcement Learning) for system\n  optimisation will be explored in this course.</p>\n<h2>Syllabus</h2>\n<p>This course provides perspectives on large-scale data\n  processing, including data-flow programming, graph data\n  processing, probabilistic programming and computer\n  system\u00a0optimisation, especially using machine learning\n  approaches, thus providing a solid basis to work on\u00a0the next\n  generation of distributed systems.</p>\n<p>The module consists of 8 sessions, with 5 sessions on specific\n  aspects of large-scale data processing\u00a0research. Each\n  session discusses 3-4 papers, led by the assigned students. One\n  session is a hands-on\u00a0tutorial on on dataflow programming\n  fundamentals. The first session advises on how\n  to\u00a0read/review a paper together with a brief introduction on\n  different perspectives in large-scale data<br/>\n  processing and optimisation. The last session is dedicated to the\n  student presentation of opensource\u00a0project studies.</p>\n<ol>\n<li>Introduction to large-scale data processing and\n    optimisation</li>\n<li>Data flow programming: Map/Reduce to TensorFlow</li>\n<li>Large-scale graph data processing: storage, processing\n    model and parallel processing</li>\n<li>Dataflow programming hands-on tutorial</li>\n<li>Probabilistic Programming</li>\n<li>Optimisations in ML Compiler</li>\n<li>Optimisations of Computer systems using ML</li>\n<li>Presentation of Open Source Project Study</li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Understand key concepts of scalable data processing\n    approaches in future computer systems.</li>\n<li>Obtain a clear understanding of building distributed\n    systems using data centric programming and large-scale data\n    processing.</li>\n<li>Understand a large and complex parameter space in computer\n    system's optimisation and applicability of Machine Learning\n    approach.</li>\n</ul>\n<h2>Coursework</h2>\n<h3>Reading Club:</h3>\n<ul>\n<li>The preparation for the reading club will involve 1-3\n    papers every week. At each session, around 3-4 papers are\n    selected under the given topic, and the students present their\n    review work.</li>\n<li>Hands-on tutorial session of data flow programming\n    including writing an application of processing streaming in\n    Twitter data and/or Deep Neural Networks using Google\n    TensorFlow using cluster computing.</li>\n</ul>\n<h3>Reports</h3>\n<p>The following three reports are required, which could be\n  extended from the assignment of the reading club, within the\n  scope of data centric systems.</p>\n<ol>\n<li>Review report on a full length paper (max 1800 words)\n      <ul>\n<li>Describe the contribution of the paper in depth with\n        criticisms</li>\n<li>Crystallise the significant novelty in contrast to\n        other related work</li>\n<li>Suggestions for future work</li>\n</ul>\n</li>\n<li>Survey report on sub-topic in large-scale data processing\n    and optimisation (max 2000 words)\n      <ul>\n<li>Pick up to 5 papers as core papers in the survey\n        scope</li>\n<li>Read the above and expand reading through related\n        work</li>\n<li>Comprehend the view and finish an own survey paper</li>\n</ul>\n</li>\n<li>Project study and exploration of a prototype (max 2500\n    words)\n      <ul>\n<li>What is the significance of the project in the research\n        domain?</li>\n<li>Compare with similar and succeeding projects</li>\n<li>Demonstrate the project by exploring its prototype</li>\n</ul>\n</li>\n</ol>\n<p>Reports 1 should be handed in by the end of 5th week; Report 2\n  in the 8th week and Report 3 on the first day of Lent Term.</p>\n<h2>Assessment</h2>\n<p>The final grade for the course will be provided as a\n  percentage, and the assessment will consist of two parts:</p>\n<ol>\n<li>25%: for reading club (participation, presentation)</li>\n<li>75%: for the three reports:\n      <ul>\n<li>15%: Intensive review report</li>\n<li>25%: Survey report</li>\n<li>35%: Project study</li>\n</ul>\n</li>\n</ol>\n<h2>Recommended reading</h2>\n<ol>\n<li>M. Abadi et al. TensorFlow: A System for Large-Scale\n    Machine Learning, OSDI, 2016.</li>\n<li>D. Aken et al.: Automatic Database Management System Tuning\n    Through Large-scale Machine Learning, SIGMOD, 2017.</li>\n<li>J. Ansel et al. Opentuner: an extensible framework for\n    program autotuning. PACT, 2014.</li>\n<li>V. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building\n    Auto-Tuners with Structured Bayesian Optimization, WWW,\n    2017.</li>\n<li>J. Dean et al. Large scale distributed deep networks. NIPS,\n    2012.</li>\n<li>G. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N.\n    Leiser, G. Czajkowski. Pregel: A System for Large-Scale Graph\n    Processing, SIGMOD, 2010.</li>\n<li>A. Mirhoseini et al. Device Placement Optimization with\n    Reinforcement Learning, ICML, 2017.</li>\n<li>D. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M.\n    Abadi. Naiad: A Timely Dataflow System, SOSP, 2013.</li>\n<li>M. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki:\n    RLgraph: Modular Computation Graphs for Deep Reinforcement\n    Learning, SysML, 2019.</li>\n<li>Z. Jia, O. Padon, J. Thomas, T. Warszawski, M.\n    Zaharia,\u00a0 A. Aiken: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/jia_SOSP_2019.pdf\">\n<u>TASO: Optimizing Deep Learning Computation with Automated\n    Generation of Graph Substitutions</u></a>: SOSP, 2019.</li>\n<li>H. Mao et al.: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/mao_OR_2019.pdf\">\n<u>Park: An Open Platform for Learning-Augmented Computer\n    Systems</u></a>, OpenReview, 2019.</li>\n<li>J. Shao, et al.: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2023_2024/papers/SHAO_NEURIPS_2022.pdf\">\n<u>Tensor Program Optimization with Probabilistic\n    Programs</u></a>, NeurIPS, 2022.\u00a0</li>\n</ol>\n<p>A complete list can be found on the course material web page.\n  See also 2023-2024 course material on the previous course\n  <a href=\"https://www.cl.cam.ac.uk/teaching/2324/R244/materials.html\">\n  Large-Scale Data Processing and Optimisation</a>.</p>\n", "course_name": "Large-scale data processing and optimisation", "course_code": "R244", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R244", "lecturers": ["ey204"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L48": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The module \u201cMachine Learning and the Physical World\u201d is\n  focused on machine learning systems that interact directly with\n  the real world. Building artificial systems that interact with\n  the physical world have significantly different challenges\n  compared to the purely digital domain. In the real world data is\n  scares, often uncertain and decisions can have costly and\n  irreversible consequences. However, we also have the benefit of\n  centuries of scientific knowledge that we can draw from. This\n  module will provide the methodological background to machine\n  learning applied in this scenario. We will study how we can build\n  models with a principled treatment of uncertainty, allowing us to\n  leverage prior knowledge and provide decisions that can be\n  interrogated.</p>\n<p>There are three principle points about machine learning in the\n  real world that will concern us.</p>\n<ol>\n<li>We often have a mechanistic understanding of the real world\n    which we should be able to bootstrap to make decisions. For\n    example, equations from physics or an understanding of\n    economics.</li>\n<li>Real world decisions have consequences which may have\n    costs, and often these cost functions need to be assimilated\n    into our machine learning system.</li>\n<li>The real world is surprising, it does things that you do\n    not expect and accounting for these challenges requires us to\n    build more robust and or interpretable systems.</li>\n</ol>\n<p>Decision making in the real world hasn\u2019t begun only with the\n  advent of machine learning technologies. There are other domains\n  which take these areas seriously, physics, environmental\n  scientists, econometricians, statisticians, operational\n  researchers. This course identifies how machine learning can\n  contribute and become a tool within these fields. It will equip\n  you with an understanding of methodologies based on uncertainty\n  and decision making functions for delivering on these\n  challenges.</p>\n<h2>Objectives</h2>\n<p>You will gain detailed knowledge of</p>\n<ul>\n<li>surrogate models and uncertainty</li>\n<li>surrogate-based optimization</li>\n<li>sensitivity analysis</li>\n<li>experimental design</li>\n</ul>\n<p>You will gain knowledge of</p>\n<ul>\n<li>counterfactual analysis</li>\n<li>surrogate-based quadrature</li>\n</ul>\n<h2>Schedule</h2>\n<p>Week 1:\u00a0Introduction to the unit and foundation of\n  probabilistic modelling</p>\n<p>Week 2:\u00a0Gaussian processes and probablistic inference</p>\n<p>Week 3:\u00a0Simulation and Sequential decision making under\n  uncertainty</p>\n<p>Week 4:\u00a0Emulation and Experimental Design</p>\n<p>Week 5:\u00a0Sensitivity Analysis and Multifidelity\n  Modelling</p>\n<p>Week 6-8:\u00a0Case studies of applications and Projects</p>\n<h2>Practical work</h2>\n<p>During the first five weeks of the unit we will provide a\n  weekly worksheet that will focus on implementation and practical\n  exploration of the material covered in the lectures. The\n  worksheets will allow you to build up a these methods without\n  relying on extensive external libraries. You are free to use any\n  programming language of choice however we highly recommended the\n  use of =Python=.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Two individual tasks\u00a0(15% each)</li>\n<li>Group Project (70%).\u00a0Each group will work on an\n    application of uncertainty that covers the material of the\n    first 5 weeks of lectures in the unit. Each group will submit a\n    report which will form the basis of the assessment. In addition\n    to the report each group will also attend a short oral\n    examination based on the material covered both in the report\n    and the taught material.</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p>Rasmussen, C. E. and Williams, C. K. I. (2006). <em>Gaussian\n  Processes for Machine Learning</em>. MIT Press</p>\n<p>Bishop, C. (2006). <em>Pattern recognition and machine\n  learning</em>. Springer.<br/>\n<a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\" target=\"_blank\">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>\n<p>Laplace, P. S. (1902). <em>A Philosophical Essay on\n  Probabilities</em>. John Wiley &amp; Sons.<br/>\n<a href=\"https://archive.org/details/philosophicaless00lapliala\" target=\"_blank\">https://archive.org/details/philosophicaless00lapliala</a></p>\n", "course_name": "Machine Learning and the Physical World", "course_code": "L48", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L48", "lecturers": ["ndl21"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L335": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims at introducing the theoretical foundations\n  and practical techniques for machine perception, the capability\n  of computers to interpret data resulting from sensor\n  measurements. The course will teach the fundamentals and modern\n  techniques of machine perception, i.e. reconstructing the real\n  world starting from sensor measurements with a focus on machine\n  perception for visual data. The topics covered will be\n  image/geometry representations for machine perception, semantic\n  segmentation, object detection and recognition, geometry capture,\n  appearance modeling and acquisition, motion detection and\n  estimation, human-in-the-loop machine perception, select topics\n  in applied machine perception.</p>\n<p>Machine perception/computer vision is a rapidly expanding area\n  of research with real-world applications. An understanding of\n  machine perception is also important for robotics, interactive\n  graphics (especially AR/VR), applied machine learning, and\n  several other fields and industries. This course will provide a\n  fundamental understanding of and practical experience with the\n  relevant techniques.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Students will understand the theoretical underpinnings of\n    the modern machine perception techniques for reconstructing\n    models of reality starting from an incomplete and imperfect\n    view of reality.</li>\n<li>Students will be able to apply machine perception theory to\n    solve practical problems, e.g. classification of images,\n    geometry capture.</li>\n<li>Students will gain an understanding of which machine\n    perception techniques are appropriate for different tasks and\n    scenarios.</li>\n<li>Students will have hands-on experience with some of these\n    techniques via developing a functional machine perception\n    system in their projects.</li>\n<li>Students will have practical experience with the current\n    prominent machine perception frameworks.</li>\n</ul>\n<h2>Syllabus</h2>\n<ul>\n<li>The fundamentals of machine learning for machine\n    perception</li>\n<li>Deep neural networks and frameworks for machine\n    perception</li>\n<li>Semantic segmentation of objects and humans</li>\n<li>Object detection and recognition</li>\n<li>Motion estimation, tracking and recognition</li>\n<li>3D geometry capture</li>\n<li>Appearance modeling and acquisition</li>\n<li>Select topics in applied machine perception</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>A practical exercise, worth 20% of the mark. This will\n    cover the basics and theory of machine perception and some of\n    the practical techniques the students will likely use in their\n    projects. This is individual work. No GPU hours will be needed\n    for the practical work.</li>\n</ul>\n<ul>\n<li>A machine perception project worth 80% of the marks:\n      <ul>\n<li>Course projects will be selected by the students\n        following the possible project\u00a0themes proposed by the\n        lecturer,\u00a0and will be checked by the lecturer for\n        appropriateness.\u00a0</li>\n</ul>\n<ul>\n<li>The students will form groups of 2-3 to design,\n        implement, report, and present a project to tackle a given\n        task in machine perception.</li>\n</ul>\n<ul>\n<li>The final mark will be composed of an\n        implementation/report mark (60%) and a presentation mark\n        (20%). Each team member will be evaluated based on her/his\n        contribution.</li>\n</ul>\n<ul>\n<li>Each project will have extensions to be completed only\n        by the ACS students. Each student will write a different\n        part of the report, whose author will be clearly marked.\n        Each student will further summarise her/his contributions\n        to the project in the same report.</li>\n</ul>\n</li>\n</ul>\n<h2>Recommended Reading List</h2>\n<ul>\n<li><em>Computer Vision: Algorithms and Applications, Richard\n    Szeliski, Springer, 2010.</em></li>\n<li><em>Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron\n    Courville, MIT Press, 2016.</em></li>\n<li><em>Machine Learning and Visual Perception, Baochang Zhang,\n    De Gruyter, 2020.</em></li>\n</ul>\n", "course_name": "Machine Visual Perception", "course_code": "L335", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L335", "lecturers": ["cpt23"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R269": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to introduce the latest research advancements\n  in mobile systems and mobile data machine learning, spanning a\n  range of domains including systems, data gathering, analytics and\n  machine learning, on device machine learning and applications\n  such as health, transportation, behavior monitoring,\n  cyber-physical systems, autonomous vehicles, drones. The course\n  will cover current and seminal research papers in the area of\n  research.</p>\n<h2>Syllabus</h2>\n<p>The course will consist of one introductory lecture, seven\n  two-hour, and one three-hour, sessions covering a variety of\n  topics roughly including the following material (some variation\n  in the topics might happen from year to year):</p>\n<ol>\n<li>System, Energy and Security</li>\n<li>Backscatter Communication, Battery Free and Energy\n    Harvesting Devices</li>\n<li>New Sensing Modalities</li>\n<li>Machine Learning on Wearable Data</li>\n<li>On Device Machine Learning</li>\n<li>Mobile and Wearable Health</li>\n<li>Mobile and Wearable Systems of Sustainability</li>\n</ol>\n<p>Each week, three class participants will be assigned to\n  introduce assigned three papers via 20-minute presentations,\n  conference-style and highlighting critically its features. Each\n  presentation will be followed by 10 minutes of questions. This\n  will be followed by 10 minutes of general discussion. Slides will\n  be used for presentation.</p>\n<p>Students will give one or more presentations each term. Each\n  student will submit a paper review each week for one of the three\n  papers presented except for the week they will be presenting\n  slides. Each review will follow a template and be up to 1,000\n  words. Each review will receive a maximum of 10 points. As a\n  result, each student will produce 6-7 reviews and at least one\n  presentation, probably two. All participants are expected to\n  attend and participate in every class; the instructor must be\n  notified of any absences in advance.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should have an\n  understanding of the recent key research in mobile and sensor\n  systems and mobile analytics as well as an improved critical\n  thinking over research papers.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Aggregate mark for 7 assignments from 6-7 reports and 1-2\n    presentations. Each report or presentation will contribute one\n    seventh of the course mark.</li>\n<li>A tick for presence and participation to each class will\n    also be awarded.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Readings from the most recent conferences such as AAAI, ACM\n  KDD, ACM MobiCom, ACM MobiSys, ACM SenSys, ACM UbiComp, ICLR,\n  ICML Neurips, and WWW pertinent to mobile systems and data.</p>\n", "course_name": "Mobile, Wearable Systems and Machine Learning", "course_code": "R269", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R269", "lecturers": ["cm542", "yl868"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R02": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide the world with more network\n  architects. The 2011-2012 version was oriented around the\n  evolution of IP to support new services like multicast, mobility,\n  multihoming, pub/sub and, in general, data oriented networking.\n  The course is a <em>paper reading</em> which puts the onus on the\n  student to do the work.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>IPng [2 lectures, Jon Crowcroft]</li>\n<li>New Architectures [2 lectures, Jon Crowcroft]</li>\n<li>Multicast [2 lectures, Jon Crowcroft]</li>\n<li>Content Distribution and Content Centric Networks [2\n    lectures, Jon Crowcroft]</li>\n<li>Resource Pooling [2 lectures, Jon Crowcroft]</li>\n<li>Green Networking [2 lectures, Jon Crowcroft]</li>\n<li>Alternative Router Implementions [2 lectures, Jon\n    Crowcroft]</li>\n<li>Data Center Networks [2 Lectures, Jon Crowcroft]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should be able to:</p>\n<ul>\n<li>contribute to new network system designs;</li>\n<li>engineer evolutionary changes in network systems;</li>\n<li>identify and repair architectural design flaws in networked\n    systems;</li>\n<li>see that there are no perfect solutions (aside from\n    academic ones) for routing, addressing, naming;</li>\n<li>understand tradeoffs in modularisation and other pressures\n    on clean software systems implementation, and see how the world\n    is changing the proper choices in protocol layering, or non\n    layered or cross-layered.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Assessment is through three graded essays (each chosen\n  individually from a number of suggested or student-chosen\n  topics), as follows:</p>\n<ol>\n<li>Analysis of two different architectures for a particular\n    scenario in terms of cost/performance tradeoffs for some\n    functionality and design dimension, for example:\n      <ul>\n<li>ATM \u2013 e.g. for hardware <em>versus</em> software\n        tradeoff</li>\n<li>IP \u2013 e.g. for mobility, multi-homing, multicast,\n        multipath</li>\n<li>3GPP \u2013 e.g. for plain complexity <em>versus</em>\n        complicatedness</li>\n</ul>\n</li>\n<li>A discursive essay on a specific communications systems\n    component, in a particular context, such as <em>ad hoc</em>\n    routing, or wireless sensor networks.</li>\n<li>A bespoke network design for a narrow, well specified\n    specialised target scenario, for example:\n      <ul>\n<li>A customer baggage tracking network for an\n        airport.</li>\n<li>in-flight entertainment system.</li>\n<li>in-car network for monitoring and control.</li>\n<li>inter-car sensor/control network for automatic\n        highways.</li>\n</ul>\n</li>\n</ol>\n<h2>Practical work</h2>\n<p>This course does not feature any implementation work due to\n  time constraints.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Three 1,200-word essays (worth 25% each), and</li>\n<li>an annotated bibliography (25%).</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Pre-course reading:</p>\n<p>Keshav, S. (1997). <em>An engineering approach to computer\n  networking</em>. Addison-Wesley (1st ed.). ISBN 0201634422<br/>\n  Peterson, L.L. and Davie, B.S. (2007). <em>Computer networks: a\n  systems approach</em>. Morgan Kaufmann (4th ed.).</p>\n<p>Design patterns:</p>\n<p>Day, John (2007). <em>Patterns in network architecture: a\n  return to fundamentals</em>. Prentice Hall.</p>\n<p>Example systems:</p>\n<p>Krishnamurthy, B. and Rexford, J. (2001). <em>Web protocols\n  and practice: HTTP/1.1, Networking protocols, caching, and\n  traffic measurement</em>. Addison-Wesley.</p>\n<p>Economics and networks:</p>\n<p>Frank, Robert H. (2008). <em>The economic naturalist: why\n  economics explains almost everything</em>.</p>\n<p>Papers:</p>\n<p>Certainly, a collection of papers (see <a href=\"http://ccr.sigcomm.org/\">ACM CCR</a> which publishes notable\n  network researchers' favourite ten papers every 6 months or\n  so).</p>\n", "course_name": "Network Architectures", "course_code": "R02", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R02", "lecturers": ["jac22"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L390": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course introduces the fundamental techniques of natural\n  language processing. It aims to explain the potential and the\n  main limitations of these techniques. Some current research\n  issues are introduced and some current and potential applications\n  discussed and evaluated. Students will also be introduced to\n  practical experimentation in natural language processing.</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Overview.</strong>\u00a0Brief history\n    of\u00a0NLP\u00a0research, some current applications,\n    components of\u00a0NLP\u00a0systems.</li>\n<li><strong>Morphology and Finite State\n    Techniques.</strong>\u00a0Morphology in different languages,\n    importance of morphological analysis in\u00a0NLP, finite-state\n    techniques in\u00a0NLP.</li>\n<li><strong>Part-of-Speech Tagging and Log-Linear\n    Models.</strong>\u00a0Lexical categories, word tagging, corpora\n    and annotations, empirical evaluation.</li>\n<li><strong>Phrase Structure and Structure\n    Prediction.</strong>\u00a0Phrase structures, structured\n    prediction, context-free grammars, weights and probabilities.\n    Some limitations of context-free grammars.</li>\n<li><strong>Dependency Parsing.</strong>\u00a0Dependency\n    structure, grammar-free parsing,\n    incremental\u00a0processing.\u00a0</li>\n<li><strong>Gradient Descent and Neural\n    Nets.</strong>\u00a0Parameter optimisation by gradient descent.\n    Non-linear functions with neural network layers. Log-linear\n    model as softmax layer. Current findings of\n    Neural\u00a0NLP.</li>\n<li><strong>Word representations</strong>. Representing words\n    with vectors, count-based\u00a0and prediction-based approaches,\n    similarity metrics.</li>\n<li><strong>Recurrent Neural Networks.</strong>\u00a0Modelling\n    sequences, parameter sharing in recurrent neural networks,\n    neural\u00a0language\u00a0models, word prediction.</li>\n<li><strong>Compositional Semantics.</strong>\u00a0Logical\n    representations, compositional semantics, lambda calculus,\n    inference and robust entailment.</li>\n<li><strong>Lexical Semantics.</strong>\u00a0Semantic\n    relations, WordNet, word senses.</li>\n<li><strong>Discourse.</strong>\u00a0Discourse relations,\n    anaphora resolution, summarization.</li>\n<li>\n<strong>Natural\u00a0Language\u00a0Generation.</strong>\u00a0Challenges\n    of\u00a0natural\u00a0language\u00a0generation (NLG), tasks in\n    NLG, surface\u00a0realisation.</li>\n<li><strong>Practical and assignments.</strong>\u00a0Students\n    will build\n    a\u00a0natural\u00a0language\u00a0processing\u00a0system which\n    will be trained and evaluated on supplied data. The system will\n    be built from existing components, but students will be\n    expected to compare approaches and\n    some\u00a0programming\u00a0will be required for this. Several\n    assignments will be set during the practicals for\n    assessment.</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>be able to discuss the current and likely future\n    performance of several NLP applications;</li>\n<li>be able to describe briefly a fundamental technique for\n    processing language for several subtasks, such as morphological\n    processing, parsing, word sense disambiguation etc.;</li>\n<li>understand how these techniques draw on and relate to other\n    areas of computer science.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Jurafsky, D. &amp; Martin, J. (2023).\u00a0<i>Speech and\n  language processing</i>. Prentice Hall (3rd ed. draft, <a href=\"https://web.stanford.edu/~jurafsky/slp3/\">online</a>).</p>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Assignment 1 - 10% of marks</li>\n<li>Assignment 2 - 60% of marks</li>\n<li>Assignment 3 - 30% of marks</li>\n</ul>\n", "course_name": "Overview of Natural Language Processing", "course_code": "L390", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L390", "lecturers": ["ws390"], "lectures": 18, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "P342": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is an advanced course in human-computer interaction, with\n  a specialist focus on intelligent user interfaces and interaction\n  with machine-learning and artificial intelligence technologies.\n  The format will be largely Practical, with students carrying out\n  a mini-project involving empirical research investigation. These\n  studies will investigate human interaction with some kind of\n  model-based system for planning, decision-making, automation etc.\n  Possible study formats might include: System evaluation, Field\n  observation, Hypothesis testing experiment, Design intervention\n  or Corpus analysis, following set examples from recent research\n  publications. Project work will be formally evaluated through a\n  report and presentation.</p>\n<h2>Lectures</h2>\n<p>(note that Lectures 2-7 also include one hour class discussion\n  of practical work)<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Current research\n  themes in intelligent user interfaces<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Program\n  synthesis<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Mixed initiative\n  interaction<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Interpretability /\n  explainable AI<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Labelling as a\n  fundamental problem<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Machine learning\n  risks and bias<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Visualisation and\n  visual analytics<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Student research\n  presentations</p>\n<h2>Objectives</h2>\n<p>By the end of the course students should:<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be familiar with\n  current state of the art in intelligent interactive systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 understand the human\n  factors that are most critical in the design of such systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to evaluate\n  evidence for and against the utility of novel systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 have experience of\n  conducting user studies meeting the quality criteria of this\n  field<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to write up\n  and present user research in a professional manner</p>\n<h2>Class Size</h2>\n<p>This module can accommodate upto 20 Part II, Part III and\n  MPhil students.</p>\n<h2>Recommended reading</h2>\n<p>Brad A. Myers and Richard McDaniel (2000). <a href=\"http://web.media.mit.edu/~lieber/Your-Wish/03-Myers.pdf\">Demonstrational\n  Interfaces: Sometimes You Need a Little Intelligence, Sometimes\n  You Need a Lot</a>.</p>\n<p>Alan Blackwell (2024). <a href=\"https://moralcodes.pubpub.org\">Moral Codes:\u00a0Designing\n  alternatives to AI</a></p>\n<h2>Assessment - Part II Students</h2>\n<p>The format will be largely practical, with students carrying\n  out an individual mini-project involving empirical research\n  investigation.</p>\n<p>Assignment 1: six incremental submissions which\u00a0together\n  contribute 20% to the final module mark.</p>\n<p>Assignment 2: Final report - 80% of the final module mark<br/>\n  \u00a0</p>\n", "course_name": "Practical Research in Human-centred AI", "course_code": "P342", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/P342", "lecturers": ["afb21"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L46": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course will examine the emerging principles and\n  methodologies that underpin scalable and efficient machine\n  learning systems. Primarily, the course will focus on an exciting\n  cross-section of algorithms and system techniques that are used\n  to support the training and inference of machine learning models\n  under a spectrum of computing systems that range from constrained\n  embedded systems up to large-scale distributed systems. It will\n  also touch upon the new engineering practices that are developing\n  in support of such systems at scale. When needed to appreciate\n  issues of scalability and efficiency, the course will drill down\n  to certain aspects of computer architecture, systems software and\n  distributed systems and explore how these interact with the usage\n  and deployment of state-of-the-art machine learning.</p>\n<h2>Syllabus</h2>\n<p>Topics covered may include the following, with confirmation a\n  month before the course begins:</p>\n<ul>\n<li>System Performance Trade-offs</li>\n<li>Distributed Learning Algorithms\u00a0</li>\n<li>Model Compression\u00a0</li>\n<li>Deep Learning Compilers\u00a0</li>\n<li>Frameworks and Run-times\u00a0</li>\n<li>Scalable Inference Serving\u00a0</li>\n<li>Development Practices\u00a0</li>\n<li>Automated Machine Learning\u00a0</li>\n<li>Federated Learning\u00a0</li>\n</ul>\n<p>Primarily, topics are covered with conventional lectures.\n  However, where appropriate, material will be delivered through\n  hands-on lab tutorials. Lab tutorials will make use of hardware\n  including ARM microcontrollers and multi-GPU machines to explore\n  forms of efficient machine learning (any necessary equipment will\n  be provided to students)</p>\n<h2>Assessment</h2>\n<p>Each student will be assessed on 3 labs which will be worth\n  30% of their grade. They will also undertake a written project\n  report which will be worth\u00a070% of the grade. This report\n  will detail an investigation into a particular aspect of machine\n  learning systems, this report will be made available\n  publicly.</p>\n", "course_name": "Principles of Machine Learning Systems", "course_code": "L46", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L46", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L81": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module introduces students to interactive theorem proving\n  using Isabelle and Coq. It includes techniques for specifying\n  formal models of software and hardware systems and for deriving\n  properties of these models.</p>\n<p>\u00a0</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction to proof assistants and logic.</li>\n<li>Reasoning in predicate logic and typed set theory.</li>\n<li>Reasoning in dependent type theory.</li>\n<li>Inductive definitions and recursive functions: modelling\n    them in logic, reasoning about them.</li>\n<li>Modelling operational semantics definitions and proving\n    properties.</li>\n</ul>\n<p>\u00a0</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>possess good skills in the use of Isabelle and Coq;</li>\n<li>be able to specify inductive definitions and perform proofs\n    by induction;</li>\n<li>be able to formalise and reason about a variety of\n    specifications in a proof assistant.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Practical sessions will allow students to develop skills. Some\n  of the exercises will serve as the basis for assessment.</p>\n<h2>Assessment</h2>\n<p>Assessment will be based on two marked assignments (due in\n  weeks 5 and 8) with students performing small formalisation and\n  verification projects in a proof assistant together with a\n  write-up explaining their work (word limit 2,500 per project for\n  the write-up). Each of the assignments will be worth 100 marks,\n  distributed as follows:</p>\n<ul>\n<li>50 marks for completing basic formalisation and\n    verification tasks assessing grasp of the material taught in\n    the lecture. Students will submit their work as theory files\n    for either the Isabelle or Coq proof assistant, and they will\n    be assessed for correctness and completeness of the\n    specifications and proofs.</li>\n<li>20 marks for completing designated more challenging tasks,\n    requiring the use of advanced techniques or creative proof\n    strategies.</li>\n<li>30 marks for a clear write-up explaining the design\n    decisions made during the formalisation and the strategy used\n    for the proofs, where 10 of these marks will be reserved for\n    write-ups of exceptional quality, e.g. demonstrating particular\n    insight.</li>\n</ul>\n<p>The main tasks in the assignments will be designed to assess\n  the student's proficiency with the basic material and techniques\n  taught in the lectures and the practical sessions, while the more\n  challenging tasks will give exceptional students the opportunity\n  to earn distinction marks</p>\n<h2>Recommended reading</h2>\n<p>Nipkow, T., Klein, G. (2014). <i>Concrete Semantics with\n  Isabelle/HOL</i>. (The first part of this book, \u201cProgramming and\n  Proving in Isabelle/HOL\u201d, comes with the Isabelle\n  distribution.)\u00a0</p>\n<p>Nipkow, T., Paulson, L.C. and Wenzel, M. (2002). <i>A proof\n  assistant for higher-order logic.</i> Springer LNCS\n  2283.\u00a0</p>\n<p>The Software Foundations series of books, in particular the\n  first two:</p>\n<ul>\n<li>Pierce, B., Azevedo de Amorim, A., Casinghino, C.,\n    Gaboardi, M., Greenberg, M., Hri\u0163cu, C., Sj\u00f6berg, V., Yorgey,\n    B. (2023). <i>Logical Foundations</i>\u00a0</li>\n<li>Pierce, B., Azevedo de Amorim, A., Casinghino, C.,\n    Gaboardi, M., Greenberg, M., Hri\u0163cu, C., Sj\u00f6berg, V., Tolmach,\n    A., Yorgey, B. (2023). <i>Programming Language\n    Foundations\u00a0</i></li>\n</ul>\n<p>Chlipala, A. (2022). <i>Certified programming with dependent\n  types: a pragmatic introduction to the Coq proof assistant.</i>\n  MIT Press.\u00a0</p>\n<p>Sergey, I. (2014). <i>Programs and Proofs: Mechanizing\n  Mathematics with Dependent Types.</i>\u00a0</p>\n<p>All of these are freely available online.</p>\n", "course_name": "Proof Assistants", "course_code": "L81", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L81", "lecturers": ["tb592", "mgapb2", "pes20"], "lectures": null, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L130": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module is a research-focused introduction to the theory\n  of quantum computing. The aim is to prepare the students to\n  conduct research in quantum algorithms and quantum complexity\n  theory.</p>\n<h2>Syllabus</h2>\n<p>Topics will vary from year to year, based on developments in\n  cutting edge research. Representative topics include:</p>\n<p>Quantum algorithms:</p>\n<ul>\n<li>Quantum learning theory</li>\n<li>Quantum property testing</li>\n<li>Shadow tomography</li>\n<li>Quantum walks</li>\n<li>Quantum state/unitary synthesis.</li>\n<li>Structure vs randomness in quantum algorithms</li>\n</ul>\n<p>Quantum complexity theory:</p>\n<ul>\n<li>The quantum PCP conjecture</li>\n<li>Entanglement and MIP</li>\n<li>State complexity, AdS/CFT, and quantum gravity</li>\n<li>Quantum locally testable codes</li>\n<li>Pseudorandom states and unitaries</li>\n<li>Quantum zero-knowledge proofs</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with contemporary quantum algorithms</li>\n<li>develop an understanding of the power and limitations of\n    quantum computation</li>\n<li>conduct research in quantum algorithms and complexity\n    theory</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>Mini research project (70%)</li>\n<li>Presentation (20%)</li>\n<li>Attendance and participation (10%)</li>\n</ul>\n<p>Timelines for the assignment submissions:\u00a0</p>\n<ul>\n<li>submit questions/observations on a weekly basis (i.e.,\n    Weeks 2-8)</li>\n<li>deliver talks in Weeks 5-8</li>\n<li>submit their mini-project at the end of term</li>\n</ul>\n<h2>Recommended reading material and resources</h2>\n<p>\u201cQuantum Computing Since Democritus\u201d by Scott Aaronson</p>\n<p>\u201cIntroduction to Quantum Information Science\u201d by Scott\n  Aaronson</p>\n<p>\u201cQuantum Computing: Lecture Notes\u201d by Ronald de Wolf</p>\n<p>\u201cQuantum Computation and Quantum Information\u201d by Nielsen and\n  Chuang</p>\n", "course_name": "Quantum Algorithms and Complexity", "course_code": "L130", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L130", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L132": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course covers the architecture of a practical-scale\n  quantum computer. We will examine the resource requirements of\n  practical quantum applications, understand the different layers\n  of the quantum stack, the techniques used in these layers and\n  examine how these layers come together to enable practical\n  quantum advantage over classical computing.</p>\n<h2>Syllabus</h2>\n<p>The course has two parts: a series of lectures to cover\n  important aspects of the quantum stack and a set of student\n  presentations. The following are a list of representative\n  topics:</p>\n<p>- Basics of quantum computing<br/>\n  - The fault-tolerant quantum stack<br/>\n  - Compilation<br/>\n  - Instruction sets<br/>\n  - Implementations of quantum error correction<br/>\n  - Implementation of magic state distillation<br/>\n  - Resource estimation</p>\n<p>Student presentations will be based on a reading list of\n  important papers in quantum architecture.\u00a0<br/>\n  \u00a0</p>\n<h2>Objectives</h2>\n<p><span style=\"color:black;\">At the end of the course, students\n  will have a broad understanding of the quantum computing stack.\n  They will understand how major qubit technologies work, design\n  challenges in real quantum hardware, how quantum applications are\n  mapped to a system and the importance of quantum error correction\n  for scalability.\u00a0</span></p>\n<h2>Assessment</h2>\n<ul>\n<li>Seminar presentation: 20%\n      <ul>\n<li>Read one paper from a provided reading list</li>\n<li>Prepare a 20 minute presentation on it + 10 minutes of\n        answering questions.</li>\n<li>The student presentation should be similar to a\n        conference presentation - convey the problem, what are\n        prior solutions, what did the paper do, what are the\n        results, what are future directions</li>\n<li>\n<p>For the 20% of the score, the score will be split as\n          15% and 5%. 15% based on how well the student understands\n          and explains the paper to the rest of the audience. 5%\n          based on the Q&amp;A session.</p>\n<p>\u00a0</p>\n</li>\n</ul>\n</li>\n<li>Course project: 80% (split across a proposal, mid-term\n    report, final report)\n      <ul>\n<li>A. Research proposal - at least 500 words (10% of\n        total)</li>\n<li>B. Mid-term report - at least 1000 words (including\n        aspects like the research problem, literature review, what\n        questions will be evaluated, any early ideas or methods\n        (20% of total)\u00a0</li>\n<li>C. Final report - up to 4 pages double column in a\n        conference paper style with 3000-4000 words. In addition to\n        the mid term report, should include aspects like results\n        and future directions. (50% of the total)</li>\n<li>D. Report on contributions (in case of group work by 2\n        students) - 1 paragraph on individual contribution of the\n        student. 1 paragraph on teammate's contribution.</li>\n<li>Students may work alone in the project, in which case\n        they need only components A-C. Students may work in pairs\n        of two, but they should in addition individually submit D.\n        The instructor may conduct a short viva in case\n        contributions from both team members are not clear or\n        imbalanced.</li>\n</ul>\n</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Nielsen M.A., Chuang I.L. (2010). Quantum Computation and\n  Quantum Information. Cambridge University Press.</p>\n<p>Mermin N.D. (2007). Quantum Computer Science: An Introduction.\n  Cambridge University Press.</p>\n<p>Assessing requirements to scale to practical quantum advantage\n  (2022) Beverland et al.\u00a0</p>\n", "course_name": "Understanding Quantum Architecture", "course_code": "L132", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L132", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L118": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Teaching</h2>\n<p>The teaching style will be lecture-based, but supported by a\n  practical component where students will learn to use a proof\n  assistant for higher category theory, and build a small portfolio\n  of proofs. Towards the end of the course we will explore some of\n  the exciting computer science research literature on monoidal and\n  higher categories, and students will choose a paper and present\n  it to the class.</p>\n<h2>Aims</h2>\n<p>The module will introduce advanced topics in category theory.\n  The aim is to train students to engage and start modern research\n  on the mathematical foundations of higher categories, the\n  graphical calculus, monoids and representations, type theories,\n  and their applications in theoretical computer science, both\n  classical and quantum.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Be familiar with the techniques of compositional category\n    theory.</li>\n<li>Have a strong understanding of basic categorical semantic\n    models.</li>\n<li>Begun exploring current research in monoidal categories and\n    higher structures.</li>\n</ul>\n<h2>Syllabus</h2>\n<p>Part 1, lecture course:<br/>\n  The first part of the course introduces concepts from monoidal\n  categories and higher categories, and explores their application\n  in computer science.<br/>\n  \u2010 Monoidal categories and the graphical calculus<br/>\n  \u2010 The proof assistant homotopy.io<br/>\n  \u2010 Coherence theorems and higher category theory<br/>\n  \u2010 Linearity, superposition, duality, quantum entanglement<br/>\n  \u2010 Monoids, Frobenius algebras and bialgebras<br/>\n  \u2010 Type theory for higher category theory</p>\n<p>Part 2, exploring the research frontier:<br/>\n  In the second part of the course, students choose a research\n  paper to study, and give a presentation to the class.<br/>\n  There is a nice varied literature related to the topics of the\n  course, and the lecturer will supply a list of suggested\n  papers.\u00a0</p>\n<h2>Classes</h2>\n<p>There will be three exercise sheets for homework, with\n  accompanying\u00a0classes by a teaching assistant to go over\n  them.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Problem sheets (50%)</li>\n<li>Class presentation (20%)</li>\n<li>Practical portfolio (30%)</li>\n</ul>\n<h2>Reading List</h2>\n<p>Chris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory:\n  An Introduction\u201d, Oxford\u00a0University Press</p>\n", "course_name": "Advanced Topics in Category Theory", "course_code": "L118", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L118", "lecturers": ["jv258"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R01": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2><strong>Class limit</strong></h2>\n<p>Maximum 12 students; mininum 6 students</p>\n<h2>Aims</h2>\n<p>This module will attempt to provide an overview of \u201csystems\u201d\n  research. This is a very broad field which has existed for over\n  50 years and which has historically included areas such as\n  operating systems, database systems, file systems, distributed\n  systems and networking, to name but a few. The course will thus\n  necessarily cover only a tiny subset of the field.</p>\n<p>Many good ideas in systems research are the result of\n  discussing and debating previous work. A primary aim of this\n  course therefore will be to educate students in the art of\n  <em>critical thinking</em>: the ability to argue for and/or\n  against a particular approach or idea. This will be done by\n  having students read and critique a set of papers each week. In\n  addition, each week will include presentations from a number of\n  participants which aim to advocate or criticise each piece of\n  work.</p>\n<h2>Syllabus</h2>\n<p>The syllabus for this course will vary from year to year so as\n  to cover a mixture of older and more contemporary systems papers.\n  Contemporary papers will be generally selected from the past 5\n  years, primarily drawn from high quality conferences such as\n  SOSP, OSDI, ASPLOS, FAST, NSDI and EuroSys. Example topics might\n  include:</p>\n<ul>\n<li><em>Systems Research and System Design</em></li>\n<li><em>OS Structure and Virtual Memory</em></li>\n<li><em>Virtualisation</em></li>\n<li><em>Consensus</em></li>\n<li><em>Scheduling</em></li>\n<li><em>Privacy</em></li>\n<li><em>Data Intensive Computing</em></li>\n<li><em>Bugs</em></li>\n</ul>\n<p>The reading each week will involve a load equivalent to 3 full\n  length papers. Students will be expected to read these in detail\n  and prepare a written summary and review. In addition, each week\n  will contain one or more short presentations by students for each\n  paper. The types of presentation will include:</p>\n<ul>\n<li><strong>Overview</strong>: a balanced presentation of the\n    paper, covering both positive and negative aspects.</li>\n<li><strong>Advocacy</strong>: a positive spin on the paper,\n    aiming to convince others of its value.</li>\n<li><strong>Criticism</strong>: a negative take on the paper,\n    focusing on its weak spots and omissions.</li>\n</ul>\n<p>These presentation roles will be assigned in advance,\n  regardless of the <em>soi disant</em> absolute merit of the paper\n  or the preference of the student. Furthermore, all students \u2013\n  regardless of any assigned presentation role in a given week \u2013\n  will be expected to participate in the class by asking questions\n  and generally entering into the debate.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should have a broad\n  understanding of some key papers and concepts in computer systems\n  research, as well as an appreciation of how to argue for or\n  against any particular idea.</p>\n<h2>Coursework and practical work</h2>\n<p>Coursework will be the production of the weekly paper reviews.\n  Practical work will be presenting papers as appropriate, as well\n  as ongoing participation in the class.</p>\n<h2>Assessment</h2>\n<p>Assessment consists of:</p>\n<ul>\n<li>One essay per week for 7 weeks (10% each)</li>\n<li>Presentation (20%)</li>\n<li>Participation in class over the term (10%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Most of the reading for this course will be in the form of the\n  selected papers each week. However, the following may be useful\n  background reading to refresh your knowledge from undergraduate\n  courses:</p>\n<p>Silberschatz, A., Peterson, J.L. and Galvin, P.C. (2005).\n  <em>Operating systems concepts</em>. Addison-Wesley (7th\n  ed.).</p>\n<p>Tanenbaum, A.S. (2008). <em>Modern Operating Systems</em>.\n  Prentice-Hall (3rd ed.).</p>\n<p>Bacon, J. and Harris, T. (2003). <em>Operating systems</em>.\n  Addison-Wesley (3rd ed.).</p>\n<p>Anderson, T. and Dahlin, M. (2014). <em>Operating Systems:\n  Principles and Practice</em>. Recursive Books (2nd ed.).</p>\n<p>Hennessy, J. and Patterson, D. (2006). <em>Computer\n  architecture: a quantitative approach</em>. Elsevier (4th ed.).\n  ISBN\u00a0978-0-12-370490-0.</p>\n<p>Kleppmann M (2016) <em>Designing Data-Intensive\n  Applications</em>, O'Reilly (1st ed.)</p>\n", "course_name": "Advanced Topics in Computer Systems", "course_code": "R01", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R01", "lecturers": ["rmm1002"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R255": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course explores current research topics in machine\n  learning\u00a0in sufficient depth that, at the end of the course,\n  participants will be in a position to contribute to research on\n  their chosen topics. Each topic will be introduced with a lecture\n  which, building on the material covered in the prerequisite\n  courses, will make the current research literature accessible.\n  Each lecture will be followed by up to three sessions which will\n  typically be run as a reading group with student presentations on\n  recent papers from the literature followed by a discussion, or a\n  practical, or similar.</p>\n<h2>Structure</h2>\n<p>Each student will attend 3 topics and each topic's sessions\n  will be spread over 5 contact hours. Students will be expected to\n  undertake readings for their selected topics. There will be some\n  group work.</p>\n<p>There will be a briefing session in Michaelmas term.</p>\n<h2>Syllabus</h2>\n<p>Students choose five\u00a0<strong>topics</strong>\u00a0in\n  preferential order from a list to be published in Michaelmas\n  term. They will be assigned to three topics out of their list.\n  Students are assessed on one of these topics which may\u00a0not\n  necessarily be their first choice topic.</p>\n<p>The topics to be offered in 2024-25 are yet to be decided but\n  to give an indicative idea of the types of topics, the ones\n  offered\u00a0in 2023-24 were:</p>\n<ol>\n<li>Imitation learning \u00a0<i>Prof A. Vlachos</i></li>\n<li>The Future of Large Language Models: data architectures\n    ethics <i>Dr M. Tomalin</i></li>\n<li>Physics and Geometry in Machine Learning <i>Dr C.\n    Mishra</i></li>\n<li>Diffusion Models and SDEs <i>Francisco Vargas, Dr C. H.\n    Ek</i></li>\n<li>Explainable Artificial Intelligence <i>M. Espinosa, Z.\n    Shams,\u00a0Prof M.\u00a0Jamnik</i></li>\n<li>Unconventional approaches to AI <i>Dr S. Banerjee</i></li>\n<li>Narratives in Artificial Intelligence and Machine Learning\n    <i>Prof N. Lawrence</i></li>\n<li>Multimodal Machine Learning <i>K.\u00a0Hemker,\n    N.\u00a0Simidjievski,\u00a0Prof M.\u00a0Jamnik</i></li>\n<li>Deep Reinforcement Learning <i>S. Morad, Dr C. H.\n    Ek</i></li>\n<li>Automation in Proof Assistants <i>A.\n    Jiang,\u00a0W.\u00a0Li, Prof M.\u00a0Jamnik</i></li>\n</ol>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be in a strong position to contribute to the research\n    topics covered;</li>\n<li>understand the fundamental methods (algorithms, data\n    analysis, specific tasks) underlying each topic;</li>\n<li>and be familiar with recent research papers and advances in\n    the field.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Students will typically work in groups to give a presentation\n  on assigned papers.\u00a0Alternatively, a topic may include\n  practical sessions.\u00a0Each topic will typically consist of one\n  preliminary lecture followed by 3 reading and discussion\n  sessions, or several lectures followed by a practical session. A\n  typical topic can accommodate up to 9 students presenting papers.\n  There will be at least 10 minutes general discussion per\n  session.</p>\n<p>Full coursework details will be published by October.</p>\n<h2>Assessment</h2>\n<p>Coursework will be marked by the topic leaders and second\n  marked by the module conveners.</p>\n<ul>\n<li>Participation in all assigned\u00a0topics, 10%</li>\n<li>Presentation or practical work or similar (for one of the\n    chosen topics), 20%</li>\n<li>Topic coursework (for one of the chosen\u00a0topics),\n    70%</li>\n</ul>\n<p>Individual topic coursework will be published late Michaelmas\n  term.</p>\n<p>Assessment criteria for topic coursework will follow project\n  assessment criteria here:\u00a0<a href=\"https://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf\"><u>https://www.cl.cam.ac.uk/teaching/exams/acs_project_marking.pdf</u></a></p>\n<p><i>Please note that students will be assessed on one of their\n  three chosen topics\u00a0but this may not\n  be\u00a0their\u00a0first choice</i>.</p>\n<h2>Recommended reading</h2>\n<p>To be confirmed by each topic convenor.</p>\n", "course_name": "Advanced topics in machine learning", "course_code": "R255", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R255", "lecturers": ["mj201"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R181": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Objectives</h2>\n<p style=\"margin-left:0cm;\"><span style=\"color:black;\">There is a\n  substantial body of academic work demonstrating that complex life\n  is built by cooperation across scales: collections of genes\n  cooperate to produce organisms, cells cooperate to produce\n  multi-cellular organisms and multicellular animals cooperate to\n  form complex social groups. Arguably, all intelligence is\n  collective intelligence. Yet, to-date, many artificially\n  intelligent agents (both embodied and virtual), are generally not\n  conceived from the ground up to interact with other intelligent\n  agents (be it machines or humans). The canonical AI problem is\n  that of a monolithic and solitary machine confronting a\n  non-social environment. This course aims to balance this trend by\n  (i) equipping students with conceptual and practical knowledge on\n  collective intelligence from a computational standpoint, and (ii)\n  by conveying various computational paradigms by which collective\n  intelligence can be modelled as well as synthesized</span>.</p>\n<h2>Assessment</h2>\n<p>The assessment will be based on a reading-group paper\n  presentation (individual or in pairs), accompanied by a 2-page\n  summary, and a technical position paper (individual work) handed\n  in after the course. The position paper will be assessed based on\n  its technical correctness, strength or arguments,<br/>\n  and clarity of research vision, and will be accompanied by a\n  brief 5-minute pre-recorded talk that summarizes key arguments\n  (any slides used are also submitted as part of the project).</p>\n<ul>\n<li>Paper presentation and 1-page summary: 25%</li>\n<li>Technical position paper: 75% (4000 word limit)</li>\n</ul>\n<h2>Recommended reading</h2>\n<ul style=\"list-style-type:disc;\">\n<li>Swarm Intelligence : From Natural to Artificial Systems,\n    Bonabeau et al (1999)</li>\n<li>Joined-Up Thinking, Hannah Critchlow, (2024)</li>\n<li>Supercooperators, Martin Nowak (2011)</li>\n<li>A Course on Cooperative Game Theory, Chakravarty et al.,\n    (2015)</li>\n<li>Governing the Commons: The Evolution of Institutions for\n    Collective Action. Ostrom (1990).\u00a0</li>\n</ul>\n", "course_name": "Computing for Collective Intelligence", "course_code": "R181", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R181", "lecturers": ["asp45"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "P79": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">We all\n  use cryptographic protocols every day: whenever we access an\n  https:// website or send a message via WhatsApp or Signal, for\n  example. In this module, students will get hands-on experience of\n  how those protocols are implemented. Students start by writing\n  their own secure messaging protocol from scratch, and then move\n  on to more advanced examples of cryptographic protocols for\n  security and privacy, such as private information retrieval\n  (searching for data without revealing what you\u2019re searching\n  for).</span></p>\n<h2>Syllabus</h2>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">This is\n  a practical course in which the first half of each of the 2-hour\n  weekly sessions is a lecture that introduces a topic, while the\n  second half is lab time during which the students can work on\n  their implementations, discuss the topics in small groups, and\n  get help from the lecturers. The module is structured into three\n  blocks as follows:\u00a0</span></p>\n<ol>\n<li><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Cryptographic\n    primitives (2 weeks). Using common building blocks such as\n    symmetric ciphers and hash functions. Implementing selected\n    aspects of elliptic curve Diffie-Hellman and digital\n    signatures. Protocol security, Dolev-Yao model, side-channel\n    attacks.\u00a0</span></li>\n<li><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Secure\n    communication (3 weeks). Authenticated key exchange (e.g.\n    SIGMA, TLS), password-authenticated key exchange (e.g. SPAKE2),\n    forward secrecy, post-compromise security, double ratchet, the\n    Signal protocol.\u00a0</span></li>\n<li><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Private\n    database lookups (3 weeks). Lattice-based post-quantum\n    cryptography, learning with errors, homomorphic encryption,\n    private information retrieval.\u00a0</span></li>\n</ol>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">In each\n  block, students will be given a description of the algorithms and\n  protocols covered (e.g. a research paper or an RFC standards\n  document), and a code template that the students should use for\n  their implementation. The implementation language will probably\n  be Python (to be confirmed after the practical materials have\n  been developed ). Students will also be given a set of test cases\n  to check their code, and will have the opportunity to test their\n  implementation communicating with other students\u2019\n  implementations. Finally, for each block, students will write and\n  submit a lab report explaining their approach and the findings\n  from their implementation.</span></p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Understand that cryptography is not magic! The mathematics\n    can look intimidating, but this module will show students that\n    they needn\u2019t be afraid of cryptography.</li>\n<li>Gain appreciation for the complexity and careful\n    implementation of real-world cryptography libraries ,and\n    understand why one should prefer vetted implementations.</li>\n<li>Be familiar with the mathematical notation and concepts\n    commonly used in descriptions of cryptographic protocols, and\n    be able to understand and implement research papers and RFCs in\n    this field.</li>\n<li>Be comfortable with cryptographic primitives commonly used\n    in protocols, and able to correctly use software libraries that\n    implement them.</li>\n<li>Have gained hands-on experience of attacks on cryptographic\n    protocols, and an appreciation for the difficulty of making\n    these protocols correct.</li>\n<li>Have been exposed to ideas and techniques from recent\n    research, which may be useful for their own future\n    research.</li>\n<li>Have practised technical writing through lab reports.</li>\n</ul>\n<h2>Assessment</h2>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Students\n  will be provided with code skeletons in one or two different\n  programming languages (probably Python, maybe Rust) to avoid them\n  struggling with setup issues. For each assignment, students are\n  required to produce a working implementation of the protocols\n  discussed in the course, using the programming language and\n  skeleton provided. Here, \u201cworking\u201d means that the algorithms are\n  functionally correct, but they are not production-quality (in\n  particular, no side-channel countermeasures such as constant-time\n  algorithms are required). For some primitives (e.g. symmetric\n  ciphers and hash functions) existing libraries should be used,\n  whereas other cryptographic algorithms will be implemented from\n  scratch. Students should submit their code as a Docker container\n  that can be run against specified test cases.\u00a0</span></p>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">After\n  each of the three blocks, students will be asked to submit a lab\n  report of approximately 2,000 words along with their code for\n  that block. In the lab report, the students should explain how\n  their implementation works and why it is correct, as well as any\n  findings from the work (e.g. any limitations or trade-offs they\n  found with the protocols). The report structure and marking\n  scheme will be specified in advance. The lab reports provide an\n  opportunity for students to provide critical insights and\n  creativity. For instance, they might compare their implementation\n  with existing real-world implementations which provide additional\n  features and guarantees.\u00a0</span></p>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Each of\n  the three lab reports (along with the associated code) will be\n  marked, forming the basis of assessment, and feedback on the\n  reports will be given to the students before the next submission\n  is due, so that students can take the feedback on board for their\n  next submission. Of the three reports, the worst mark will be\n  discarded, and the other two reports each contribute 50% of the\n  final mark. As such, students might decide to submit only two\n  reports.\u00a0</span></p>\n<p><span style=\"background-color:rgb(255,255,255);color:rgb(32,33,36);\">Students\n  may discuss their work with others, but the implementations and\n  lab reports must be individual work.</span></p>\n<h2>Recommended reading</h2>\n<p>Textbook: Jonathan Katz and Yehuda Lindell. Introduction to\n  Modern Cryptography (3rd edition). CRC Press, 2020.</p>\n<p>The textbook covers prerequisite knowledge on cryptographic\n  primitives, such as ciphers, MACs, hash functions, and\n  signatures. This book is also used in the Part II Cryptography\n  course. For the protocols we cover in this course, we are not\n  aware of a suitable textbook; instead we will use research\n  papers, lecture slides, and RFCs as resources, which will be\n  provided at the start of the module.</p>\n", "course_name": "Cryptography and Protocol Engineering", "course_code": "P79", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/P79", "lecturers": ["mk428", "dh623"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R47": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This reading group course examines foundations and current\n  research into distributed ledger (blockchain) technologies and\n  their applications. Students will read, review, and present\n  seminal research papers in this area. Once completed, students\n  should be able to integrate blockchain technologies into their\n  own research and gain familiarity with a range of research\n  skills.</p>\n<h2>Lectures</h2>\n<ol>\n<li>Introduction</li>\n<li>Consensus protocols</li>\n<li>Bitcoin and its variants</li>\n<li>Ethereum, smart contracts, and other permissionless\n    DLTs</li>\n<li>Hybrid and permissioned DLTs</li>\n<li>Applications</li>\n</ol>\n<h2>Learning objectives</h2>\n<p>There are two broad objectives: to acquire familiarity with a\n  body of work in the area of distributed ledgers and to learn some\n  specific research skills:</p>\n<ol>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf\" target=\"_blank\">How to read a paper</a></li>\n<li><a href=\"http://pages.cs.wisc.edu/~markhill/the_task_of_the_referee.pdf\" target=\"_blank\">How to review a paper</a></li>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Evaluating_a_research_paper\" target=\"_blank\">How to analyze a paper\u2019s strengths and\n    weaknesses</a></li>\n<li><a href=\"http://www-net.cs.umass.edu/kurose/talks/top_10_tips_for_writing_a_paper.ppt\" target=\"_blank\">Written</a> and <a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Giving_and_attending_talks\" target=\"_blank\">oral</a> presentation skills</li>\n</ol>\n<h2>Assessment</h2>\n<p>You are expected to read all assigned papers and submit paper\n  reviews\u00a0each week. Each review must either follow the\n  provided review form [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:b:/g/personal/sk818_cam_ac_uk/ESpwszt8Nr5PtSF5b9GGH4EBT0WLI_z-bj-jPnRd5i2YYw?e=xrDeau\" target=\"_blank\">PDF</a>] [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:u:/g/personal/sk818_cam_ac_uk/Ecpv3poyjuVKgbvC8fL48jIBRnAGTdVLBFKvMPf2RJjYrw?e=rnWfGE\" target=\"_blank\">Latex source</a>]. Each \u201creview\u201d is worth 5% of\n  your total mark, and is marked out of 100 with 60 a passing\n  grade. Marks will be awarded and penalties for late submission\n  applied according to <a href=\"http://www.cl.cam.ac.uk/teaching/exams/acs_assessment.html\" target=\"_blank\">ACS Assessment Guidelines</a>.\u00a0</p>\n<ul>\n<li>One paper review for the first week, then two paper reviews\n    each week for 6 weeks (13 reviews, 5% each) 65% (approx 600\n    words per review)</li>\n<li>Summative essay 25% (max 3000 words)</li>\n<li>Presentation 5%</li>\n<li>100% attendance in class 5% (2 marks deducted per missed\n    class)</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p>Narayanan, A. , Bonneau, J., Felten, E., Miller, A. and\n  Goldfeder, S. (2016). <em>Bitcoin and Cryptocurrency\n  Technologies: A Comprehensive Introduction</em>. Princeton\n  University Press.<br/>\n  (2016 Draft available here: <a href=\"https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf\" target=\"_blank\">https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf</a>)</p>\n", "course_name": "Distributed Ledger Technologies: Foundations and Applications", "course_code": "R47", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R47", "lecturers": ["sk818"], "lectures": null, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L193": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The recent palpable introduction of Artificial Intelligence\n  (AI) models to everyday consumer-facing products, services, and\n  tools brings forth several new technical challenges and ethical\n  considerations. Amongst these is the fact that most of these\n  models are driven by Deep Neural Networks (DNNs), models that,\n  although extremely expressive and useful, are notoriously complex\n  and opaque. This \u201cblack-box\u201d nature of DNNs limits their ability\n  to be successfully deployed in critical scenarios such as those\n  in healthcare and law. Explainable Artificial Intelligence (XAI)\n  is a fast-moving subfield of AI that aims to circumvent this\n  crucial limitation of DNNs by either\u00a0</p>\n<p>(i) constructing human-understandable explanations for their\n  predictions,\u00a0</p>\n<p>or (ii) designing novel neural architectures that are\n  interpretable by construction.\u00a0</p>\n<p>In this module, we will introduce the key ideas behind XAI\n  methods and discuss some of the important application areas of\n  these methods (e.g., healthcare, scientific discovery, debugging,\n  model auditing, etc.). We will approach this by focusing on the\n  nature of what constitutes an explanation, and discussing\n  different ways in which explanations can be constructed or learnt\n  to be generated as a by-product of a model. The main aim of this\n  module is to introduce students to several commonly used\n  approaches in XAI, both theoretically in lectures and through\n  hands-on exercises in practicals, while also bringing recent\n  promising directions within this field to their attention. We\n  hope that, by the end of this module, students will be able to\n  directly contribute to XAI research and will understand how\n  methods discussed in this module may be powerful tools for their\n  own work and research.</p>\n<h2>Syllabus</h2>\n<ol>\n<li>Overview and taxonomy of XAI (why is explainability needed,\n    definition of terms, taxonomy of the XAI space,\n    etc.)\u00a0</li>\n<li>Perturbation-based feature attribution (e.g., LIME,\n    Anchors, SHAP, etc.)</li>\n<li>Propagation-based feature attribution methods (e.g.,\n    Relevance Propagation, Saliency methods, etc.)</li>\n<li>Concept-based explainability (Net2Vec, T-CAV, ACE,\n    etc.)</li>\n<li>Interpretable architectures (CBMs and variants, Concept\n    Whitening, SENNs, etc.)</li>\n<li>Neurosymbolic methods (DeepProbLog, Neural Reasoners,\n    etc.)</li>\n<li>Sample-based Explanations (Influence functions, ProtoPNets,\n    etc.)</li>\n<li>Counterfactual explanations</li>\n</ol>\n<h2>Proposed Schedule</h2>\n<p>The 16 hours of lectures across 8 weeks will be divided as\n  follows:\u00a0<br/>\n  Week 1: 1h lecture + 1h lecture\u00a0<br/>\n  Week 2: 1h lecture + 1h reading group &amp;\n  presentations\u00a0<br/>\n  Week 3: 1h lecture + 1h reading group &amp;\n  presentations\u00a0<br/>\n  Week 4: 2h practical\u00a0<br/>\n  Week 5: 1h lecture + 1h reading group &amp;\n  presentations\u00a0<br/>\n  Week 6: 2h practical\u00a0<br/>\n  Week 7: 1h lecture + 1h reading group &amp;\n  presentations\u00a0<br/>\n  Week 8: 1h reading group &amp; presentation + 1h reading group\n  &amp; presentations</p>\n<p>In weeks where lectures are planned, one-hour lecture slots\n  will intercalate with one hour of student paper presentations.\n  During each paper presentation session, three students will\n  present a paper related to the topic covered in the earlier\n  lecture that week for about 10 minutes each + 5 minutes of\n  questions. At the end of all paper presentations, there will be a\n  discussion on all the papers. The order of student presentations\n  will be allocated randomly during the first week so that students\n  know in advance when they are expected to present. For the sake\n  of fairness, we will release the paper to be presented by each\n  student a week before their presentation slot.</p>\n<h2>Objectives</h2>\n<p>By the end of this module, students should be able to:</p>\n<ul>\n<li>Recognise and identify key concepts in XAI together with\n    their connection to related subfields in AI such as fairness,\n    accountability, and trustworthy AI.</li>\n<li>Understand how to use, design, and deploy model-agnostic\n    perturbation methods such as LIME, Anchors, and RISE. In\n    particular, students should understand the connection between\n    feature importance and cooperative game theory, and its uses in\n    methods such as SHAP.</li>\n<li>Identify the uses and limitations of propagation-based\n    feature importance methods such as Saliency, SmoothGrad,\n    GradCAM, and Integrated Gradients. Students should be able to\n    implement each of these methods on their own and connect the\n    theoretical ideas behind them to practical code, exploiting\n    modern frameworks\u2019 auto-differentiation.</li>\n<li>Understand what concept learning is and what limitations it\n    overcomes compared to traditional feature-based methods.\n    Specifically, students should understand how probing a DNN\u2019s\n    latent space may be exploited for learning useful concepts for\n    explainability.</li>\n<li>Reason about the key components of inherently interpretable\n    architectures and neuro-symbolic methods and understand how\n    interpretable neural networks can be designed from first\n    principles.</li>\n<li>Elaborate on what sample-based explanations are and how\n    influence functions and prototypical architectures such as\n    ProtoPNet can be used to construct such explanations.</li>\n<li>Explain what counterfactual explanations are, how they are\n    related to causality, and under which conditions they may be\n    useful.</li>\n</ul>\n<p>Upon completion of this module, students will have the\n  technical background and tools to use XAI as part of their own\n  research or partake in XAI research itself. Moreover, we hope\n  that by detailing a clear timeline of how this relatively young\n  subfield has developed, students may be able to better understand\n  what are some fundamental open questions in this area and what\n  are some promising directions that are currently actively being\n  explored.<br/>\n  \u00a0</p>\n<h2>Assessment</h2>\n<p><strong>(10%) student presentation</strong>: each student will\n  be randomly assigned a presentation slot at the beginning of the\n  course. We will then distribute a paper for them to present in\n  their slot a week before the presentation\u2019s scheduled time.\n  Students are expected to prepare a 10-minute presentation of\n  their assigned paper where they will present the motivation of\n  their assigned work and discuss the main methodology and findings\n  reported in that paper. We will encourage students to focus on\n  fully communicating the intuition of the work in their assigned\n  papers and try and connect it with ideas that we have previously\n  discussed in previous lectures. All students not presenting each\n  week will be asked to submit one question pertaining to each\n  paper presented that week before the paper presentations.</p>\n<p><strong>(20%) practical exercises</strong>: We will run two\n  practical sessions where students will be asked to perform a\n  series of exercises that require them to use concepts we have\n  introduced in lecture up to that point. For each practical\n  session, we will prepare a colab notebook to guide the student\n  through exercises and we will ask the students to submit their\n  solutions through this colab notebook. We expect students to\n  complete about \u2154 of the exercises in the practical class and\n  complete the rest at home as homework. Each practical will be\n  worth 10%.</p>\n<p><strong>(70%) mini-project</strong>: At the end of week 3, we\n  will hand out a list of papers for students to select their\n  mini-projects from. Each mini-project will consist of a student\n  selecting a paper from our list and reimplementing and expanding\n  the key idea in the paper. We encourage students to be as\n  creative as they want with how they drive their mini-project once\n  the paper has been selected. For example, they can reimplement\n  the technique in the paper and combine it with methodologies from\n  other works we discussed in lecture, or they can apply their\n  paper\u2019s methodology to a new domain, datasets, or setup, where\n  the technique may offer interesting and potentially novel\n  insights. We will ask all students to submit a report in a\n  workshop format of up to 4,000 words. This report, due roughly a\n  week after Lent term ends, should describe their methodology,\n  experiments, and results. A crucial aspect of this report\n  involves explaining the rationale behind different choices in\n  methodology and experiments, as well as elaborating on the\n  choices made and hypotheses tested throughout their mini-project\n  (potentially showing a deep understanding of the work they are\n  basing their mini-project on). To aid students with selecting\n  their projects and making progress on them, we will hold regular\n  office hours when students can come to discuss their progress and\n  questions with us.</p>\n<h2>Recommended reading</h2>\n<p><strong>Textbooks</strong></p>\n<p>* Christoph Molnar, \u201cInterpretable Machine Learning\u201d. (2022):\n  <a href=\"https://christophm.github.io/interpretable-ml-book/\">https://christophm.github.io/interpretable-ml-book/</a></p>\n<p><strong>Online Courses and Tutorial</strong></p>\n<p>* Su-in Lee and Ian Cover, \u201cCSEP 590B Explainable AI\u201d\n  University of Washington* Explaining Machine Learning\n  Predictions: State-of-the-art, Challenges, and Opportunities</p>\n<p>* On Explainable AI: From Theory to Motivation, Industrial\n  Applications, XAI Coding &amp; Engineering Practices - AAAI 2022\n  Turorial</p>\n<p><strong>Survey papers</strong></p>\n<p>* Arrieta, Alejandro Barredo, et al. \"Explainable Artificial\n  Intelligence (XAI): Concepts, taxonomies, opportunities and\n  challenges toward responsible AI.\" Information fusion 58 (2020):\n  82-115.</p>\n<p>* Rudin, Cynthia, et al. \"Interpretable machine learning:\n  Fundamental principles and 10 grand challenges.\" Statistics\n  Surveys 16 (2022): 1-85.</p>\n", "course_name": "Explainable Artificial Intelligence", "course_code": "L193", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L193", "lecturers": ["mj201", "zs315"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L361": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Objectives</h2>\n<p>This course aims to extend the machine learning knowledge\n  available to students in Part I (or present in typical\n  undergraduate degrees in other universities), and allow them to\n  understand how these concepts can manifest in a decentralized\n  setting. The course will consider both theoretical (e.g.,\n  decentralized optimization) and practical (e.g., networking\n  efficiency) aspects that combine to define this growing area of\n  machine learning.\u00a0</p>\n<p>At the end of the course students should:</p>\n<ul>\n<li>Understand popular methods used in federated learning</li>\n<li>Be able to construct and scale a simple federated\n    system</li>\n<li>Have gained an appreciation of the core limitations to\n    existing methods, and the approaches available to cope with\n    these issues</li>\n<li>Developed an intuition for related technologies like\n    differential privacy and secure aggregation, and are able to\n    use them within typical federated settings\u00a0</li>\n<li>Can reason about the privacy and security issues with\n    federated systems</li>\n</ul>\n<h2>Lectures</h2>\n<ol>\n<li>Course Overview. Introduction to Federated Learning.</li>\n<li>Decentralized Optimization.</li>\n<li>Statistical and Systems Heterogeneity.</li>\n<li>Variations of Federated Aggregation.</li>\n<li>Secure Aggregation.</li>\n<li>Differential Privacy within Federated Systems.</li>\n<li>Extensions to Federated Analytics.</li>\n<li>Applications to Speech, Video, Images and Robotics.</li>\n</ol>\n<h2>Lab sessions</h2>\n<ol>\n<li>Federating a Centralized ML Classifier.</li>\n<li>Behaviour under Heterogeneity.</li>\n<li>Scaling a Federated Implementation.</li>\n<li>Exploring Privacy with Federated Settings</li>\n</ol>\n<h2>Recommended Reading</h2>\n<p>Readings will be assigned for each lecture. Readings will be\n  taken from either research papers, tutorials, source code or\n  blogs that provide more comprehensive treatment of taught\n  concepts.</p>\n<h2>Assessment - Part II Students</h2>\n<p>Four labs are performed during the course, and students\n  receive 10% of their total grade for work done as part of each\n  lab. (For a total of 40% of the total grade from lab work alone).\n  Labs will primarily provide hands-on teaching opportunities, that\n  are then utilized within the lab assignment which is completed\n  outside of the lab contact time. MPhil and Part III students will\n  be given additional questions to answer within their version of\n  the lab assignment which will differ from the assignment given to\n  Part II CST students.</p>\n<p>The remainder of the course grade (60%) will be given based on\n  a hands-on project that applies the concepts taught in lectures\n  and labs. This hands-on project will be assessed based on upon a\n  combination of source code, related documentation\u00a0and brief\n  8-minute pre-recorded talk that summarizes key project elements\n  (any slides used are also submitted\u00a0as part of the project).\n  Please note, that in the case of Part II CST students, the talk\n  itself is not examinable -- as such will be made optional to\n  those students.</p>\n<p>A range of possible practical projects will be described and\n  offered to students to select from, or alternatively students may\n  propose their own. MPhil and Part III students will select from a\n  project pool that is separate from those offered to Part II CST\n  students. MPhil and Part III projects will contain a greater\n  emphasis on a research element, and the pre-recorded talks\n  provided by this student group will focus on this research\n  contribution. The project will be assessed on the level of\n  student understanding demonstrated, the degree of difficulty,\n  correctness of implementation -- and for Part III/MPhil students\n  the additional criteria of the quality and execution of the\n  research methodology, and depth and quality of results\n  analysis.</p>\n<p>This project can be done individually or in groups -- although\n  individual projects will be strongly encouraged. It will be\n  required the project is performed using a code repository that\n  also will contain all documentation -- access to this repository\n  will be shared with course staff (e.g., lecturer and TAs). Where\n  needed, marks assigned to students within a group will be\n  differentiated using this repository as an input. Furthermore if\n  groups are formed, members must be either entirely from Part\n  III/MPhil students or Part II CST, i.e., these two student groups\n  should not mix to form a project group.</p>\n<p>Projects will be made available publicly. A maximum word count\n  for written contributions for the project will be\n  enforced.\u00a0</p>\n<h2>\u00a0</h2>\n", "course_name": "Federated Learning: Theory and Practice", "course_code": "L361", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L361", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L65": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims and Objectives</h2>\n<p>Most of the patterns we see in nature can be elegantly\n  reasoned about using spatial symmetries\u2014transformations that\n  leave underlying objects unchanged. This observation has had\n  direct implications for the development of modern deep learning\n  architectures that are seemingly able to escape the \u201ccurse of\n  dimensionality\u201d and fit complex high-dimensional tasks from noisy\n  real-world data. Beyond this, one of the most generic\n  symmetries\u2014the permutation\u2014will prove remarkably powerful in\n  building models that reason over graph structured-data, which is\n  an excellent abstraction to reason about naturally-occurring,\n  irregularly-structured data. Prominent examples include molecules\n  (represented as graphs of atoms and bonds, with three-dimensional\n  coordinates provided), social networks and transportation\n  networks. Several already-impacted application areas include\n  traffic forecasting, drug discovery, social network analysis and\n  recommender systems. The module will provide the students the\n  capability to analyse irregularly- and nontrivially-structured\n  data in an effective way, and position geometric deep learning in\n  a proper context with related fields. The main aim of the course\n  is to enable students to make direct contributions to the field,\n  thoroughly assimilate the key concepts in the area, and draw\n  relevant connections to various other fields (such as NLP,\n  Fourier Analysis and Probabilistic Graphical Models). We assume\n  only a basic background in machine learning with deep neural\n  networks.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>The framework of geometric deep learning, and its key\n    building blocks: symmetries, representations, invariance and\n    equivariance</li>\n<li>Fundamentals of processing data on graphs, as well as\n    impactful application areas for graph representation\n    learning</li>\n<li>Theoretical principles of graph machine learning:\n    permutation invariance and equivariance</li>\n<li>The three \"flavours\" of spatial graph neural networks\n    (GNNs) (convolutional, attentional, message passing) and their\n    relative merits. The Transformer architecture as a special\n    case.</li>\n<li>Attaching symmetries to graphs: CNNs on images, spheres and\n    manifolds, Geometric Graphs and E(n)-equivariant GNNs</li>\n<li>Relevant connections of geometric deep learning to various\n    other fields (such as NLP, Fourier Analysis and Probabilistic\n    Graphical Models)</li>\n</ul>\n<h2>Lectures</h2>\n<p>The lectures will cover the following topics:</p>\n<ul>\n<li>Learning with invariances and symmetries: geometric deep\n    learning. Foundations of group theory and representation\n    theory.</li>\n<li>Why study data on graphs? Success stories: drug screening,\n    travel time estimation, recommender systems. Fundamentals of\n    graph data processing: network science, spectral clustering,\n    node embeddings.</li>\n<li>Permutation invariance and equivariance on sets and graphs.\n    The principal tasks of node, edge and graph classification.\n    Neural networks for point clouds: Deep Sets, PointNet;\n    universal approximation properties.</li>\n<li>The three flavours of spatial GNNs: convolutional,\n    attentional, message passing. Prominent examples: GCN, SGC,\n    ChebyNets, MoNet, GAT, GATv2, IN, MPNN, GraphNets. Tradeoffs of\n    using different GNN variants.</li>\n<li>Graph Rewiring: how to apply GNNs when there is no graph?\n    Links to natural language processing---Transformers as a\n    special case of attentional GNNs. Representative methodologies\n    for graph rewiring: GDC, SDRF, EGP, DGCNN.</li>\n<li>Expressive power of graph neural networks: the\n    Weisfeiler-Lehman hierarchy. GINs as a maximally expressive\n    GNN. Links between GNNs and graph algorithms: neural\n    algorithmic reasoning.</li>\n<li>Combining spatial symmetries with GNNs: E(n)-equivariant\n    GNNs, TFNs, SE(3)-Transformer. A deep dive into AlphaFold\n    2.</li>\n<li>Worked examples: Circulant matrices on grids, the discrete\n    Fourier transform, and convolutional networks on spheres. Graph\n    Fourier transform and the Laplacian eigenbasis.</li>\n</ul>\n<h2>Practicals</h2>\n<p>The practical is designed to complement the knowledge learnt\n  in lectures and teach students to derive additional important\n  results and architectures not directly shown in lectures. The\n  practical will be given as a series of individual exercises (each\n  either code implementation or proof/derivation). Each of these\n  exercises can be individually assessed based on a specified mark\n  budget.</p>\n<p>Possible practical topics include the study of higher-order\n  GNNs and equivariant message passing.</p>\n<h2>Assessment</h2>\n<ul>\n<li>(60%) Group Mini-project (writeup) at the end of the\n    course. The mini projects can either be self-proposed, or the\n    students can express their preference for one of the provided\n    topics, the list of which will be announced at the start of\n    term. The projects\u00a0will consist of implementing and/or\n    extending graph representation learning models in the\n    literature, applying them to publicly available datasets.\n    Students will undertake the project in pairs and submit a joint\n    writeup limited to 4,000 words (in line with other modules);\n    appendix of work logs to be included but ungraded;</li>\n<li style=\"list-style: none\"><br/></li>\n<li>(10%) Short presentation and viva: students will give a\n    short presentation to explain their individual contribution to\n    the mini-project and there will be a short viva following.</li>\n<li style=\"list-style: none\"><br/></li>\n<li>(30%) Practical work completion. Completing the exercises\n    specified in the practical\u00a0to a satisfactory standard. The\n    practical assessor should be satisfied that the student derived\n    their answers using insight gained from the course; coupled\n    with original thought, not by simple copy-pasting of relevant\n    related work. The students would submit code and a short\n    report, which would then be marked in line with the\n    predetermined mark budget for each practical item.</li>\n</ul>\n<p>The students will learn how to run advanced architectures on\n  GPU but no specific need for dedicated GPU resources. Practicals\n  will be made possible to do on CPU; if required, students can use\n  GPUs on publicly available free services (such as Colab) for\n  their mini-project work.</p>\n<h2>References</h2>\n<p>The course will be based on the following literature:</p>\n<ul>\n<li>\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics,\n    and Gauges\", by Michael Bronstein, Joan Bruna, Taco Cohen and\n    Petar Veli\u010dkovi\u0107</li>\n<li>\"Graph Representation Learning\", by Will Hamilton</li>\n<li>\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron\n    Courville.</li>\n</ul>\n", "course_name": "Geometric Deep Learning", "course_code": "L65", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L65", "lecturers": ["pl219"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L349": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The course aims to explore how wearable and mobile systems\n  sensors can be used to gather data relevant to understand health,\n  how the data can be analysed with advanced signal processing and\n  machine learning and the performance of these systems in terms of\n  diagnostics and disease progression detection.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Course Overview. Introduction to Mobile Health. Evaluation\n    metrics and methodology. Basics of Signal Processing.</li>\n<li>Inertial Measurement Units, Human Activity Recognition\n    (HAR) and Gait Analysis and Machine Learning for IMU data.</li>\n<li>Radios, Bluetooth, GPS and Cellular. Epidemiology and\n    contact tracing, Social interaction sensing and\n    applications.\u00a0Location tracking in health monitoring and\n    public health.</li>\n<li>Audio Signal Processing. Voice and Speech Analysis:\n    concepts and data analysis.\u00a0Body Sounds analysis.</li>\n<li>Photoplethysmogram and Light sensing for health (heart and\n    sleep)</li>\n<li>Contactless and wireless behaviour and physiological\n    monitoring</li>\n<li>Generative Models for Wearable Health Data</li>\n<li>Topical Guest Lectures</li>\n</ul>\n<h2>Objectives</h2>\n<p>The course aims to explore how wearable and mobile systems\n  sensors can be used to gather data relevant to understand health,\n  how the data can be analysed with advanced signal processing and\n  machine learning and the performance of these systems in terms of\n  diagnostics and disease progression detection.</p>\n<p>Roughly, each lecture contains a theory part about the working\n  of \u201csensor signals\u201d or \u201cdata analysis methods\u201d and an application\n  part which contextualises the concepts.</p>\n<p>At the end of the course students should: Understand how\n  mobile/wearable sensors capture data and their working.\n  Understand different approaches to acquiring and analysing sensor\n  data from different types of sensors. Understand the concept of\n  signal processing applied to time series data and their practical\n  application in health. Be able to extract sensor data and analyse\n  it with basic signal processing and machine learning techniques.\n  Be aware of the different health applications of the various\n  sensor techniques. The course will also touch on privacy and\n  ethics implications of the approaches developed in an orthogonal\n  fashion.</p>\n<h2>Recommended Reading</h2>\n<p>Please see Course Materials for recommended reading for each\n  session.</p>\n<h2>Assessment\u00a0-\u00a0Part II students</h2>\n<p>Two assignments will be based on two datasets which will be\n  provided to the students:</p>\n<p>Assignment 1 (shared for Part II and Part III/MPhil): this\n  will be based on a dataset and will be worth 40% of the final\n  mark. The task of the assessment will be to perform\n  pre-processing and basic data analysis in a \"colab\" and an answer\n  sheet of no more than 1000 words.</p>\n<p>Assignment 2 (Part II): This assignment (worth 60% of the\n  final mark) will be a fuller analysis of a dataset focusing on\n  machine learning algorithms and metrics. Discussion and\n  interpretation of the findings will be reported in a colab and a\n  report of no more than 1200 words.</p>\n", "course_name": "Mobile Health", "course_code": "L349", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L349", "lecturers": ["cm542", "jh2298"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L304": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>In recent years multiprocessors have become ubiquitous, but\n  building reliable concurrent systems with good performance\n  remains very challenging. The aim of this module is to introduce\n  some of the theory and the practice of concurrent programming,\n  from hardware memory models and the design of high-level\n  programming languages to the correctness and performance\n  properties of concurrent algorithms.</p>\n<h2>Lectures</h2>\n<p>Part 1: Introduction and relaxed-memory concurrency [Professor\n  P. Sewell]</p>\n<ul>\n<li><strong>Introduction</strong>. Sequential consistency,\n    atomicity, basic concurrent problems. [1 block]</li>\n<li><strong>Concurrency on real multiprocessors</strong>: the\n    relaxed memory model(s) for x86, ARM, and IBM Power, and\n    theoretical tools for reasoning about x86-TSO programs. [2\n    blocks]</li>\n<li><strong>High-level languages</strong>. An introduction to\n    C/C++11 and Java shared-memory concurrency. [1 block]</li>\n</ul>\n<p>Part 2: Concurrent algorithms [Dr T. Harris]</p>\n<ul>\n<li><strong>Concurrent programming</strong>. Simple algorithms\n    (readers/writers, stacks, queues) and correctness criteria\n    (linearisability and progress properties). Advanced\n    synchronisation patterns (e.g. some of the following:\n    optimistic and lazy list algorithms, hash tables,\n    double-checked locking, RCU, hazard pointers), with discussion\n    of performance and on the interaction between algorithm design\n    and the underlying relaxed memory models. [3 blocks]</li>\n<li><strong>Research topics</strong>, likely to include one\n    hour on transactional memory and one guest lecture. [1\n    block]</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>have a good understanding of the semantics of concurrent\n    programs, both at the multprocessor level and the C/Java\n    programming language level;</li>\n<li>have a good understanding of some key concurrent\n    algorithms, with practical experience.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<p>Two assignments each worth 50%</p>\n<h2>Recommended reading</h2>\n<p>Herlihy, M. and Shavit, N. (2008). <em>The art of\n  multiprocessor programming</em>. Morgan Kaufmann.</p>\n", "course_name": "Multicore Semantics and Programming", "course_code": "L304", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/L304", "lecturers": ["pes20", "tlh20", "cp526"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R171": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The aim of this module is to introduce students to the\n  foundations of the field of reinforcement learning (RL), discuss\n  state-of-the-art RL methods, incentivise students to produce\n  critical analysis of recent methods, and prompt students to\n  propose novel solutions that address shortcomings of existing\n  methods. If judged by the headlines, RL has seen an unprecedented\n  success in recent years. However, the vast majority of RL methods\n  still have shortcomings that might not be apparent at first\n  glance. The aim of the course is to inspire students by\n  communicating the promising aspects of RL, but ensure that\n  students develop the ability to produce a critical analysis of\n  current RL limitations.</p>\n<h2>Objectives</h2>\n<p>Students will learn the following technical concepts:</p>\n<ul>\n<li>fundamental RL terminology and mathematical formalism; a\n    brief history of RL and its connection to neuroscience and\n    biological systems</li>\n<li>RL methods for discrete action spaces, e.g. deep Q-learning\n    and large-scale Monte Carlo Tree Search</li>\n<li>methods for exploration, modelling uncertainty, and partial\n    observability for RL</li>\n<li>modern policy gradient and actor-critic methods</li>\n<li>concepts needed to construct model-based RL and Model\n    Predictive Control methods</li>\n<li>approaches to make RL data-efficient and ways to enable\n    simulation-to-reality transfer</li>\n<li>examples of fine-tuning foundation models and large\n    language models (LLMs) with human feedback; safe RL concepts;\n    examples of using RL for safety validation</li>\n<li>examples of using RL for scientific discovery</li>\n</ul>\n<p>Students will also gain experience with analysing RL methods\n  to uncover their strengths and shortcomings, as well as proposing\n  extensions to improve performance.</p>\n<p>Finally, students will gain skills needed to create and\n  deliver a successful presentation in a format similar to that of\n  conference presentations.</p>\n<p>With all of the above, students who take part in this module\n  will be well-prepared to start conducting research in the field\n  of reinforcement learning.<br/>\n  \u00a0</p>\n<h2>Syllabus</h2>\n<p><u>Topic 1: Introduction and Fundamentals</u></p>\n<ul>\n<li>Overview of RL: foundational ideas, history, and books;\n    connection to neuroscience and biological systems, recent\n    industrial applications and research demonstrations</li>\n<li>Mathematical fundamentals: Markov decision processes,\n    Bellman equations, policy and value iteration, temporal\n    difference learning</li>\n</ul>\n<p><u>Topic 2: RL in Discrete Action Spaces</u></p>\n<ul>\n<li>Q-learning, function approximation and deep Q-learning;\n    nonstationarity in RL and its implications for deep learning;\n    example applications (video games; initial example: Atari)</li>\n<li>Monte Carlo Tree Search; example applications\n    (AlphaGo)</li>\n</ul>\n<p><u>Topic 3: Exploration, Uncertainty, Partial\n  Observability</u></p>\n<ul>\n<li>Multi-armed bandits, Bayesian optimisation, regret\n    analysis</li>\n<li>Partially observable Markov decision process; belief,\n    memory, and sequence modelling (probabilistic methods,\n    recurrent networks, transformers)</li>\n</ul>\n<p><u>Topic 4: Policy Gradient and Actor-critic Methods for\n  Continuous Action Spaces</u></p>\n<ul>\n<li>Importance sampling, policy gradient theorem, actor-critic\n    methods (SPG, DDPG)</li>\n<li>Proximal policy optimisation; example applications</li>\n</ul>\n<p><u>Topic 5: Model-based RL and Model Predictive\n  Control</u></p>\n<ul>\n<li>Learning dynamics models (graph networks, stochastic\n    processes, diffusion models, physics-based models, ensembles);\n    planning with learned models</li>\n<li>Model predictive control; example applications (real-time\n    control)</li>\n</ul>\n<p><u>Topic 6: Data-efficient RL and Simulation-to-reality\n  Transfer</u></p>\n<ul>\n<li>Data-efficient learning with probabilistic methods from\n    real data (e.g. policy search in robotics), real-to-sim\n    inference and differentiable simulation, data-efficient\n    simulation-to-reality transfer</li>\n<li>RL for physical systems (successful examples in locomotion,\n    open problems in contact-rich manipulation, applications to\n    logistics, energy, and transport systems); examples of RL for\n    healthcare.</li>\n</ul>\n<p><u>Topic 7: RL with Human Feedback ; Safe RL and RL for\n  Validation</u></p>\n<ul>\n<li>Fine-tuning large language models (LLMs) and other\n    foundation models with human feedback (TRLX,RL4LMs, a\n    light-weight overview of RLHF)</li>\n<li>A review of SafeRL, example: optimising commercial HVAC\n    systems using policy improvement with constraints; improving\n    safety using RL for validation: examples in autonomous driving\n    and autonomous flying and aircraft collision avoidance</li>\n</ul>\n<p><u>Topic 8: RL for Scientific Discovery; Student\n  Presentations</u></p>\n<ul>\n<li>Examples of RL for molecular design and drug discovery,\n    active learning for synthesising new materials, RL for nuclear\n    fusion experiments</li>\n<li>Student presentations (based on essays and mini-projects)\n    for other topics in RL, e.g. multi-agent RL, hierarchical RL,\n    RL for hyperparameter optimisation and NN architecture search,\n    RL for multi-task transfer, lifelong RL, RL in biological\n    systems, etc.</li>\n</ul>\n<h2>Assessment</h2>\n<p>The assessment for this module consists of:</p>\n<ul>\n<li>Essay and mini-project (60%)\u00a0<br/>\n      Students will choose a concrete RL method from the\n      literature, then complete a 2-part essay described below:\n      <ul>\n<li>Essay Part 1 (1500 words, 20%): Introduce a formal\n        description of the method and explain how the method\n        extended the state-of-the-art at the time of its\n        publication</li>\n<li>Essay Part 2 or a mini-project (2500 words, 40%):\n        Provide a critical analysis of the chosen RL method.</li>\n</ul>\n</li>\n<li>Presentation of the essay and mini-project results\n    (15%)\u00a0<br/>\n    students will be expected to create and record a 15-minute\n    video presentation of the analysis described in their essay and\n    mini-project.</li>\n<li>Short test of RL theory fundamentals (15%)\u00a0<br/>\n    to test the understanding of RL fundamentals (~15 minutes,\n    in-class, closed book)</li>\n<li>Participation in seminar discussions (10%)\u00a0<br/>\n    take an active part in the seminars by asking clarifying\n    questions and mentioning related works.</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p><strong>Books</strong></p>\n<p>[S&amp;B] Reinforcement Learning: An Introduction (second\n  print edition). Richard S. Sutton, Andrew G. Barto. [Available\n  from the book\u2019s website as a free PDF updated in 2022]</p>\n<p>[CZ] Algorithms for Reinforcement Learning. Csaba Szepesvari.\n  [Available from the book\u2019s website as a free PDF updated in\n  2019]</p>\n<p>[MK] Algorithms for Decision Making. Mykel J. Kochenderfer Tim\n  A. WheelerKyle H. Wray. [Available from the book\u2019s website as a\n  free PDF updated in 2023]</p>\n<p>[DB] Reinforcement Learning and Optimal Control. Dimitri\n  Bertsekas. [Available from the book\u2019s website as a free PDF\n  updated in 2023]</p>\n<p><strong>Presentation guidelines</strong></p>\n<p>[KN] Ten simple rules for effective presentation slides.\n  Kristen M.Naegle. PLoS computational biology 17, no. 12\n  (2021).</p>\n", "course_name": "Reinforcement Learning", "course_code": "R171", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R171", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R225": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module is a theoretically-oriented advanced introduction\n  to the broad field of human-computer interaction, extending to\n  consider topics such as intelligent user interfaces, critical and\n  speculative design, participatory design, cognitive models of\n  users, human-centred, as well as more-than-human-centred design,\n  and others. The course will not address purely engineering\n  approaches to the development of user interfaces (unless there is\n  a clear theoretical question being addressed), but allow students\n  to effectively link the political, social, and ethical\n  considerations in HCI to specific technical and conceptual design\n  challenges. The module builds on the Practical Research in\n  Human-Centred AI course offered in Michaelmas (developed with\n  researchers at Microsoft Research Cambridge) and a collaboration\n  with the Leverhulme Centre for the Future of Intelligence at\n  Cambridge. Participants may include visitors from these groups\n  and/or interdisciplinary research students and academic guests\n  from other University departments, including Cambridge Digital\n  Humanities.</p>\n<h2>Syllabus</h2>\n<p>The syllabus will remain broadly within the area of\n  human-computer interaction, including theories of design practice\n  and the social contexts of technology use. Individual seminar\n  topics will be selected in response to contemporary and recent\n  research developments, in consultation with members of the class\n  and visiting contributors.</p>\n<p>Representative topics in the next year are likely to\n  include:</p>\n<ul>\n<li>Responsible AI and HCI</li>\n<li>Intersectional design for social justice</li>\n<li>Participatory design and co-design</li>\n<li>Sustainable AI and more-than-human-centred design</li>\n<li>Usefulness and usability of ethical design toolkits</li>\n<li>The EU AI Act in design practice</li>\n<li>AI and interface design for transparency</li>\n<li>Designing otherwise: on anarchist HCI<br/>\n    \u00a0</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should have developed\n  facility in discussing and critiquing the aims of their research,\n  especially for an audience drawn from other academic disciplines,\n  including the following skills:</p>\n<ul>\n<li>Theoretical motivation and defence of a research\n    question.</li>\n<li>Consideration of a research proposal from one or more\n    alternative theoretical perspectives.</li>\n<li>Potential critique of the theoretical basis for a programme\n    of research.</li>\n</ul>\n<h2>Coursework</h2>\n<p>In advance of each seminar, all participants must read the\n  essential reading - generally one long, plus one or two short\n  papers. Further reading is suggested for some seminars. Some\n  weeks, instead of shorter papers, students will be asked to\n  explore specific design tools or resources.</p>\n<p>Before seminars 2-8, each student will submit a written\n  commentary on the paper, either generated using a large language\n  model (LLM), or written in the style of an LLM, supplemented by a\n  short original observation.</p>\n<p>Each member of the class must select one of the 8 seminar\n  themes as the subject for a more extended critical review essay.\n  The critical review should be written as an academic research\n  essay, not in LLM style.</p>\n<p>Each seminar will include an informal design exercise to be\n  carried out in small groups. The purpose of these exercises is to\n  reflect on a variety of practical design resources, by applying\n  those resources to concrete case studies.</p>\n<p>Throughout the course, students will be expected to keep a\n  \"reflective diary\", briefly reporting themes that arise in\n  discussion, reflections on the design exercises, and ways in\n  which the readings relate to their own research interests.<br/>\n  \u00a0</p>\n<h2>Practical work</h2>\n<p>There is no practical work element in this course.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Written critical review of a single discussion text\n    (20%)</li>\n<li>Reflective diary submitted at the end of the module\n    (60%)</li>\n<li>In advance of the session, each student will submit a\n    written commentary on the paper, either generated using a large\n    language model (LLM), or written in the style of an LLM. A\n    short original observation should be added (20%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>To be assigned during the module, as discussed above. Most set\n  readings will be available either from the ACM Digital Library,\n  or from institutional repositories of the authors.</p>\n<h2>Further Information</h2>\n<p>Students from the MPhil in the Ethics of AI, Data and\n  Algorithms, MSt in AI, and MPhil in Digital Humanities courses\n  also attend the lectures for this module.</p>\n", "course_name": "Theories of Socio-digital Design for Human-Centred AI", "course_code": "R225", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R225", "lecturers": ["afb21"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R252": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The objectives of this course is to expose you to one of the\n  most active contemporary research\u00a0directions within machine\n  learning: the theory of deep learning (DL). While the first wave\n  of\u00a0modern DL has focussed on empirical breakthroughs and\n  ever more complex techniques, the\u00a0attention is now shifting\n  to building a solid mathematical understanding of why these\n  techniques<br/>\n  work so well in the first place. The purpose of this course is to\n  review this recent progress\u00a0through a mixture of reading\n  group sessions and invited talks by leading researchers in\n  the\u00a0topic, and prepare you to embark on a PhD in modern deep\n  learning research. Compared to\u00a0typical, non-mathematical\n  courses on deep learning, this advanced module will appeal to\n  those\u00a0who have strong foundations in mathematics and\n  theoretical computer science. In a way, this\u00a0course is our\n  answer to the question \u201cWhat should the world\u2019s best computer\n  science students\u00a0know about deep learning in 2023?\u201d</p>\n<h2>Learning Outcomes</h2>\n<p>This course should prepare the best students to start a PhD in\n  the theory and mathematics of\u00a0deep learning, and to start\n  formulating their own hypotheses in this space. You will\n  be\u00a0introduced to a range of empirical and mathematical tools\n  developed in recent years for the\u00a0study of deep learning\n  behaviour, and you will build an awareness of the main open\n  questions\u00a0and current lines of attack. At the end of the\n  course you will:</p>\n<ol>\n<li>be able to explain why classical learning theory is\n    insufficient to describe the\u00a0phenomenon of generalization\n    in DL</li>\n<li>be able to design and interpret empirical studies aimed at\n    understanding generalization</li>\n<li>be able to explain the role of overparameterization: be\n    able to use deep linear models as\u00a0a model to study\n    implicit regularisation of gradient-based learning</li>\n<li>be able to state PAC-Bayes and Information-theoretic\n    bounds, and apply them to DL</li>\n<li>be able to explain the connection between Gaussian\n    processes and neural networks,\u00a0and will be able to study\n    learning dynamics in the neural tangent kernel (NTK)\n    regime.</li>\n<li>be able to formulate your own hypotheses about DL and\n    choose tools to prove/test them</li>\n<li>leverage your deeper theoretical understanding to produce\n    more robust, rigorous and\u00a0reproducible solutions to\n    practical machine learning problems.</li>\n</ol>\n<h2>Syllabus</h2>\n<p>Each week we'll have two to four student-lead presentations\n  about a research paper chosen from a reading list.\u00a0The\n  reading list loosely follows the weekly breakdown below (but we\n  adapt it each year based as this is an active research area):</p>\n<p>Week 1: Introduction to the topic<br/>\n  Week 2: Empirical Studies of Deep Learning Phenomena<br/>\n  Week 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena<br/>\n  Week 4: Implicit Regularization in Deep Linear Models<br/>\n  Week 5: Approximation Theory<br/>\n  Week 6: Networks in the Infinite Width Limit<br/>\n  Week 7: PAC-Bayes and Information Theoretic Bounds for SGD<br/>\n  Week 8: Discussion and Coursework Spotlight Session</p>\n<h2>Assessment</h2>\n<p>Students will be assessed on the following basis:</p>\n<ol>\n<li>20% for presentation/content contributed to the module:\n    Each student will have an\u00a0opportunity to present one of\n    the recommended papers during Weeks 1-7 (30 minute\u00a0slot\n    including Q&amp;A). For the presentation, students should aim\n    to communicate the core\u00a0ideas behind the paper, and\n    clearly present the results, conclusions, and\n    future\u00a0directions. Where possible, students are encouraged\n    to comment on how the work itself\u00a0fits into broader\n    research goals.</li>\n<li>10% for active participation (regular attendance and\n    contribution to discussions during\u00a0the Q&amp;A\n    sessions).</li>\n<li>70% for a group project report, with a word limit of 4000.\n    Either (1) an original research\u00a0proposal/report with a\n    hypothesis, review of related literature, and ideally\n    preliminary\u00a0findings, or, (2) reproduction and ideally\n    extension of an existing relevant paper.<br/>\n    Coursework reports are marked in line with general ACS\n    guidelines, reports receiving\u00a0top marks will have have\n    demonstrable research value (contain an original\n    research\u00a0idea, extension of existing work, or a thorough\n    reproduction effort which is valuable to\u00a0the research\n    community). Additionally, some projects will be suggested\n    during the first weeks of the course, although students are\n    encouraged to come up with their own ideas. Students may be\n    required to participate in group projects, with groups of size\n    2-3 (the class groups will be separated). For any given\n    project, individual contributions would be noted for assessment\n    though a viva component.</li>\n</ol>\n<h2>Relationship with related modules</h2>\n<p>This course can be considered as an advanced follow-up to the\n  Part IIB course on Deep Neural\u00a0Networks. That course\n  introduces some high level\u00a0concepts that this course\n  significantly expands on.</p>\n<p>This module complements L48: Machine Learning in the Physical\n  World and L46: Principles of\u00a0Machine Learning Systems, which\n  focus on applications and hardware/systems aspects of\n  ML\u00a0respectively.</p>\n<h2>Recommended reading</h2>\n<ul>\n<li><a href=\"https://www.pnas.org/cc/arthur-m-sackler-colloquium-on-the-science-of-deep-learning\">\n    PNAS Colloquium on the Science of Deep Learning</a></li>\n<li><a href=\"https://mml-book.github.io/\">Mathematics of\n    Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng\n    Soon Ong.</a></li>\n<li><a href=\"https://probml.github.io/pml-book/book1.html\">Probabilistic\n    Machine Learning: An Introduction book by Kevin Murphy</a></li>\n<li><a href=\"https://mjt.cs.illinois.edu/dlt/\">Matus\n    Telgarsky's lecture notes on deep learning\n    theory</a>\u00a0</li>\n</ul>\n<p>These are in addition to the papers which will be discussed in\n  the lectures.</p>\n", "course_name": "Theory of Deep Learning", "course_code": "R252", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/R252", "lecturers": [], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "P56": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is a practical course, actively building and extending\n  software tools to observe\u00a0the detailed behaviour of\n  transaction-oriented datacenter-like software. Students\n  will\u00a0observe and understand sources of user-facing tail\n  latency, including that stemming<br/>\n  from resource contention, cross-program interference, bad\n  software locking, and simple\u00a0design errors.<br/>\n  A 2-hour weekly hybrid, lecture/lab format permits students\n  continuous monitored\u00a0progress with complexity of tasks\n  building naturally upon the previous weeks learning.</p>\n<h2>Objectives</h2>\n<p>Upon successful completion of the course, students will be\n  able to:</p>\n<ul>\n<li>Make order-of-magnitude estimates of software, hardware,\n    and I/O speeds</li>\n<li>Make valid measurements of actual software, hardware, and\n    I/O speeds</li>\n<li>Create observation facilities, including logs and\n    dashboards, as part of a software system design</li>\n<li>Create tracing facilities to fully observe the execution of\n    complex software</li>\n<li>Time-align traces across multiple computers</li>\n<li>Display dense tracing information in meaningful ways</li>\n<li>Reason about the sources of real-time and transaction\n    delays, including cross-program\u00a0resource interference,\n    remote procedure call delays, and software locking\n    surprises</li>\n<li>Fix programs based on the above reasoning, making their\n    response times faster and more robust</li>\n</ul>\n<h2><br/>\n  Syllabus</h2>\n<p><u>Measurement</u><br/>\n  \u00a0 \u00a0Week 1 Intro, Measuring CPU time, rdtsc, Measuring\n  memory access times, Measuring disks,<br/>\n  \u00a0 \u00a0Week 2 gettimeofday, logs Measuring networks, Remote\n  Procedure Calls, Multi-threads, locks<br/>\n<u>Observation</u><br/>\n  \u00a0 \u00a0Week 3 RPC, logs, displaying traces,\n  interference<br/>\n  \u00a0 \u00a0Week 4 Antagonist programs, Logging, dashboards,\n  profiling.<br/>\n<u>KUtrace</u><br/>\n  \u00a0 \u00a0Week 5 Kernel patches, hello world,\n  post-processing<br/>\n  \u00a0 \u00a0Week 6 KUtrace multi-CPU time display<br/>\n  \u00a0 \u00a0Week 7 Client-server KUtrace, with antagonists and\n  interference<br/>\n  \u00a0 \u00a0Week 8 Other trace mysteries</p>\n<h2><br/>\n  Assessment</h2>\n<p>This is a Lab-based module; assessment is based upon reports\n  covering guided laboratory\u00a0work performed each\n  week.\u00a0Assessment will be via two submissions:</p>\n<ul>\n<li>20% assignment deadline week 4 based upon Lab work from\n    Weeks 1-4; word target\u00a01000, limit 2000</li>\n<li>80% assignment deadline week 8 based upon lab work from\n    weeks 5-8; word target\u00a02000, limit 4000</li>\n</ul>\n<p>The intent of the first assessment point is to provide rich\n  feedback to students\u00a0based upon a 20% assignment permitting\n  focussed and improved work to be executedfor the final\n  assignment.</p>\n<p>All work in this module is expected to be the effort of the\n  student. Enough equipment\u00a0is provisioned to allow each\n  student an independent set of apparatus. While\n  classmembers\u00a0may find sharing operational experience\n  valuable all assessment is based upon\u00a0a students sole\n  submission based upon. their own experiments and findings.</p>\n<h2>Reading Material</h2>\n<p><u>Core text</u></p>\n<ul>\n<li>Richard L. Sites, <em>Understanding Software Dynamics</em>,\n    Addison-Wesley Professional\u00a0Computing Series, 2022</li>\n</ul>\n<p>Further reading: papers and presentations that you might find\n  interesting. None are\u00a0required reading \u2013 they are\n  well-written and/or informative.</p>\n<ul>\n<li>John K. Ousterhout et al., <em>A trace-driven analysis of\n    the UNIX 4.2 BSD file system\u00a0ACM SIGOPS Operating Systems\n    Review</em>, Vol. 19, No. 5, Dec, 1985 <a href=\"https://dl.acm.org/doi/pdf/10.1145/323627.323631\">https://dl.acm.org/doi/pdf/10.1145/323627.323631</a></li>\n<li>Luiz Andr\u00b4e Barroso, Jimmy Clidaras, Urs H\u00a8olzle, <em>The\n    Datacenter as a Computer:\u00a0An Introduction to the Design of\n    Warehouse-Scale Machines</em>,\u00a0Second Edition</li>\n<li>John L. Hennessy, David A. Patterson, <em>Computer\n    Architecture, A Quantitative\u00a0Approach</em> 5th\n    Edition</li>\n<li>George Varghese, <em>Network Algorithmics</em>. Morgan\n    Kaufmann</li>\n<li>Actually keeping datacenter software up and running \u2013 how\n    Google runs production\u00a0systems Site Reliability\n    Engineering Edited by Betsy Beyer, Chris Jones,\n    Jennifer\u00a0Petoff and Niall Richard Murphy free PDF:\n    <a href=\"https://landing.google.com/sre/book.html\">https://landing.google.com/sre/book.html</a></li>\n<li>How NOT to run datacenters (27 minute video, funny/sad)\n    dotScale 2014 - Robert\u00a0Kennedy - <em>Life in the Trenches\n    of healthcare.gov</em> <a href=\"https://www.youtube.com/watch?v=GLQyj-kBRdo\">https://www.youtube.com/watch?v=GLQyj-kBRdo</a></li>\n</ul>\n<p>An extended (optional) reading list will also be provided.</p>\n", "course_name": "Understanding Networked-Systems Performance", "course_code": "P56", "course_url": "https://www.cl.cam.ac.uk/teaching/2425/P56", "lecturers": ["awm22"], "lectures": 16, "year": "2425", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}}