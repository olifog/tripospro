{"L352": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Advanced Graphics covers topics related to processing,\n  perception and display of images. The focus of the course is on\n  the algorithms behind new emerging display technologies, such as\n  virtual reality, augmented reality, and high dynamic range\n  displays. It complements two computer graphics courses,\n  Introduction to Graphics and Further Graphics, by introducing\n  problems that became the part of graphics pipeline: tone-mapping,\n  post-processing, displays and models of visual perception.</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>GP-GPU</strong>: scheduling and thread mapping,\n    reductions.</li>\n<li><strong>Advanced image processing</strong>: edge-stopping\n    filters, pyramids, optimization-based image processing.</li>\n<li><strong>Beyond 2D</strong>: stereo rendering and light\n    fields.</li>\n<li><strong>Models of visual perception</strong>: visual\n    system, brightness perception, detection and discrimination,\n    contrast sensitivity function, contrast constancy, perceptually\n    uniform spaces, depth perception.</li>\n<li><strong>High Dynamic Range and tone mapping</strong>:\n    dynamic range, display model, methods of tone-mapping.</li>\n<li><strong>Display technologies</strong>: 2D displays, 3D\n    displays, temporal display characteristic, HDR displays.</li>\n<li><strong>Virtual and Augmented Reality</strong>: display\n    technologies, VR rendering, orientation tracking, pose\n    tracking, perceptual considerations, panoramic imaging.</li>\n</ul>\n<p>Please note that the OpenCL lectures may be replaced\u00a0with\n  one that is more graphics-oriented.\u00a0</p>\n<h2>Objectives</h2>\n<p>By the end of the course students should be able to:</p>\n<ul>\n<li>implement real-time image processing methods on a GPU\n    (OpenCL);</li>\n<li>design and implement a tone-mapping algorithm;</li>\n<li>describe the limitations of display technologies (dynamic\n    range, brightness, visual comfort, VR simulation sickness) and\n    how they can be addressed using computational methods\n    (tone-mapping, HDR displays);</li>\n<li>describe the limitations of the visual system and how those\n    limitation can be exploited in computer graphics and image\n    processing.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Two practical exercises, worth 50% of the marks.</li>\n<li>One test, worth 50% of the marks</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Hainich, R. and Bimber, O. (2016) <em>Displays: Fundamentals\n  and Applications</em>. CRC Press (2nd ed.).<br/>\n  Boreskov, A. and Shikin, E. (2013) <em>Computer Graphics: From\n  Pixels to Programmable Graphics Hardware</em>. CRC Press.<br/>\n  Reinhard, E., et. al. (2010) <em>High Dynamic Range Imaging:\n  Acquisition, Display, and Image-Based Lighting</em>. Morgan\n  Kaufmann (2nd ed.).</p>\n", "course_name": "Advanced Graphics and Image Processing", "course_code": "L352", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L352", "lecturers": ["rkm38"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R265": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims to provide students with an introduction to a\n  range of advanced topics in computer architecture. It will\n  explore the current and future challenges facing the architects\n  of modern computers. These will also be used to illustrate the\n  many different influences and trade-offs involved in computer\n  architecture.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should:</p>\n<ul>\n<li>understand the challenges of designing and verifying modern\n    microprocessors</li>\n<li>be familiar with recent research themes and emerging\n    challenges</li>\n<li>appreciate the complex trade-offs at the heart of computer\n    architecture</li>\n</ul>\n<h2>Syllabus</h2>\n<p>Each seminar will focus on a different topic. The proposed\n  topics are listed below but there may be some minor changes to\n  this:</p>\n<ul>\n<li>Trends in computer architecture</li>\n<li>State-of-the-art microprocessor design</li>\n<li>Memory system design</li>\n<li>Hardware reliability</li>\n<li>Specification, verification and test <em>(may be be\n    replaced with a different topic)</em></li>\n<li>Hardware security (2)</li>\n<li>HW accelerators and accelerators for machine learning</li>\n</ul>\n<p>Each two hour seminar will include three student presentations\n  (15mins) questions (5mins) and a broader discussion of the topics\n  (around 30mins). The last part of the seminar will include a\n  short scene setting lecture (around 20mins) to introduce the\n  following week's topic.</p>\n<h2>Assessment</h2>\n<p>Each week students will compare and contrast two of the main\n  papers and submit a written summary and review in advance of each\n  seminar (except when presenting).</p>\n<p>Students will be expected to give a number of 15 minute\n  presentations.</p>\n<p>Essays and presentations will be marked out of 10. After\n  dropping the lowest mark, the remaining marks will be scaled to\n  give a final score out of 100.</p>\n<p>Students will give at least one presentation during the\n  course. They will not be required to submit an essay during the\n  weeks they are presenting.</p>\n<p>Each presentation will focus on a single paper from the\n  reading list. Marks will be awarded for clarity and the\n  communication of the paper's key ideas, an analysis of the work's\n  strengths and weaknesses and the work\u2019s relationship to related\n  work and broader trends and constraints.</p>\n<h2>Recommended prerequisite reading</h2>\n<p>Patterson, D. A., Hennessy, J. L. (2017). <em>Computer\n  organization and design: The Hardware/software interface RISC-V\n  edition</em> Morgan Kaufmann. ISBN 978-0-12-812275-4.</p>\n<p>Hennessy, J. and Patterson, D. (2012). <em>Computer\n  architecture: a quantitative approach</em>. Elsevier (5th ed.)\n  ISBN 9780123838728. (the 3rd and 4th editions are also\n  relevant)</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Advanced Topics in Computer Architecture", "course_code": "R265", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R265", "lecturers": ["rdm34", "swm11", "tmj32"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L108": {"supervisions": 0, "prerequisite_for": ["L118"], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>Category theory provides a unified treatment of mathematical\n  properties and constructions that can be expressed in terms of\n  'morphisms' between structures. It gives a precise framework for\n  comparing one branch of mathematics (organized as a category)\n  with another and for the transfer of problems in one area to\n  another. Since its origins in the 1940s motivated by connections\n  between algebra and geometry, category theory has been applied to\n  diverse fields, including computer science, logic and\n  linguistics. This course introduces the basic notions of category\n  theory: adjunction, natural transformation, functor and category.\n  We will use category theory to organize and develop the kinds of\n  structure that arise in models and semantics for logics and\n  programming languages.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction; some history. Definition of category. The\n    category of sets and functions.</li>\n<li>Commutative diagrams. Examples of categories: preorders and\n    monotone functions; monoids and monoid homomorphisms; a\n    preorder as a category; a monoid as a category. Definition of\n    isomorphism. Informal notion of a 'category-theoretic'\n    property.</li>\n<li>Terminal objects. The opposite of a category and the\n    duality principle. Initial objects. Free monoids as initial\n    objects.</li>\n<li>Binary products and coproducts. Cartesian categories.</li>\n<li>Exponential objects: in the category of sets and in\n    general. Cartesian closed categories: definition and\n    examples.</li>\n<li>Intuitionistic Propositional Logic (IPL) in Natural\n    Deduction style. Semantics of IPL in a cartesian closed\n    preorder.</li>\n<li>Simply Typed Lambda Calculus (STLC). The typing relation.\n    Semantics of STLC types and terms in a cartesian closed\n    category (ccc). The internal language of a ccc. The\n    Curry-Howard-Lawvere correspondence.</li>\n<li>Functors. Contravariance. Identity and composition for\n    functors.</li>\n<li>Size: small categories and locally small categories. The\n    category of small categories. Finite products of\n    categories.</li>\n<li>Natural transformations. Functor categories. The category\n    of small categories is cartesian closed.</li>\n<li>Hom functors. Natural isomorphisms. Adjunctions. Examples\n    of adjoint functors. Theorem characterising the existence of\n    right (respectively left) adjoints in terms of a universal\n    property.</li>\n<li>Dependent types. Dependent product sets and dependent\n    function sets as adjoint functors. Equivalence of categories.\n    Example: the category of I-indexed sets and functions is\n    equivalent to the slice category Set/I.</li>\n<li>Presheaves. The Yoneda Lemma. Categories of presheaves are\n    cartesian closed.</li>\n<li>Monads. Modelling notions of computation as monads. Moggi's\n    computational lambda calculus.</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with some of the basic notions of category\n    theory and its connections with logic and programming language\n    semantics</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>a graded exercise sheet (25% of the final mark), and</li>\n<li>a take-home test (75%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Awodey, S. (2010). <em>Category theory</em>. Oxford University\n  Press (2nd ed.).</p>\n<p>Crole, R. L. (1994). <em>Categories for types</em>. Cambridge\n  University Press.</p>\n<p>Lambek, J. and Scott, P. J. (1986). <em>Introduction to higher\n  order categorical logic</em>. Cambridge University Press.</p>\n<p>Pitts, A. M. (2000). <em>Categorical Logic</em>. Chapter 2 of\n  S. Abramsky, D. M. Gabbay and T. S. E. Maibaum (Eds) Handbook of\n  Logic in Computer Science, Volume 5. Oxford University Press.\n  (Draft copy available <a href=\"http://www.cl.cam.ac.uk/~amp12/papers/catl/catl.pdf\">here</a>.)</p>\n<h2>Class Size</h2>\n<p>This module can accommodate upto 15 Part II students plus 15\n  MPhil / Part III students.</p>\n", "course_name": "Category Theory", "course_code": "L108", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L108", "lecturers": ["amp12"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R209": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims to provide students with an introduction to\n  the history and central themes of computer security, from its\n  1970s foundations to some current research topics, with a theme\n  of how to defend cloud-based systems against capable motivated\n  opponents. The course considers first local computer systems and\n  then distributed systems; however, we will rapidly discover that\n  this is an artificial distinction that only becomes more awkward\n  as we enter the current period. Throughout the course, we will\n  consider proposed systems along with the adversarial research\n  intended to identify gaps and vulnerabilities.</p>\n<h2>Syllabus</h2>\n<p>There will be eight two-hour seminars on topics along the\n  lines of the following. Students are expected to read the\n  required set papers before each class. All students are expected\n  to submit a brief written summary of the readings in advance of\n  each class, and students will be nominated to give brief\n  presentations of each paper, or of cross-cutting aspects of all\n  the papers, to lead discussion.</p>\n<ul>\n<li>Origins and foundations of computer security</li>\n<li>Adversarial Reasoning</li>\n<li>Access control</li>\n<li>Security economics</li>\n<li>Passwords</li>\n<li>Capability systems</li>\n<li>Cryptographic protocols</li>\n<li>Correctness vs. mitigation</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the principles of computer security</li>\n<li>be familiar with long-term and recent research themes</li>\n<li>appreciate the challenges of defending high-value\n    systems</li>\n</ul>\n<h2>Coursework</h2>\n<p>Participants will be expected to undertake six hours of\n  preparatory work before each meeting. This will involve:</p>\n<ul>\n<li>Reading a set number of papers</li>\n<li>Following up references and other related work</li>\n<li>Writing a weekly essay summarising assigned papers\n    <strong>or</strong>, as assigned by the course instructor,\n    preparing and delivering a 20-minute presentation on a specific\n    paper</li>\n<li>Essay text or presentation slides must be submitted via\n    Moodle, by the specified deadline</li>\n<li>Participating in class discussion on both the assigned\n    papers and broader issues raised by the week's readings</li>\n</ul>\n<p>Each week, the lecturers will lead an interactive session to\n  discuss the assigned reading material, drawing on the\n  presentations and essays submitted by the students before the\n  start of the class.</p>\n<p>Weekly essays will be up to 1,250 words summarising the\n  complete set of assigned papers, identifying common themes,\n  discussing the broader context, and enumerating possible class\n  discussion topics. While essays need not be 1,250 words in\n  length, participants are advised that essays under 1000 words are\n  unlikely to contain sufficient detail or discussion to achieve\n  full marks.</p>\n<p>All participants are expected to attend and participate in\n  every class; the instructor must be notified of any absences in\n  advance.</p>\n<h2>Practical work</h2>\n<p>None</p>\n<h2>Assessment</h2>\n<p>From the second week onwards, course participants are awarded\n  a maximum of 10 marks each week reflecting the quality of the\n  submitted essay or presentation. The lowest essay or presentation\n  mark of the term will be dropped. Remaining marks will be scaled\n  to a maximum final score out of 100.</p>\n<p>For essays, a total of ten marks can be awarded. Up to two\n  marks are assigned for adequate coverage of each of five\n  sections/areas: summary of papers; discussion of key themes\n  spanning the papers; consideration of current context; literature\n  review; and class discussion questions.</p>\n<p>For presentations, a total of ten marks can be awarded.\n  Criteria include: effective teaching of the key ideas; a critical\n  evaluation of the work; tracing related research; considering\n  current implications vs historical context of the work; and\n  successful answering of Q&amp;A as well as triggering a useful\n  and interesting class discussion.</p>\n<p>Neither essays nor presentations are due in the first week.\n  All submitted essays should provide a word count.</p>\n<h2>Recommended reading</h2>\n<p>Anderson, R. J. (2020). <em>Security Engineering</em>, Wiley\n  (third edition)<br/>\n  Gollmann, D. (2010). <em>Computer Security</em>, Wiley<br/>\n  Marshall Kirk McKusick, George V. Neville-Neil, and Robert N. M.\n  Watson. 'Chapter 5 - Security', The Design and Implementation of\n  the FreeBSD Operating System, 2nd Edition, Pearson Education,\n  Boston, MA, USA, September 2014</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Computer Security: Principles and Foundations", "course_code": "R209", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R209", "lecturers": ["rja14", "rnw24", "ah793"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L314": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course teaches the basic signal-processing principles\n  necessary to understand many modern high-tech systems, with\n  application examples focussing on audio processing, image coding,\n  communication systems and software-defined radio. Students will\n  gain practical experience from numerical experiments in\n  programming assignments (in MATLAB, NumPy or Julia).</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Signals and systems.</strong>\u00a0Discrete\n    sequences and systems: types and properties. Amplitude, phase,\n    frequency, modulation, decibels, root-mean square. Linear\n    time-invariant systems, convolution. Some examples from\n    electronics, optics and acoustics.</li>\n<li><strong>Phasors.</strong>\u00a0Eigen functions of linear\n    time-invariant systems. Review of complex arithmetic. Phasors\n    as orthogonal base functions.</li>\n<li><strong>Fourier transform.</strong>\u00a0Forms and\n    properties of the Fourier transform. Convolution theorem. Rect\n    and sinc.</li>\n<li><strong>Dirac\u2019s delta function.</strong>\u00a0Fourier\n    representation of sine waves, impulse combs in the time and\n    frequency domain. Amplitude-modulation in the frequency\n    domain.</li>\n<li><strong>Discrete sequences and\n    spectra.</strong>\u00a0Sampling of continuous signals, periodic\n    signals, aliasing, interpolation, sampling and reconstruction,\n    sample-rate conversion, oversampling, spectral inversion.</li>\n<li><strong>Discrete Fourier\n    transform.</strong>\u00a0Continuous\u00a0<em>versus</em>\u00a0discrete\n    Fourier transform, symmetry, linearity, FFT, real-valued FFT,\n    FFT-based convolution, zero padding, FFT-based resampling,\n    deconvolution exercise.</li>\n<li><strong>Spectral estimation.</strong>\u00a0Short-time\n    Fourier transform, leakage and scalloping phenomena, windowing,\n    zero padding. Audio and voice examples. DTFM exercise.</li>\n<li><strong>Finite impulse-response\n    filters.</strong>\u00a0Properties of filters, implementation\n    forms, window-based FIR design, use of frequency-inversion to\n    obtain high-pass filters, use of modulation to obtain band-pass\n    filters.</li>\n<li><strong>Infinite impulse-response\n    filters.</strong>\u00a0Sequences as\n    polynomials,\u00a0<em>z</em>-transform, zeros and poles, some\n    analog IIR design techniques (Butterworth, Chebyshev I/II,\n    elliptic filters, second-order cascade form).</li>\n<li><strong>Band-pass signals.</strong>\u00a0Band-pass sampling\n    and reconstruction, IQ up and down conversion, superheterodyne\n    receivers, software-defined radio front-ends, IQ representation\n    of AM and FM signals and their demodulation.</li>\n<li><strong>Digital\n    communication.</strong>\u00a0Pulse-amplitude modulation.\n    Matched-filter detector. Pulse shapes, inter-symbol\n    interference, equalization. IQ representation of ASK, BSK, PSK,\n    QAM and FSK signals.\u00a0[2 hours]</li>\n<li><strong>Random sequences and noise.</strong>\u00a0Random\n    variables, stationary and ergodic processes, autocorrelation,\n    cross-correlation, deterministic cross-correlation sequences,\n    filtered random sequences, white noise, periodic\n    averaging.</li>\n<li><strong>Correlation coding.</strong>\u00a0Entropy, delta\n    coding, linear prediction,\n    dependence\u00a0<em>versus</em>\u00a0correlation, random\n    vectors, covariance, decorrelation, matrix diagonalization,\n    eigen decomposition, Karhunen-Lo\u00e8ve transform, principal\n    component analysis. Relation to orthogonal transform coding\n    using fixed basis vectors, such as DCT.</li>\n<li><strong>Lossy versus lossless\n    compression.</strong>\u00a0What information is discarded by\n    human senses and can be eliminated by encoders? Perceptual\n    scales, audio masking, spatial resolution, colour coordinates,\n    some demonstration experiments.</li>\n<li><strong>Quantization, image coding\n    standards.</strong>\u00a0Uniform and logarithmic quantization,\n    A/\u00b5-law coding, dithering, JPEG.</li>\n</ul>\n<h2>Objectives</h2>\n<ul>\n<li>apply basic properties of time-invariant linear\n    systems;</li>\n<li>understand sampling, aliasing, convolution, filtering, the\n    pitfalls of spectral estimation;</li>\n<li>explain the above in time and frequency domain\n    representations;</li>\n<li>use filter-design software;</li>\n<li>visualize and discuss digital filters in\n    the\u00a0<em>z</em>-domain;</li>\n<li>use the FFT for convolution, deconvolution, filtering;</li>\n<li>implement, apply and evaluate simple DSP applications;</li>\n<li>familiarity with a number of signal-processing concepts\n    used in digital communication systems</li>\n</ul>\n<h2>Assessment - Part II students</h2>\n<ul>\n<li>Three homework programming assignments, each comprising 20%\n    of the mark</li>\n<li>Written take-home assignment, comprising 40% of the total\n    mark.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Lyons, R.G. (2010).\u00a0<em>Understanding digital signal\n  processing.</em>\u00a0Prentice Hall (3rd ed.).<br/>\n  Oppenheim, A.V. and Schafer, R.W. (2007).\u00a0<em>Discrete-time\n  digital signal processing.</em>\u00a0Prentice Hall (3rd ed.).<br/>\n  Stein, J. (2000).\u00a0<em>Digital signal processing \u2013 a computer\n  science perspective.</em>\u00a0Wiley.</p>\n", "course_name": "Digital Signal Processing", "course_code": "L314", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L314", "lecturers": ["mgk25"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "P230": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is an advanced course in human-computer interaction, with\n  a specialist focus on intelligent user interfaces and interaction\n  with machine-learning and artificial intelligence technologies.\n  The format will be largely Practical, with students carrying out\n  a mini-project involving empirical research investigation. These\n  studies will investigate human interaction with some kind of\n  model-based system for planning, decision-making, automation etc.\n  Possible study formats might include: System evaluation, Field\n  observation, Hypothesis testing experiment, Design intervention\n  or Corpus analysis, following set examples from recent research\n  publications. Project work will be formally evaluated through a\n  report and presentation.</p>\n<h2>Lectures</h2>\n<p>(note that Lectures 2-7 also include one hour class discussion\n  of practical work)<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Current research\n  themes in intelligent user interfaces<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Program\n  synthesis<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Mixed initiative\n  interaction<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Interpretability /\n  explainable AI<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Labelling as a\n  fundamental problem<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Machine learning\n  risks and bias<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Visualisation and\n  visual analytics<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 Student research\n  presentations</p>\n<h2>Objectives</h2>\n<p>By the end of the course students should:<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be familiar with\n  current state of the art in intelligent interactive systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 understand the human\n  factors that are most critical in the design of such systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to evaluate\n  evidence for and against the utility of novel systems<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 have experience of\n  conducting user studies meeting the quality criteria of this\n  field<br/>\n  \u00a0\u00a0\u00a0\u00a0\u00a0\u00a0\u00a0 \u2022 be able to write up\n  and present user research in a professional manner</p>\n<h2>Assessment - Part II Students</h2>\n<p>The format will be largely practical, with students carrying\n  out an individual mini-project involving empirical research\n  investigation.</p>\n<p>Assignment 1: six incremental submissions which\u00a0together\n  contribute 20% to the final module mark.</p>\n<p>Assignment 2: Final report - 80% of the final module mark</p>\n<h2>Recommended reading</h2>\n<p>Brad A. Myers and Richard McDaniel (2000). <a href=\"http://web.media.mit.edu/~lieber/Your-Wish/03-Myers.pdf\">Demonstrational\n  Interfaces: Sometimes You Need a Little Intelligence, Sometimes\n  You Need a Lot</a>.<br/>\n  \u00a0</p>\n", "course_name": "Interaction with Machine Learning", "course_code": "P230", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/P230", "lecturers": ["afb21"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L98": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This is a lecture-style course that introduces students to\n  various aspects of the semantics of\u00a0Natural Languages\n  (mainly English):</p>\n<ul>\n<li>Lexical Semantics, with an emphasis on theory and\n    phenomenology (4 sessions)</li>\n<li>Compositional Semantics (9 sessions)</li>\n<li>Discourse and pragmatics-related aspects of semantics (3\n    sessions)</li>\n</ul>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Give an operational definition of what is meant by\n    \u201cmeaning\u201d (for instance, above and\u00a0beyond syntax);</li>\n<li>Name the types of phenomena in language that require\n    semantic consideration, in terms\u00a0of lexical, compositional\n    and discourse/pragmatic aspects, in other words, argue\n    why\u00a0semantics is important;</li>\n<li>Demonstrate an understanding of the basics of various\n    semantic representations,\u00a0including logic-based and\n    graph-based semantic representations, their properties,\n    how\u00a0they are used and why they are important, and how they\n    are different from syntactic\u00a0representations;</li>\n<li>Know how such semantic representations are derived during\n    or after parsing, and how\u00a0they can be analysed and mapped\n    to surface strings;</li>\n<li>Understand applications of semantic representations e.g.\n    reasoning, validation, and\u00a0methods how these are\n    approached.</li>\n<li>When designing NL tasks that clearly require semantic\n    processing (e.g.\u00a0knowledge-based QA), to be aware of and\n    reuse semantic representations and\u00a0algorithms when\n    designing the task, rather than reinventing the wheel.</li>\n</ul>\n<h2>Practical advantages of this course for NLP students</h2>\n<ul>\n<li>Knowledge of underlying semantic effects helps improve NLP\n    evaluation, for instance by\u00a0providing more meaningful\n    error analysis. You will be able to link particular errors\n    to\u00a0design decisions inside your system.</li>\n<li>You will learn methods for better benchmarking of your\n    system, whatever the task may\u00a0be. Supervised ML systems\n    (in particular black-box systems such as Deep Learning)\n    are\u00a0only as clever as the datasets they are based on. In\n    this course, you will learn to design\u00a0datasets so that\n    they are harder to trick without real understanding, or\n    critique existing\u00a0datasets.</li>\n<li>You will be able to design tests for ML systems that better\n    pinpoint which aspects of\u00a0language an end-to-end system\n    has \u201cunderstood\u201d.</li>\n<li>You will learn to detect ambiguity and ill-formed semantics\n    in human-human\u00a0communication. This can serve to write more\n    clearly and logically.</li>\n<li>You will learn about decomposing complex semantics-reliant\n    tasks sensibly so that you\u00a0can reuse the techniques\n    underlying semantic analyzers in a modular way. In this\n    way,\u00a0rather than being forced to treat complex tasks in an\n    end-to-end manner, you will be able\u00a0to profit from partial\n    explanations and a better error analysis already built into\n    the\u00a0system.</li>\n</ul>\n<h2>Syllabus</h2>\n<ol>\n<li>Introduction</li>\n<li>Event structure</li>\n<li>Discourse references</li>\n<li>Word sense</li>\n<li>Truth-condition semantics</li>\n<li>Compositionality and syntax-semantics interface</li>\n<li>Weakly compositional phenomena\u00a0</li>\n<li>Compositional distributional semantics</li>\n<li>Clause union</li>\n<li>Scope</li>\n<li>Graph-based meaning representation</li>\n<li>Information Structure</li>\n<li>Speech act</li>\n<li>Presupposition</li>\n<li>Coherence</li>\n<li>Meaning representation-mediated NLU\u00a0</li>\n</ol>\n<h2>Assessment</h2>\n<ul>\n<li>Two short tasks\u00a0(10% each)</li>\n<li>Project (80%)</li>\n</ul>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Introduction to Computational Semantics", "course_code": "L98", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L98", "lecturers": ["ws390", "sht25"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L95": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide a brief introduction to\n  linguistics for computer scientists and then goes on to cover\n  some of the core tasks in natural language processing (NLP),\n  focussing on statistical parsing of sentences to yield syntactic\n  and semantic representations. We will look at how to evaluate\n  parsers and see how well state-of-the-art tools perform given\n  current techniques.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Linguistics for NLP - morphology, syntax, semantics</li>\n<li>Parsing - grammars, treebanks, representations and\n    evaluation, statistical parse\u00a0</li>\n<li>Interpretation - compositional semantics\u00a0</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the basic properties of human languages and be\n    familiar with descriptive and theoretical frameworks for\n    handling these properties;</li>\n<li>understand the design of tools for NLP tasks such as\n    parsing and be able to apply them to text and evaluate their\n    performance;</li>\n<li>understand some of the basic principles of the\n    representation of linguistic meaning.</li>\n</ul>\n<h2>Practical work</h2>\n<ul>\n<li>Week 1-7: There will be non-assessed practical exercises\n    between sessions and during sessions.</li>\n<li>Week 8: Download and apply two parsers to a designated\n    text. Evaluate the performance of the tools quantitatively and\n    qualitatively.</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>There will be one\u00a0presentation\u00a010% of the final\n    mark; presentation topics will be allocated at the start of\n    term.</li>\n<li>An assessed practical report\u00a0of not more than 5000\n    words. It will contribute 90% of the final mark.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Jurafsky, D. and Martin, J. (2008). <em>Speech and Language\n  Processing</em>. Prentice-Hall (2nd ed.). (See also 3rd ed.\n  available online.)</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Introduction to Natural Language Syntax and Parsing", "course_code": "L95", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L95", "lecturers": ["pjb48"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L310": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course teaches the foundations of autonomous mobile\n  robots, covering topics such as perception, motion control, and\n  planning. It also teaches algorithmic strategies that enable the\n  coordination of multi-robot systems and robot swarms. The course\n  will feature several practical sessions with hands-on robot\n  programming. The students will undertake mini-projects, which\n  will be formally evaluated through a report and presentation.</p>\n<h2>Lectures</h2>\n<p>Lectures (2 lectures per week):</p>\n<ol>\n<li>Introduction (A. Prorok) -- Oct. 7<br/>\n    a. Why study robotics?<br/>\n    b. The basics of mobile autonomy<br/>\n    c. History of robotics research</li>\n<li>Architectures (A. Prorok) -- Oct. 14<br/>\n    a. Autonomy and sensor-actuator loops<br/>\n    b. Reactive vs deliberative decision-making (and control)<br/>\n    c. Control architectures</li>\n<li>Introduction to kinematics (F. Forni and F. Iida) -- Oct.\n    21<br/>\n    a. Motion models; robots with non-holonomic constraints<br/>\n    b. Kinematics; forward and inverse kinematics<br/>\n    c. Open-loop vs closed-loop control; intro to PID control.</li>\n<li>Introduction to dynamics (F. Iida and F. Forni) -- Oct.\n    28<br/>\n    a. Dynamics models<br/>\n    b. Open-loop and closed-loop control<br/>\n    c. PID control applied to dynamic systems.</li>\n<li>Perception and Localization (R. Harle) -- Nov. 4<br/>\n    a. Sensors and sensor models, odometry<br/>\n    b. Maximum likelihood estimation and sensor fusion<br/>\n    c. Noise and belief representation<br/>\n    d. Bayes rule, Bayes filter, Particle Filter, KF<br/>\n    e. Grid localization and map representations</li>\n<li>Navigation and Planning (A. Prorok) -- Nov. 11<br/>\n    a. Basic concepts<br/>\n    b. Reactive navigation (without a roadmap)<br/>\n    c. Deliberative planning (with a roadmap)<br/>\n    d. Planning in multi-robot systems</li>\n<li>Multi-Robot Systems (A. Prorok) -- Nov.18<br/>\n    a. Introduction to Multi-Robot Systems (MRS)<br/>\n    b. Centralized vs decentralized architectures<br/>\n    c. Collective movement (formations, flocking)<br/>\n    d. Task assignment</li>\n<li>Introduction to Advanced Robotics (A. Prorok) -- Nov.\n    25<br/>\n    a. Introduction to reinforcement learning methods<br/>\n    b. Model-based vs model-free approaches<br/>\n    c. Open robotics problems</li>\n</ol>\n<p>Pre-recorded material is available here:<br/>\n<a href=\"https://www.youtube.com/playlist?list=PLaTKfS3-bDpDyOwrxLcQRGxY9XJw33ANo\">\n  https://www.youtube.com/playlist?list=PLaTKfS3-bDpDyOwrxLcQRGxY9XJw33ANo</a></p>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>understand how to control a mobile robot;</li>\n<li>understand how a robot perceives its environment;</li>\n<li>understand how a robot plans actions (navigation\n    paths);</li>\n<li>know paradigms of coordination in systems of multiple\n    robots;</li>\n<li>know classical multi-robot problems and their solution\n    methods;</li>\n<li>Know how to use ROS (Robot Operating System, <a href=\"http://www.ros.org\" id=\"tex2html21\" name=\"tex2html21\">http://www.ros.org</a>).</li>\n</ul>\n<h2>Assessment - Part II Students:</h2>\n<p>For undergraduate students, the assignments will be 100%\n  coursework and consist of two elements: (1) experimental work\n  using a robot simulator and real robots, and (2) theory /\n  understanding. The exercises will require data collection and\n  analysis. The balance between practice and theory will depend on\n  the exercise topic. Each student will submit a written report.\n  Students will be expected to be able to demonstrate any results\n  reported in their hand-in.<br/>\n  Assessment: Each assignment will compose 45% of the final mark;\n  the remaining 10% of the mark will be determined by the student's\n  performance in a 1-on-1 presentation with either the lecturer or\n  a senior assessor. The mark for each assignment will be\n  determined in part by the score achieved in the written report,\n  and in part by the performance of the student during a\n  questioning session. The lecturers will hold an in-person\n  questioning session with each student to discuss their\n  submissions. Submissions are non-anonymous.</p>\n<p>Assignment 1:\u00a0Covers material from Lectures 1-4<br/>\n  Assignment 2:\u00a0Covers material from Lectures 5-8</p>\n<p>Presentation: One-on-one presentation of assignments.</p>\n<h2>Recommended reading</h2>\n<p>Siegwart, R., Nourbakhsh, I.R. and Scaramuzza, D. (2004).\n  <em>Autonomous mobile robots</em>. MIT Press.<br/>\n  Thrun, S., Wolfram B. and Dieter F. (2005). <em>Probabilistic\n  robotics</em>. MIT Press.<br/>\n  Mondada, F. and Mordechai B. (2018) <em>Elements of\n  Robotics</em>. Springer<br/>\n  Siciliano, B. and Khatib, O. (2016) <em>Springer handbook of\n  robotics</em>. Springer.<br/>\n  Mesbahi, M. and Egerstedt, M. (2010) <em>Graph theoretic methods\n  in multiagent networks</em>. Princeton University Press.</p>\n", "course_name": "Introduction to Robotics", "course_code": "L310", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L310", "lecturers": ["asp45", "rkh23"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R244": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module provides an introduction to large-scale data\n  processing, optimisation, and the impact on computer system's\n  architecture. Large-scale distributed applications with high\n  volume data processing such as training of machine learning will\n  grow ever more in importance. Supporting the design and\n  implementation of robust, secure, and heterogeneous large-scale\n  distributed systems is essential. To deal with distributed\n  systems with a large and complex parameter space, tuning and\n  optimising computer systems is becoming an important and complex\n  task, which also deals with the characteristics of input data and\n  algorithms used in the applications. Algorithm designers are\n  often unaware of the constraints imposed by systems and the best\n  way to consider these when designing algorithms with massive\n  volume of data. On the other hand, computer systems often miss\n  advances in algorithm design that can be used to cut down\n  processing time and scale up systems in terms of the size of the\n  problem they can address. Integrating machine learning approaches\n  (e.g. Bayesian Optimisation, Reinforcement Learning) for system\n  optimisation will be explored in this course.</p>\n<h2>Syllabus</h2>\n<p>This course provides perspectives on large-scale data\n  processing, including data-flow programming, graph data\n  processing, probabilistic programming and computer\n  system\u00a0optimisation, especially using machine learning\n  approaches, thus providing a solid basis to work on\u00a0the next\n  generation of distributed systems.</p>\n<p>The module consists of 8 sessions, with 5 sessions on specific\n  aspects of large-scale data processing\u00a0research. Each\n  session discusses 3-4 papers, led by the assigned students. One\n  session is a hands-on\u00a0tutorial on MapReduce using data flow\n  programming of\u00a0Deep Neural Networks training\u00a0using\n  Google TensorFlow also Bayersian Optimisation basics. The first\n  session advises on how to\u00a0read/review a paper together with\n  a brief introduction on different perspectives in large-scale\n  data<br/>\n  processing and optimisation. The last session is dedicated to the\n  student presentation of opensource\u00a0project studies.</p>\n<ol>\n<li>Introduction to large-scale data processing and\n    optimisation</li>\n<li>Data flow programming: Map/Reduce to TensorFlow</li>\n<li>Large-scale graph data processing: storage, processing\n    model and parallel processing</li>\n<li>Map/Reduce and Deep Neural Network using TensorFlow\n    hands-on tutorial</li>\n<li>Probabilistic Programming</li>\n<li>Many Aspects of Optimisation in\u00a0Computer Systems</li>\n<li>Optimisation of Computer Systems using ML</li>\n<li>Presentation of Open Source Project Study</li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Understand key concepts of scalable data processing\n    approaches in future computer systems.</li>\n<li>Obtain a clear understanding of building distributed\n    systems using data centric programming and large-scale data\n    processing.</li>\n<li>Understand a large and complex parameter space in computer\n    system's optimisation and applicability of Machine Learning\n    approach.</li>\n</ul>\n<h2>Coursework</h2>\n<h3>Reading Club:</h3>\n<ul>\n<li>The preparation for the reading club will involve 1-3\n    papers every week. At each session, around 3-4 papers are\n    selected under the given topic, and the students present their\n    review work.</li>\n<li>Hands-on tutorial session of data flow programming\n    including writing an application of processing streaming in\n    Twitter data and/or Deep Neural Networks using Google\n    TensorFlow using cluster computing.</li>\n</ul>\n<h3>Reports</h3>\n<p>The following three reports are required, which could be\n  extended from the assignment of the reading club, within the\n  scope of data centric systems.</p>\n<ol>\n<li>Review report on a full length paper (max 1800 words)\n      <ul>\n<li>Describe the contribution of the paper in depth with\n        criticisms</li>\n<li>Crystallise the significant novelty in contrast to\n        other related work</li>\n<li>Suggestions for future work</li>\n</ul>\n</li>\n<li>Survey report on sub-topic in large-scale data processing\n    and optimisation (max 2000 words)\n      <ul>\n<li>Pick up to 5 papers as core papers in the survey\n        scope</li>\n<li>Read the above and expand reading through related\n        work</li>\n<li>Comprehend the view and finish an own survey paper</li>\n</ul>\n</li>\n<li>Project study and exploration of a prototype (max 2500\n    words)\n      <ul>\n<li>What is the significance of the project in the research\n        domain?</li>\n<li>Compare with similar and succeeding projects</li>\n<li>Demonstrate the project by exploring its prototype</li>\n</ul>\n</li>\n</ol>\n<p>Reports 1 and 2 should be handed in by the end of 5th week and\n  7th week of the course. Report 3 should be handed in by the end\n  of the Michaelmas Term.</p>\n<h2>Assessment</h2>\n<p>The final grade for the course will be provided as a\n  percentage, and the assessment will consist of two parts:</p>\n<ol>\n<li>25%: for reading club (participation, presentation)</li>\n<li>75%: for the three reports:\n      <ul>\n<li>15%: Intensive review report</li>\n<li>25%: Survey report</li>\n<li>35%: Project study</li>\n</ul>\n</li>\n</ol>\n<h2>Recommended reading</h2>\n<ol>\n<li>M. Abadi et al. TensorFlow: A System for Large-Scale\n    Machine Learning, OSDI, 2016.</li>\n<li>D. Aken et al.: Automatic Database Management System Tuning\n    Through Large-scale Machine Learning, SIGMOD, 2017.</li>\n<li>J. Ansel et al. Opentuner: an extensible framework for\n    program autotuning. PACT, 2014.</li>\n<li>V. Dalibard, M. Schaarschmidt, E. Yoneki. BOAT: Building\n    Auto-Tuners with Structured Bayesian Optimization, WWW,\n    2017.</li>\n<li>J. Dean et al. Large scale distributed deep networks. NIPS,\n    2012.</li>\n<li>G. Malewicz, M. Austern, A. Bik, J. Dehnert, I. Horn, N.\n    Leiser, G. Czajkowski. Pregel: A System for Large-Scale Graph\n    Processing, SIGMOD, 2010.</li>\n<li>A. Mirhoseini et al. Device Placement Optimization with\n    Reinforcement Learning, ICML, 2017.</li>\n<li>D. Murray, F. McSherry, R. Isaacs, M. Isard, P. Barham, M.\n    Abadi. Naiad: A Timely Dataflow System, SOSP, 2013.</li>\n<li>M. Schaarschmidt, S. Mika, K. Fricke and E. Yoneki:\n    RLgraph: Modular Computation Graphs for Deep Reinforcement\n    Learning, SysML, 2019.</li>\n<li>Z. Jia, O. Padon, J. Thomas, T. Warszawski, M.\n    Zaharia,\u00a0 A. Aiken: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/jia_SOSP_2019.pdf\" style=\"color:#0563c1; text-decoration:underline\" target=\"_blank\">TASO: Optimizing Deep Learning Computation with\n    Automated Generation of Graph Substitutions</a>: SOSP,\n    2019.</li>\n<li>H. Mao et al.: <a href=\"https://www.cl.cam.ac.uk/~ey204/teaching/ACS/R244_2020_2021/papers/mao_OR_2019.pdf\" style=\"color:#0563c1; text-decoration:underline\" target=\"_blank\">Park: An Open Platform for Learning-Augmented Computer\n    Systems</a>, OpenReview, 2019.</li>\n</ol>\n<p>A complete list can be found on the course material web page.\n  See also 2019-2020 course material on the previous course\n  <a href=\"https://www.cl.cam.ac.uk/teaching/1920/R244/materials.html\" target=\"_blank\">Large-Scale Data Processing and\n  Optimisation</a>.</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Large-scale data processing and optimisation", "course_code": "R244", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R244", "lecturers": ["ey204"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L48": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The module \u201cMachine Learning and the Physical World\u201d is\n  focused on machine learning systems that interact directly with\n  the real world. Building artificial systems that interact with\n  the physical world have significantly different challenges\n  compared to the purely digital domain. In the real world data is\n  scares, often uncertain and decisions can have costly and\n  irreversible consequences. However, we also have the benefit of\n  centuries of scientific knowledge that we can draw from. This\n  module will provide the methodological background to machine\n  learning applied in this scenario. We will study how we can build\n  models with a principled treatment of uncertainty, allowing us to\n  leverage prior knowledge and provide decisions that can be\n  interrogated.</p>\n<p>There are three principle points about machine learning in the\n  real world that will concern us.</p>\n<ol>\n<li>We often have a mechanistic understanding of the real world\n    which we should be able to bootstrap to make decisions. For\n    example, equations from physics or an understanding of\n    economics.</li>\n<li>Real world decisions have consequences which may have\n    costs, and often these cost functions need to be assimilated\n    into our machine learning system.</li>\n<li>The real world is surprising, it does things that you do\n    not expect and accounting for these challenges requires us to\n    build more robust and or interpretable systems.</li>\n</ol>\n<p>Decision making in the real world hasn\u2019t begun only with the\n  advent of machine learning technologies. There are other domains\n  which take these areas seriously, physics, environmental\n  scientists, econometricians, statisticians, operational\n  researchers. This course identifies how machine learning can\n  contribute and become a tool within these fields. It will equip\n  you with an understanding of methodologies based on uncertainty\n  and decision making functions for delivering on these\n  challenges.</p>\n<h2>Objectives</h2>\n<p>You will gain detailed knowledge of</p>\n<ul>\n<li>surrogate models and uncertainty</li>\n<li>surrogate-based optimization</li>\n<li>sensitivity analysis</li>\n<li>experimental design</li>\n</ul>\n<p>You will gain knowledge of</p>\n<ul>\n<li>counterfactual analysis</li>\n<li>surrogate-based quadrature</li>\n</ul>\n<h2>Schedule</h2>\n<p>Week 1:\u00a0Introduction to the unit and foundation of\n  probabilistic modelling</p>\n<p>Week 2:\u00a0Gaussian processes and probablistic inference</p>\n<p>Week 3:\u00a0Simulation and Sequential decision making under\n  uncertainty</p>\n<p>Week 4:\u00a0Emulation and Experimental Design</p>\n<p>Week 5:\u00a0Sensitivity Analysis and Multifidelity\n  Modelling</p>\n<p>Week 6-8:\u00a0Case studies of applications and Projects</p>\n<h2>Practical work</h2>\n<p>During the first five weeks of the unit we will provide a\n  weekly worksheet that will focus on implementation and practical\n  exploration of the material covered in the lectures. The\n  worksheets will allow you to build up a these methods without\n  relying on extensive external libraries. You are free to use any\n  programming language of choice however we highly recommended the\n  use of =Python=.</p>\n<h2>Assessment</h2>\n<p>This unit will be assessed using a group project. Each group\n  will work on an application of uncertainty that covers the\n  material of the first 5 weeks of lectures in the unit. Each group\n  will submit a report which will form the basis of the assessment.\n  In addition to the report each group will also attend a short\n  oral examination based on the material covered both in the report\n  and the taught material.</p>\n<h2>Recommended Reading</h2>\n<p>Rasmussen, C. E. and Williams, C. K. I. (2006). <em>Gaussian\n  Processes for Machine Learning</em>. MIT Press</p>\n<p>Bishop, C. (2006). <em>Pattern recognition and machine\n  learning</em>. Springer.<br/>\n<a href=\"https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf\" target=\"_blank\">https://www.microsoft.com/en-us/research/uploads/prod/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf</a></p>\n<p>Laplace, P. S. (1902). <em>A Philosophical Essay on\n  Probabilities</em>. John Wiley &amp; Sons.<br/>\n<a href=\"https://archive.org/details/philosophicaless00lapliala\" target=\"_blank\">https://archive.org/details/philosophicaless00lapliala</a></p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Machine Learning and the Physical World", "course_code": "L48", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L48", "lecturers": ["ndl21"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L101": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide an introduction to machine\n  learning with specific application to tasks such as document\n  classification, spam email filtering, language modelling,\n  part-of-speech tagging, and named entity and event recognition\n  for textual information extraction. We will cover supervised,\n  weakly-supervised and unsupervised approaches using generative\n  and discriminative linear and non-linear classifiers, such as\n  Naive Bayes, Perceptron, Multi-Layer Perceptron, Logistic\n  Regression, clustering / dimensionality-reduction methods, such\n  as latent Dirichlet allocation and neural word embeddings.</p>\n<h2>Syllabus</h2>\n<p>Classification by machine learning: classification, types of\n  classifier, generative vs. discriminative models,\n  (un-/semi-)supervised training.</p>\n<p>Document Classification: by topic, sentiment, spam content,\n  etc, bag-of-words, word embeddings, feature selection /\n  induction.</p>\n<p>Structured prediction: sequence tagging, graph parsing,\n  incremental language generation with recurrent neural\n  networks.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>understand the issues involved in applying machine learning\n    approaches to a range of language processing applications;</li>\n<li>understand the theory underlying a number of machine\n    learning approaches that have been applied to language\n    processing, including: Naive Bayes, Perceptron, Logistic\n    Regression, and Multi-Layer Perceptron;</li>\n<li>understand some applications and specific tasks including:\n    document topic classification and clustering, SPAM filtering,\n    PoS tagging, named entity recognition, event extraction,\n    language modelling and word embeddings.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Students will be expected to undertake reading for assigned\n  lectures and seminars. Each student will give a 20 minute\n  presentation of one paper.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Students will receive one tick worth 5% for attendance at\n    seminar sessions, reading of assigned material, and\n    satisfactory contribution during seminars.</li>\n<li>Students will receive a second tick worth 5% for a\n    satisfactory presentation of an assigned paper.</li>\n<li>students will undertake a small project to be agreed with\n    the lecturers and write a project report of not more than 5000\n    words. The report will be due around the beginning of the Lent\n    Term (see academic calendar for precise date), will be assessed\n    by the lecturers, and will account for 90% of the module\n    marks.</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Bishop, C. (2006). <em>Pattern recognition and machine\n  learning</em>. Springer. (Chaps: 1, 2, 4-9, 13).</p>\n<p>Jurafsky, D. &amp; Martin, J. (2008). <em>Speech and language\n  processing</em>. Prentice Hall (2nd ed.). (Chaps: 4-6, 22)(see\n  also 3rd ed. draft, online).</p>\n<p>Manning, C., Raghavan, P. &amp; Schutze, H. (2008).\n  <em>Introduction to information retrieval</em>. Cambridge\n  University Press. (Chaps: 12-18).</p>\n<p>Goodfellow et al, DL (Chaps 6-12).</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Machine Learning for Language Processing", "course_code": "L101", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L101", "lecturers": ["av308"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L335": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course aims at introducing the theoretical foundations\n  and practical techniques for machine perception, the capability\n  of computers to interpret data resulting from sensor\n  measurements. The course will teach the fundamentals and modern\n  techniques of machine perception, i.e. reconstructing the real\n  world starting from sensor measurements with a focus on machine\n  perception for visual data. The topics covered will be\n  image/geometry representations for machine perception, semantic\n  segmentation, object detection and recognition, geometry capture,\n  appearance modeling and acquisition, motion detection and\n  estimation, human-in-the-loop machine perception, select topics\n  in applied machine perception.</p>\n<p>Machine perception/computer vision is a rapidly expanding area\n  of research with real-world applications. An understanding of\n  machine perception is also important for robotics, interactive\n  graphics (especially AR/VR), applied machine learning, and\n  several other fields and industries. This course will provide a\n  fundamental understanding of and practical experience with the\n  relevant techniques.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Students will understand the theoretical underpinnings of\n    the modern machine perception techniques for reconstructing\n    models of reality starting from an incomplete and imperfect\n    view of reality.</li>\n<li>Students will be able to apply machine perception theory to\n    solve practical problems, e.g. classification of images,\n    geometry capture.</li>\n<li>Students will gain an understanding of which machine\n    perception techniques are appropriate for different tasks and\n    scenarios.</li>\n<li>Students will have hands-on experience with some of these\n    techniques via developing a functional machine perception\n    system in their projects.</li>\n<li>Students will have practical experience with the current\n    prominent machine perception frameworks.</li>\n</ul>\n<h2>Syllabus</h2>\n<ul>\n<li>The fundamentals of machine learning for machine\n    perception</li>\n<li>Deep neural networks and frameworks for machine\n    perception</li>\n<li>Semantic segmentation of objects and humans</li>\n<li>Object detection and recognition</li>\n<li>Motion estimation, tracking and recognition</li>\n<li>3D geometry capture</li>\n<li>Appearance modeling and acquisition</li>\n<li>Select topics in applied machine perception</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>A practical exercise, worth 20% of the mark. This will\n    cover the basics and theory of machine perception and some of\n    the practical techniques the students will likely use in their\n    projects. This is individual work. No GPU hours will be needed\n    for the practical work.</li>\n</ul>\n<ul>\n<li>A machine perception project worth 80% of the marks:\n      <ul>\n<li>Course projects will be selected by the students\n        following the possible project\u00a0themes proposed by the\n        lecturer,\u00a0and will be checked by the lecturer for\n        appropriateness.\u00a0</li>\n</ul>\n<ul>\n<li>The students will form groups of 2-3 to design,\n        implement, report, and present a project to tackle a given\n        task in machine perception.</li>\n</ul>\n<ul>\n<li>The final mark will be composed of an\n        implementation/report mark (60%) and a presentation mark\n        (20%). Each team member will be evaluated based on her/his\n        contribution.</li>\n</ul>\n<ul>\n<li>Each project will have extensions to be completed only\n        by the ACS students. Each student will write a different\n        part of the report, whose author will be clearly marked.\n        Each student will further summarise her/his contributions\n        to the project in the same report.</li>\n</ul>\n</li>\n</ul>\n<h2>Recommended Reading List</h2>\n<ul>\n<li><em>Computer Vision: Algorithms and Applications, Richard\n    Szeliski, Springer, 2010.</em></li>\n<li><em>Deep Learning, Ian Goodfellow, Yoshua Bengio, and Aaron\n    Courville, MIT Press, 2016.</em></li>\n<li><em>Machine Learning and Visual Perception, Baochang Zhang,\n    De Gruyter, 2020.</em></li>\n</ul>\n", "course_name": "Machine Visual Perception", "course_code": "L335", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L335", "lecturers": ["cpt23"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L304": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>In recent years multiprocessors have become ubiquitous, but\n  building reliable concurrent systems with good performance\n  remains very challenging. The aim of this module is to introduce\n  some of the theory and the practice of concurrent programming,\n  from hardware memory models and the design of high-level\n  programming languages to the correctness and performance\n  properties of concurrent algorithms.</p>\n<h2>Lectures</h2>\n<p>Part 1: Introduction and relaxed-memory concurrency [Professor\n  P. Sewell]</p>\n<ul>\n<li><strong>Introduction</strong>. Sequential consistency,\n    atomicity, basic concurrent problems. [1 block]</li>\n<li><strong>Concurrency on real multiprocessors</strong>: the\n    relaxed memory model(s) for x86, ARM, and IBM Power, and\n    theoretical tools for reasoning about x86-TSO programs. [2\n    blocks]</li>\n<li><strong>High-level languages</strong>. An introduction to\n    C/C++11 and Java shared-memory concurrency. [1 block]</li>\n</ul>\n<p>Part 2: Concurrent algorithms [Dr T. Harris]</p>\n<ul>\n<li><strong>Concurrent programming</strong>. Simple algorithms\n    (readers/writers, stacks, queues) and correctness criteria\n    (linearisability and progress properties). Advanced\n    synchronisation patterns (e.g. some of the following:\n    optimistic and lazy list algorithms, hash tables,\n    double-checked locking, RCU, hazard pointers), with discussion\n    of performance and on the interaction between algorithm design\n    and the underlying relaxed memory models. [3 blocks]</li>\n<li><strong>Research topics</strong>, likely to include one\n    hour on transactional memory and one guest lecture. [1\n    block]</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>have a good understanding of the semantics of concurrent\n    programs, both at the multprocessor level and the C/Java\n    programming language level;</li>\n<li>have a good understanding of some key concurrent\n    algorithms, with practical experience.</li>\n</ul>\n<h2>Assessment</h2>\n<p>Two assignments each worth 50%</p>\n<h2>Recommended reading</h2>\n<p>Herlihy, M. and Shavit, N. (2008). <em>The art of\n  multiprocessor programming</em>. Morgan Kaufmann.</p>\n", "course_name": "Multicore Semantics and Programming", "course_code": "L304", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L304", "lecturers": ["pes20", "tlh20"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R02": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide the world with more network\n  architects. The 2011-2012 version was oriented around the\n  evolution of IP to support new services like multicast, mobility,\n  multihoming, pub/sub and, in general, data oriented networking.\n  The course is a <em>paper reading</em> which puts the onus on the\n  student to do the work.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>IPng [2 lectures, Jon Crowcroft]</li>\n<li>New Architectures [2 lectures, Jon Crowcroft]</li>\n<li>Multicast [2 lectures, Jon Crowcroft]</li>\n<li>Content Distribution and Content Centric Networks [2\n    lectures, Jon Crowcroft]</li>\n<li>Resource Pooling [2 lectures, Jon Crowcroft]</li>\n<li>Green Networking [2 lectures, Jon Crowcroft]</li>\n<li>Alternative Router Implementions [2 lectures, Jon\n    Crowcroft]</li>\n<li>Data Center Networks [2 Lectures, Jon Crowcroft]</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should be able to:</p>\n<ul>\n<li>contribute to new network system designs;</li>\n<li>engineer evolutionary changes in network systems;</li>\n<li>identify and repair architectural design flaws in networked\n    systems;</li>\n<li>see that there are no perfect solutions (aside from\n    academic ones) for routing, addressing, naming;</li>\n<li>understand tradeoffs in modularisation and other pressures\n    on clean software systems implementation, and see how the world\n    is changing the proper choices in protocol layering, or non\n    layered or cross-layered.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Assessment is through three graded essays (each chosen\n  individually from a number of suggested or student-chosen\n  topics), as follows:</p>\n<ol>\n<li>Analysis of two different architectures for a particular\n    scenario in terms of cost/performance tradeoffs for some\n    functionality and design dimension, for example:\n      <ul>\n<li>ATM \u2013 e.g. for hardware <em>versus</em> software\n        tradeoff</li>\n<li>IP \u2013 e.g. for mobility, multi-homing, multicast,\n        multipath</li>\n<li>3GPP \u2013 e.g. for plain complexity <em>versus</em>\n        complicatedness</li>\n</ul>\n</li>\n<li>A discursive essay on a specific communications systems\n    component, in a particular context, such as <em>ad hoc</em>\n    routing, or wireless sensor networks.</li>\n<li>A bespoke network design for a narrow, well specified\n    specialised target scenario, for example:\n      <ul>\n<li>A customer baggage tracking network for an\n        airport.</li>\n<li>in-flight entertainment system.</li>\n<li>in-car network for monitoring and control.</li>\n<li>inter-car sensor/control network for automatic\n        highways.</li>\n</ul>\n</li>\n</ol>\n<h2>Practical work</h2>\n<p>This course does not feature any implementation work due to\n  time constraints.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Three 1,200-word essays (worth 25% each), and</li>\n<li>an annotated bibliography (25%).</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Pre-course reading:</p>\n<p>Keshav, S. (1997). <em>An engineering approach to computer\n  networking</em>. Addison-Wesley (1st ed.). ISBN 0201634422<br/>\n  Peterson, L.L. and Davie, B.S. (2007). <em>Computer networks: a\n  systems approach</em>. Morgan Kaufmann (4th ed.).</p>\n<p>Design patterns:</p>\n<p>Day, John (2007). <em>Patterns in network architecture: a\n  return to fundamentals</em>. Prentice Hall.</p>\n<p>Example systems:</p>\n<p>Krishnamurthy, B. and Rexford, J. (2001). <em>Web protocols\n  and practice: HTTP/1.1, Networking protocols, caching, and\n  traffic measurement</em>. Addison-Wesley.</p>\n<p>Economics and networks:</p>\n<p>Frank, Robert H. (2008). <em>The economic naturalist: why\n  economics explains almost everything</em>.</p>\n<p>Papers:</p>\n<p>Certainly, a collection of papers (see <a href=\"http://ccr.sigcomm.org/\">ACM CCR</a> which publishes notable\n  network researchers' favourite ten papers every 6 months or\n  so).</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Network Architectures", "course_code": "R02", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R02", "lecturers": ["jac22"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L90": {"supervisions": 0, "prerequisite_for": ["L95"], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course introduces the fundamental techniques of natural\n  language processing. It aims to explain the potential and the\n  main limitations of these techniques. Some current research\n  issues are introduced and some current and potential applications\n  discussed and evaluated. Students will also be introduced to\n  practical experimentation in natural language processing.</p>\n<h2>Lectures</h2>\n<ul>\n<li><strong>Overview.</strong>\u00a0Brief history\n    of\u00a0NLP\u00a0research, some current applications,\n    components of\u00a0NLP\u00a0systems.</li>\n<li><strong>Morphology and Finite State\n    Techniques.</strong>\u00a0Morphology in different languages,\n    importance of morphological analysis in\u00a0NLP, finite-state\n    techniques in\u00a0NLP.</li>\n<li><strong>Part-of-Speech Tagging and Log-Linear\n    Models.</strong>\u00a0Lexical categories, word tagging, corpora\n    and annotations, empirical evaluation.</li>\n<li><strong>Phrase Structure and Structure\n    Prediction.</strong>\u00a0Phrase structures, structured\n    prediction, context-free grammars, weights and probabilities.\n    Some limitations of context-free grammars.</li>\n<li><strong>Dependency Parsing.</strong>\u00a0Dependency\n    structure, grammar-free parsing,\n    incremental\u00a0processing.\u00a0</li>\n<li><strong>Gradient Descent and Neural\n    Nets.</strong>\u00a0Parameter optimisation by gradient descent.\n    Non-linear functions with neural network layers. Log-linear\n    model as softmax layer. Current findings of\n    Neural\u00a0NLP.</li>\n<li><strong>Word representations</strong>. Representing words\n    with vectors, count-based\u00a0and prediction-based approaches,\n    similarity metrics.</li>\n<li><strong>Recurrent Neural Networks.</strong>\u00a0Modelling\n    sequences, parameter sharing in recurrent neural networks,\n    neural\u00a0language\u00a0models, word prediction.</li>\n<li><strong>Compositional Semantics.</strong>\u00a0Logical\n    representations, compositional semantics, lambda calculus,\n    inference and robust entailment.</li>\n<li><strong>Lexical Semantics.</strong>\u00a0Semantic\n    relations, WordNet, word senses.</li>\n<li><strong>Discourse.</strong>\u00a0Discourse relations,\n    anaphora resolution, summarization.</li>\n<li>\n<strong>Natural\u00a0Language\u00a0Generation.</strong>\u00a0Challenges\n    of\u00a0natural\u00a0language\u00a0generation (NLG), tasks in\n    NLG, surface\u00a0realisation.</li>\n<li><strong>Practical and assignments.</strong>\u00a0Students\n    will build\n    a\u00a0natural\u00a0language\u00a0processing\u00a0system which\n    will be trained and evaluated on supplied data. The system will\n    be built from existing components, but students will be\n    expected to compare approaches and\n    some\u00a0programming\u00a0will be required for this. Several\n    assignments will be set during the practicals for\n    assessment.</li>\n</ul>\n<h2>Objectives</h2>\n<p>By the end of the course students should:</p>\n<ul>\n<li>be able to discuss the current and likely future\n    performance of several NLP applications;</li>\n<li>be able to describe briefly a fundamental technique for\n    processing language for several subtasks, such as morphological\n    processing, parsing, word sense disambiguation etc.;</li>\n<li>understand how these techniques draw on and relate to other\n    areas of computer science.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<ul>\n<li>Assignment 1 - 10% of marks</li>\n<li>Assignment 2 - 25% of marks</li>\n<li>Assignment 3 - 65% of marks</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>* Jurafsky, D. and Martin, J. (2008) <em>Speech and language\n  processing</em>. Prentice Hall.</p>\n", "course_name": "Overview of Natural Language Processing", "course_code": "L90", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L90", "lecturers": ["sht25", "apc38", "ws390"], "lectures": 18, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L46": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course will examine the emerging principles and\n  methodologies that underpin scalable and efficient machine\n  learning systems. Primarily, the course will focus on an exciting\n  cross-section of algorithms and system techniques that are used\n  to support the training and inference of machine learning models\n  under a spectrum of computing systems that range from constrained\n  embedded systems up to large-scale distributed systems. It will\n  also touch upon the new engineering practices that are developing\n  in support of such systems at scale. When needed to appreciate\n  issues of scalability and efficiency, the course will drill down\n  to certain aspects of computer architecture, systems software and\n  distributed systems and explore how these interact with the usage\n  and deployment of state-of-the-art machine learning.</p>\n<h2>Syllabus</h2>\n<p>Topics covered may include the following, with confirmation a\n  month before the course begins:</p>\n<ul>\n<li>System Performance Trade-offs</li>\n<li>Distributed Learning Algorithms\u00a0</li>\n<li>Model Compression\u00a0</li>\n<li>Deep Learning Compilers\u00a0</li>\n<li>Frameworks and Run-times\u00a0</li>\n<li>Scalable Inference Serving\u00a0</li>\n<li>Development Practices\u00a0</li>\n<li>Automated Machine Learning\u00a0</li>\n<li>Federated Learning\u00a0</li>\n</ul>\n<p>Primarily, topics are covered with conventional lectures.\n  However, where appropriate, material will be delivered through\n  hands-on lab tutorials. Lab tutorials will make use of hardware\n  including ARM microcontrollers and multi-GPU machines to explore\n  forms of efficient machine learning (any necessary equipment will\n  be provided to students)</p>\n<h2>Assessment</h2>\n<p>Each student will be assessed on 3 labs which will be worth\n  30% of their grade. They will also undertake a written project\n  report which will be worth\u00a070% of the grade. This report\n  will detail an investigation into a particular aspect of machine\n  learning systems, this report will be made available\n  publicly.</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Principles of Machine Learning Systems", "course_code": "L46", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L46", "lecturers": [], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "R252": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The objectives of this course is to expose you to one of the\n  most active contemporary research\u00a0directions within machine\n  learning: the theory of deep learning (DL). While the first wave\n  of\u00a0modern DL has focussed on empirical breakthroughs and\n  ever more complex techniques, the\u00a0attention is now shifting\n  to building a solid mathematical understanding of why these\n  techniques<br/>\n  work so well in the first place. The purpose of this course is to\n  review this recent progress\u00a0through a mixture of reading\n  group sessions and invited talks by leading researchers in\n  the\u00a0topic, and prepare you to embark on a PhD in modern deep\n  learning research. Compared to\u00a0typical, non-mathematical\n  courses on deep learning, this advanced module will appeal to\n  those\u00a0who have strong foundations in mathematics and\n  theoretical computer science. In a way, this\u00a0course is our\n  answer to the question \u201cWhat should the world\u2019s best computer\n  science students\u00a0know about deep learning in 2021?\u201d</p>\n<h2>Learning Outcomes</h2>\n<p>This course should prepare the best students to start a PhD in\n  the theory and mathematics of\u00a0deep learning, and to start\n  formulating their own hypotheses in this space. You will\n  be\u00a0introduced to a range of empirical and mathematical tools\n  developed in recent years for the\u00a0study of deep learning\n  behaviour, and you will build an awareness of the main open\n  questions\u00a0and current lines of attack. At the end of the\n  course you will:</p>\n<ol>\n<li>be able to explain why classical learning theory is\n    insufficient to describe the\u00a0phenomenon of generalization\n    in DL</li>\n<li>be able to design and interpret empirical studies aimed at\n    understanding generalization</li>\n<li>be able to explain the role of overparameterization: be\n    able to use deep linear models as\u00a0a model to study\n    implicit regularisation of gradient-based learning</li>\n<li>be able to state PAC-Bayes and Information-theoretic\n    bounds, and apply them to DL</li>\n<li>be able to explain the connection between Gaussian\n    processes and neural networks,\u00a0and will be able to study\n    learning dynamics in the neural tangent kernel (NTK)\n    regime.</li>\n<li>be able to formulate your own hypotheses about DL and\n    choose tools to prove/test them</li>\n<li>leverage your deeper theoretical understanding to produce\n    more robust, rigorous and\u00a0reproducible solutions to\n    practical machine learning problems.</li>\n</ol>\n<h2>Syllabus</h2>\n<p>Each week we'll have two or three student-lead presentations\n  about a research paper chosen from a reading list. Occasionally,\n  we'll include invited guest lectures by top researchers in the\n  field. The reading list follows the weekly breakdown below:</p>\n<p>Week 1: Introduction to the topic<br/>\n  Week 2: Empirical Studies of Deep Learning Phenomena<br/>\n  Week 3: Interpolation Regime and \u201cDouble Descent\u201d Phenomena<br/>\n  Week 4: Implicit Regularization in Deep Linear Models<br/>\n  Week 5: Approximation Theory<br/>\n  Week 6: Networks in the Infinite Width Limit<br/>\n  Week 7: PAC-Bayes and Information Theoretic Bounds for SGD<br/>\n  Week 8: Discussion and Coursework Spotlight Session</p>\n<h2>Assessment</h2>\n<p>Students will be assessed on the following basis:</p>\n<ol>\n<li>20% for presentation/content contributed to the module:\n    Each student will have an\u00a0opportunity to present one of\n    the recommended papers during Weeks 1-7 (30 minute\u00a0slot +\n    10 mins Q&amp;A). For the presentation, students should aim to\n    communicate the core\u00a0ideas behind the paper, and clearly\n    present the results, conclusions, and future\u00a0directions.\n    Where possible, students are encouraged to comment on how the\n    work itself\u00a0fits into broader research goals.</li>\n<li>10% for active participation (regular attendance and\n    contribution to discussions during\u00a0the Q&amp;A\n    sessions).</li>\n<li>70% for a coursework report, with a word limit of 4000.\n    Either (1) an original research\u00a0proposal/report with a\n    hypothesis, review of related literature, and ideally\n    preliminary\u00a0findings, or, (2) reproduction and ideally\n    extension of an existing relevant paper.<br/>\n    Coursework reports are marked in line with general ACS\n    guidelines, reports receiving\u00a0top marks will have have\n    demonstrable research value (contain an original\n    research\u00a0idea, extension of existing work, or a thorough\n    reproduction effort which is valuable to\u00a0the research\n    community). Additionally, some projects will be suggested\n    during the first weeks of the course, although students are\n    encouraged to come up with their own ideas. Students may be\n    required to participate in group projects, with groups of size\n    2-3 (the class groups will be separated). For any given\n    project, individual contributions would be noted for assessment\n    though a viva component.</li>\n</ol>\n<h2>Relationship with related modules</h2>\n<p>This course can be considered as an advanced follow-up to the\n  Part IIB course on Deep Neural\u00a0Networks. That course\n  introduces some high level\u00a0concepts that this course\n  significantly expands on.</p>\n<p>This module complements L48: Machine Learning in the Physical\n  World and L46: Principles of\u00a0Machine Learning Systems, which\n  focus on applications and hardware/systems aspects of\n  ML\u00a0respectively.</p>\n<h2>Recommended reading</h2>\n<ul>\n<li><a href=\"https://www.pnas.org/cc/arthur-m-sackler-colloquium-on-the-science-of-deep-learning\">\n    PNAS Colloquium on the Science of Deep Learning</a></li>\n<li><a href=\"https://mml-book.github.io/\">Mathematics of\n    Machine Learning book by Marc Deisenroth, Aldo Faisal and Cheng\n    Soon Ong.</a></li>\n<li><a href=\"https://probml.github.io/pml-book/book1.html\">Probabilistic\n    Machine Learning: An Introduction book by Kevin Murphy</a></li>\n<li><a href=\"https://mjt.cs.illinois.edu/dlt/\">Matus\n    Telgarsky's lecture notes on deep learning\n    theory</a>\u00a0</li>\n</ul>\n<p>These are in addition to the papers which will be discussed in\n  the lectures.</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Theory of Deep Learning", "course_code": "R252", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R252", "lecturers": [], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": true, "lent": false, "easter": false}, "L41": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p><em>Systems research</em> refers to the study of a broad range\n  of behaviours arising from complex system design, including:\n  low-level operating systems; resource sharing and scheduling;\n  interactions between hardware and software; network-protocol\n  design and implementation; separation of mutually distrusting\n  parties on a common platform; and control of distributed-system\n  behaviours such as concurrency and data replication. This module\n  will:</p>\n<ol>\n<li>Teach systems-analysis methodology and practice through\n    tracing and performance profiling experiments;</li>\n<li>Expose students to real-world systems artefacts such as\n    I/O, IPC,and network-stack implementations, and consider their\n    hardware-software interactions with CPUs;</li>\n<li>Develop scientific experimentation, analysis and\n    presentation skills through a series of laboratory assignments;\n    and</li>\n<li>Assign a selection of original research papers to give\n    insight into potential research topics and approaches.</li>\n</ol>\n<p>The teaching style will blend lectures and hands-on labs that\n  teach methodology, design principles, and practical skills.\n  Students will be taught about (and assessed via) a series of\n  lab\u00a0assignments based on practical work. The systems studied\n  are real, and all wires will be live.</p>\n<h2>Prerequisites</h2>\n<p>It is strongly recommended that students:</p>\n<ol>\n<li>Have previously (and successfully) completed an\n    undergraduate operating-system course\n     -- or have equivalent experience\n    through project or open-source work.</li>\n<li>Have reasonable comfort with the C and Python programming\n    languages. C is the primary implementation language for systems\n    that we will analyse, requiring reading fluency; userspace C\n    programs will also be written and extended as part of lab\n    exercises. Python will be used as our data-collection and\n    processing language, and provides useful tools for data\n    analysis and presentation.</li>\n<li>Review an undergraduate operating-system textbook (such as\n    the 'Dinosaur Book') to ensure that basic OS concepts such as\n    the <em>process model</em>, <em>inter-process\n    communication</em>, <em>filesystems</em>, <em>network\n    stacks</em>, and <em>virtual memory</em> are familiar.</li>\n</ol>\n<h2>Syllabus</h2>\n<p>The sessions are split up into three submodules:</p>\n<ol>\n<li>\n<strong>Introduction to kernels and kernel\n      tracing/analysis</strong>\n<p>The purpose of this submodule is to introduce students to\n      the structure of a contemporary operating system kernel\n      through tracing and profiling.</p>\n<ul>\n<li><strong>Lecture 1:</strong> Introduction: OSes and this\n        course\u00a0(1h)</li>\n<li><strong>Lecture 2:</strong> Kernels and Tracing\n        (1h)</li>\n<li><strong>Lecturelet 1:</strong> I/O Lab (30m)</li>\n<li><strong>Lab 1:</strong>\u00a0I/O (2x2h lab sessions, if\n        in person; otherwise short 1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 1 -\u00a0\n        I/O\u00a0</strong></li>\n</ul>\n</li>\n<li>\n<strong>Processors, processes, and threads</strong>\n<p>This submodule introduces students to concrete\n      implications of the UNIX process model: processes and threads\n      in both userspace and kernelspace, the hardware foundations\n      for kernel and process isolation, system calls, and\n      traps.</p>\n<ul>\n<li><strong>Lecture 3:</strong> The Process Model - 1\n        (1h)</li>\n<li><strong>Lecture 4:</strong> The Process Model - 2\n        (1h)</li>\n<li><strong>Lecturelet 2:</strong> IPC Lab (30m)</li>\n<li><strong>Lab 2:</strong>\u00a0IPC (2x2h lab sessions, if\n        in person; otherwise short 1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 2 - IPC\n        \u00a0</strong></li>\n</ul>\n</li>\n<li>\n<strong>TCP/IP</strong>\n<p>This submodule introduces students to a contemporary,\n      multithreaded, multiprocessing network stack, with a\n      particular interest in the TCP protocol. Labs will consider\n      both the behaviour of a single TCP connection, exploring the\n      TCP state machine, socket-buffer interactions with flow\n      control, and TCP congestion control. Students will use\n      DUMMYNET to simulate network latency and explore how TCP slow\n      start and congestion avoidance respond to network conditions.\n      The second marked lab assignment will be written.</p>\n<ul>\n<li><strong>Lecture 5:</strong> The Network Stack (1)\n        (1h)</li>\n<li><strong>Lecture 6:</strong> The Network Stack (2)\n        (1h)</li>\n<li><strong>Lecturelet 3:</strong> TCP/IP Lab (30m)</li>\n<li><strong>Lab 3:</strong> TCP/IP (2x2h lab sessions, if\n        in person; otherwise short1:1 supervisions)</li>\n<li><strong>Deliverable: Lab Assignment 3 -\u00a0 TCP\n        /IP</strong></li>\n</ul>\n</li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Have a good understanding of high-level OS kernel\n    structure</li>\n<li>Gained insight into hardware-software interactions for\n    compute and I/O</li>\n<li>Have practical skills in system tracing and performance\n    analysis</li>\n<li>Have been exposed to research ideas in system structure and\n    behaviour</li>\n<li>Have learned how to perform systems-style performance\n    evaluations</li>\n<li>Have learned how to present systems evaluation results</li>\n</ul>\n<h2>Assessment</h2>\n<ul>\n<li>Exercise 1: Getting started with tracing 10%\n      <ul>\n<li>\n<p>This starting exercise gets students working with\n          DTrace on their RPi4 through some simple interactive\n          exercises.</p>\n</li>\n</ul>\n</li>\n<li>Exercise 2:\u00a0I/O 20%\n      <ul>\n<li>\n<p>This second exercise asks students to analyse I/O\n          performance exploring specific performance behaviour\n          using their now developed tracing skills.</p>\n</li>\n</ul>\n</li>\n<li>Exercise 3: IPC 70%\n      <ul>\n<li>\n<p>This third exercise asks students to analyse IPC\n          performance exploring specific performance behaviour\n          using now developed analysis skills.</p>\n</li>\n</ul>\n</li>\n</ul>\n<h2>Recommended reading</h2>\n<h3>Primary module texts</h3>\n<p>Course texts provide instruction on statistics,\n  operating-system design and implementation, and system tracing.\n  You will be asked to read selected chapters from these, but will\n  likely find other content in them useful as you proceed with the\n  labs.</p>\n<p>Marshall Kirk McKusick, George V. Neville-Neil, and Robert N.\n  M. Watson. <em>The Design and Implementation of the FreeBSD\n  Operating System, 2nd Edition</em>, Pearson Education, Boston,\n  MA, USA, September 2014.</p>\n<p>Brendan Gregg and Jim Mauro. <em>DTrace: Dynamic Tracing in\n  Oracle Solaris, Mac OS X and FreeBSD</em>, Prentice Hall Press,\n  Upper Saddle River, NJ, USA, April 2011.</p>\n<h3>Additional texts</h3>\n<p>Abraham Silberschatz, Peter Baer Galvin, and Greg Gagne,\n  <em>Operating System Concepts, Eighth Edition</em>, John Wiley\n  and Sons, Inc., New York, NY, USA, July 2008.</p>\n<p>Brendan Gregg. <em>Systems Performance: Enterprise and the\n  Cloud</em>, Prentice Hall Press, Upper Saddle River, NJ, USA,\n  October 2013.</p>\n<h3>Research-paper readings</h3>\n<p>Research-paper readings will be announced as the terms\n  proceed, but will likely include original papers on BPF, DTrace,\n  OS scheduling, OS scalability, network stacks, and systems\n  modelling.</p>\n", "course_name": "Advanced Operating Systems", "course_code": "L41", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L41", "lecturers": ["rnw24"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L32": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims and Objectives</h2>\n<p>This module aims to extend the knowledge and skills of\n  students in designing and developing autonomous machines and\n  researching robotics-related topics. Beyond the Introduction to\n  Robotics course given in Michaelmas Term, the Advanced Robotics\n  course will focus on more advanced topics such as Robot Learning,\n  Underactuated Robot Control, Soft Robotics, Human-Robot\n  Interaction, and Multi-Agent Systems, which are not covered in\n  the introductory course.</p>\n<h2>Lectures</h2>\n<ol>\n<li>Introduction\n      <ul>\n<li>Course overview</li>\n<li>History and landscape of robotics</li>\n<li>basic knowledge and theories (kinematics, dynamics,\n        planning/search)</li>\n</ul>\n</li>\n<li>Underactuated Robotics\n      <ul>\n<li>Problem formulation and modelling</li>\n<li>Control approaches of underactuated systems</li>\n<li>Case studies</li>\n</ul>\n</li>\n<li>Robot Learning and Adaptation\n      <ul>\n<li>Model-based learning approaches</li>\n<li>Model-free learning approaches</li>\n<li>Optimization methods and case studies</li>\n</ul>\n</li>\n<li>Soft Robotics (2L; F Iida)\n      <ul>\n<li>Soft material/body robot modelling</li>\n<li>Soft actuators and sensors</li>\n<li>Control and learning of soft robots</li>\n</ul>\n</li>\n<li>5. Human-Robot Interaction 1\n      <ul>\n<li>Introduction to human-robot interaction</li>\n<li>Theoretical frameworks (spatial, nonverbal, verbal\n        interactions)</li>\n<li>Research methods, applications, robots in society</li>\n</ul>\n</li>\n<li>Distributed Robotics, Multi-Agent Systems\n      <ul>\n<li>Planning and control in multi-robot systems</li>\n<li>Methods for learning coordination and cooperation in\n        multi-agent systems</li>\n</ul>\n</li>\n<li>Coursework Presentations</li>\n</ol>\n<h2>Assessment</h2>\n<p>The assessment will be 100% coursework and consist of three\n  elements:</p>\n<ol>\n<li>first individual written report (30%)</li>\n<li>intermediate group project presentation (20%)</li>\n<li>final individual written report (50%)</li>\n</ol>\n<p>The first report is about theoretical questions on the topics\n  of advanced robotics.<br/>\n  The project will be conducted in groups of 2-3 students, and the\n  topics should be either or both simulation/hardware. The\n  intermediate presentation will be delivered by the groups.<br/>\n  The final report is expected to be a professional presentation\n  about the project, extended from the intermediate\n  presentation.</p>\n<h2>Recommended reading</h2>\n<ul>\n<li>Ronald C. Arkin 1949. Behavior-Based Robotics / Ronald C.\n    Arkin. Cambridge, Mass. : MIT Press, c1998.; 1998.</li>\n<li>Bruno Siciliano 1959; Oussama Khatib editor., eds. Springer\n    Handbook of Robotics / Edited by Bruno Siciliano, Oussama\n    Khatib. 2nd Edition. Cham : Springer International Publishing,\n    2016.; 2016.</li>\n<li>Rolf Pfeifer. Understanding Intelligence / Rolf Pfeifer and\n    Christian Scheier ; with Figures by Alex Riegler and Cartoons\n    by Isabelle Follath. (Christian Scheier, ed.). MIT Press;\n    1999.</li>\n<li>Fantoni, Isabelle, Lozano, Rogelio, Non-linear Control for\n    Underactuated Mechanical Systems, Springer, 2002</li>\n</ul>\n", "course_name": "Advanced Robotics", "course_code": "L32", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L32", "lecturers": ["asp45"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L118": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Teaching</h2>\n<p>The teaching style will be largely based on lectures, but\n  supported by a practical component\u00a0where students will learn\n  to use a proof assistant for higher category theory.</p>\n<h2>Aims</h2>\n<p>The module will introduce advanced topics in category theory.\n  The aim is to train students to\u00a0engage and start modern\n  research on the mathematical foundations of higher\n  categories,\u00a0the graphical calculus, logical systems,\n  programming languages, type theories, and their\u00a0applications\n  in theoretical computer science, both classical and quantum.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Be familiar with the techniques of compositional category\n    theory.</li>\n<li>Have a strong understanding of basic categorical semantic\n    models.</li>\n<li>Have glimpsed current research in higher categorical\n    structures.</li>\n</ul>\n<h2>Syllabus</h2>\n<ul>\n<li>Monoidal categories and the graphical calculus (Lectures 1\n    and 2)</li>\n<li>Coherence, higher categories (Lectures 3 and 4)</li>\n<li>Linearity, superposition (Lecture 5)</li>\n<li>Duality, quantum entanglement (Lecture 6)</li>\n<li>Monoids, Frobenius algebras, and bialgebras (Lectures 7 and\n    8)</li>\n<li>Monoidal models of dualities (Lecture 9)</li>\n<li>Presheaf categories (Lectures 10, 11, and 12)</li>\n<li>Models of abstract syntax (Lecture 13 and 14)</li>\n<li>Normalisation by evaluation (Lectures 15 and 16)</li>\n</ul>\n<h2>Practical</h2>\n<p>There will be 4 practical sessions where students will be\n  guided to use the proof assistant\u00a0homotopy.io. These would\n  take place in the computer room, but could be delivered\n  over\u00a0Zoom if required, since the tool is web-based and can\n  be easily accessed from any location.<br/>\n  Once students gain an understanding of the tool, they will choose\n  problems to attempt from\u00a0a long list of suggestions, of\n  varying difficulty, from easy bookwork to\n  research-level.\u00a0<br/>\n  At the end of the course, students will be required to submit 5\n  homotopy.io project\u00a0workspaces, demonstrating their best\n  work using the system. At least 3 of these workspaces\u00a0should\n  be attempts on research-level problems. The practical portfolio\n  will be graded and\u00a0form part of the assessment of the\n  course.<br/>\n  No special computing resources are required, the tool runs\n  adequately on an ordinary\u00a0laptop.</p>\n<h2>Classes</h2>\n<p>There will be four exercise sheets for optional homework with\n  accompanying\u00a0classes by a teaching assistant to go over\n  them.</p>\n<h2>Assessment</h2>\n<ul>\n<li>Take-home test\u00a0(65%)</li>\n<li>Practical portfolio (35%)</li>\n</ul>\n<h2>Reading List</h2>\n<p>Chris Heunen and Jamie Vicary, \u201cCategory for Quantum Theory:\n  An Introduction\u201d, Oxford\u00a0University Press</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Advanced Topics in Category Theory", "course_code": "L118", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L118", "lecturers": ["mpf23", "jv258"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R01": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module will attempt to provide an overview of \u201csystems\u201d\n  research. This is a very broad field which has existed for over\n  50 years and which has historically included areas such as\n  operating systems, database systems, file systems, distributed\n  systems and networking, to name but a few. The course will thus\n  necessarily cover only a tiny subset of the field.</p>\n<p>Many good ideas in systems research are the result of\n  discussing and debating previous work. A primary aim of this\n  course therefore will be to educate students in the art of\n  <em>critical thinking</em>: the ability to argue for and/or\n  against a particular approach or idea. This will be done by\n  having students read and critique a set of papers each week. In\n  addition, each week will include presentations from a number of\n  participants which aim to advocate or criticise each piece of\n  work.</p>\n<h2>Syllabus</h2>\n<p>The syllabus for this course will vary from year to year so as\n  to cover a mixture of older and more contemporary systems papers.\n  Contemporary papers will be generally selected from the past 5\n  years, primarily drawn from high quality conferences such as\n  SOSP, OSDI, ASPLOS, FAST, NSDI and EuroSys. Example topics might\n  include:</p>\n<ul>\n<li><em>Systems Research and System Design</em>,</li>\n<li><em>OS Structure and Virtual Memory</em>,</li>\n<li><em>Systems Virtualisation</em>,</li>\n<li><em>Consensus</em>,</li>\n<li><em>Scheduling</em>,</li>\n<li><em>Privacy</em>,</li>\n<li><em>Data Intensive Computing</em>, and</li>\n<li><em>Bugs</em>.</li>\n</ul>\n<p>The reading each week will involve a load equivalent to 3 full\n  length papers. Students will be expected to read these in detail\n  and prepare a written summary and review. In addition, each week\n  will contain one or more short presentations by students for each\n  paper. The types of presentation will include:</p>\n<ul>\n<li><strong>Overview</strong>: a balanced presentation of the\n    paper, covering both positive and negative aspects.</li>\n<li><strong>Advocacy</strong>: a positive spin on the paper,\n    aiming to convince others of its value.</li>\n<li><strong>Criticism</strong>: a negative take on the paper,\n    focusing on its weak spots and omissions.</li>\n</ul>\n<p>These presentation roles will be assigned in advance,\n  regardless of the <em>soi disant</em> absolute merit of the paper\n  or the preference of the student. Furthermore, all students \u2013\n  regardless of any assigned presentation role in a given week \u2013\n  will be expected to participate in the class by asking questions\n  and generally entering into the debate.</p>\n<h2>Objectives</h2>\n<p>On completion of this module students should have a broad\n  understanding of some key papers and concepts in computer systems\n  research, as well as an appreciation of how to argue for or\n  against any particular idea.</p>\n<h2>Coursework and practical work</h2>\n<p>Coursework will be the production of the weekly paper reviews.\n  Practical work will be presenting papers as appropriate, as well\n  as ongoing participation in the class.</p>\n<h2>Assessment</h2>\n<p>Assessment consists of:</p>\n<ul>\n<li>One essay per week for 7 weeks (10% each)</li>\n<li>Presentation (20%)</li>\n<li>Participation in class over the term (10%)</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Most of the reading for this course will be in the form of the\n  selected papers each week. However, the following may be useful\n  background reading to refresh your knowledge from undergraduate\n  courses:</p>\n<p>Silberschatz, A., Peterson, J.L. and Galvin, P.C. (2005).\n  <em>Operating systems concepts</em>. Addison-Wesley (7th\n  ed.).</p>\n<p>Tanenbaum, A.S. (2008). <em>Modern Operating Systems</em>.\n  Prentice-Hall (3rd ed.).</p>\n<p>Bacon, J. and Harris, T. (2003). <em>Operating systems</em>.\n  Addison-Wesley (3rd ed.).</p>\n<p>Anderson, T. and Dahlin, M. (2014). <em>Operating Systems:\n  Principles and Practice</em>. Recursive Books (2nd ed.).</p>\n<p>Hennessy, J. and Patterson, D. (2006). <em>Computer\n  architecture: a quantitative approach</em>. Elsevier (4th ed.).\n  ISBN\u00a0978-0-12-370490-0.</p>\n<p>Kleppmann M (2016) <em>Designing Data-Intensive\n  Applications</em>, O'Reilly (1st ed.)</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Advanced Topics in Computer Systems", "course_code": "R01", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R01", "lecturers": ["rmm1002"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R255": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course explores current research topics in machine\n  learning\u00a0in sufficient depth that, at the end of the course,\n  participants will be in a position to contribute to research on\n  their chosen topics. Each topic will be introduced with a lecture\n  which, building on the material covered in the prerequisite\n  courses, will make the current research literature accessible.\n  Each lecture will be followed by up to three seminar sessions\n  which will typically be run as a reading group with student\n  presentations on recent papers from the literature followed by a\n  discussion.</p>\n<h2>Structure</h2>\n<p>Each student will attend 3 topics and each topic's sessions\n  will be spread over 5 contact hours. Students will be expected to\n  undertake readings for their selected topics. There will be some\n  group work.</p>\n<p>There will be a briefing session in Michaelmas term.</p>\n<h2>Syllabus</h2>\n<p>Students choose exactly\n  three\u00a0<strong>topics</strong>\u00a0in preferential order\n  from a list to be published in Michaelmas term. Students are\n  assessed on one of these topics which may\u00a0not necessarily be\n  their first choice topic.</p>\n<p>The topics to be offered in 2022-23 are yet to be decided but\n  to give an indicative idea of the types of topics, the ones\n  offered\u00a0in 2021-22 were:</p>\n<ol>\n<li>Imitation learning\u00a0<em>Dr A. Vlachos</em></li>\n<li>Applications of Machine Learning to Psychiatry <em>Dr S.\n    Morgan</em></li>\n<li>Federated Learning\u00a0<em>Dr N.\u00a0Lane</em></li>\n<li>Reinforcement learning\u00a0<em>Dr A.\u00a0Prorok</em></li>\n<li>Machine Learning of self-organizing structures: from\n    textures to developmental biology\u00a0<em>Dr\n    B.\u00a0Dumitrascu</em></li>\n<li>Causal Inference <em>Dr F.\u00a0Husz\u00e1r</em></li>\n<li>Bias in datasets\u00a0<em>Dr M.\u00a0Tomalin</em></li>\n<li>Probabilistic Numerics: Computation as statistical\n    inference\u00a0<em>Dr C. H.\u00a0Ek</em></li>\n</ol>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be in a strong position to contribute to the research\n    topics covered;</li>\n<li>understand the fundamental methods (algorithms, data\n    analysis, specific tasks) underlying each topic;</li>\n<li>and be familiar with recent research papers and advances in\n    the field.</li>\n</ul>\n<h2>Coursework</h2>\n<p>Students will work in groups to give a presentation on\n  assigned papers. Each topic will typically consist of one\n  preliminary lecture followed by 3 reading and discussion\n  sessions. A typical topic can accommodate up to 9 students\n  presenting papers. There will be at least 10 minutes general\n  discussion per session.</p>\n<p>Full coursework details will be published by October.</p>\n<h2>Assessment</h2>\n<p>Coursework will be marked by the topic leaders and second\n  marked by the module conveners.</p>\n<ul>\n<li>Participation/attendance in\u00a0three topics, 10%</li>\n<li>Presentation (for one of the chosen topics), 20%</li>\n<li>Topic coursework (for one of the chosen\u00a0topics),\n    70%</li>\n</ul>\n<p>Individual topic coursework will be published late Michaelmas\n  term.</p>\n<p><em>Please note that students will be assessed on one of their\n  three chosen topics\u00a0but this may not\n  be\u00a0their\u00a0first choice</em>.</p>\n<h2>Recommended reading</h2>\n<p>To be confirmed.</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Advanced topics in machine learning", "course_code": "R255", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R255", "lecturers": ["mj201"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R254": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This course examines major topics relating to cybercrime from\n  an interdisciplinary perspective. These include offence types and\n  techniques, targets, victimisation, social and financial cost,\n  criminal marketplaces, offenders, detection and prevention, and\n  regulation and policing. The course outlines: key debates in\n  cybercrime research; how crime is committed using computer\n  systems; and provides an understanding of how cybercrime is\n  regulated, policed, detected, and prevented.</p>\n<h2>Syllabus</h2>\n<p>The course will consist of eight two-hour sessions\n  covering:</p>\n<ul>\n<li>Tools and techniques of cybercrime</li>\n<li>Cybercrime victimisation</li>\n<li>Costs and harms of cybercrime</li>\n<li>Criminal marketplaces</li>\n<li>Cybercrime offenders and offender pathways</li>\n<li>Cybercrime prevention (situational and social\n    approaches)</li>\n<li>Regulation and policy</li>\n<li>Cybercrime and the criminal justice system</li>\n</ul>\n<p>\u00a0</p>\n<p>All participants are expected to attend and participate in\n  every class, and to read the specified papers beforehand. The\n  instructor must be notified of any absences in advance.</p>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Have a broad knowledge of the key themes, debates, theory,\n    and research in relation to cybercrime;</li>\n<li>Have developed further skills in critical analysis;</li>\n<li>Have developed skills in presenting a case study,\n    critically evaluating current issues, and writing about\n    cybercrime;</li>\n<li>Have a sound understanding of strategies to combat and\n    prevent cybercrime;</li>\n<li>Understand the ethical and practical challenges in\n    conducting cybercrime research.</li>\n</ul>\n<h2>Assessment - Part II Students</h2>\n<p>You will assessed by 4 essays each worth 25% of the total\n  marks</p>\n<h2>Recommended reading</h2>\n<p>Please see Course Materials for recommended reading for each\n  session.</p>\n<p>\u00a0</p>\n", "course_name": "Cybercrime", "course_code": "R254", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R254", "lecturers": ["ah793", "rja14", "rnc1"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "R47": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This reading group course examines foundations and current\n  research into distributed ledger (blockchain) technologies and\n  their applications. Students will read, review, and present\n  seminal research papers in this area. Once completed, students\n  should be able to integrate blockchain technologies into their\n  own research and gain familiarity with a range of research\n  skills.</p>\n<h2>Lectures</h2>\n<ol>\n<li>Introduction</li>\n<li>Consensus protocols</li>\n<li>Bitcoin and its variants</li>\n<li>Ethereum, smart contracts, and other permissionless\n    DLTs</li>\n<li>Hybrid and permissioned DLTs</li>\n<li>Applications</li>\n</ol>\n<h2>Learning objectives</h2>\n<p>There are two broad objectives: to acquire familiarity with a\n  body of work in the area of distributed ledgers and to learn some\n  specific research skills:</p>\n<ol>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/home/Papers/data/07/paper-reading.pdf\" target=\"_blank\">How to read a paper</a></li>\n<li><a href=\"http://pages.cs.wisc.edu/~markhill/the_task_of_the_referee.pdf\" target=\"_blank\">How to review a paper</a></li>\n<li><a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Evaluating_a_research_paper\" target=\"_blank\">How to analyze a paper\u2019s strengths and\n    weaknesses</a></li>\n<li><a href=\"http://www-net.cs.umass.edu/kurose/talks/top_10_tips_for_writing_a_paper.ppt\" target=\"_blank\">Written</a> and <a href=\"http://blizzard.cs.uwaterloo.ca/keshav/wiki/index.php/Giving_and_attending_talks\" target=\"_blank\">oral</a> presentation skills</li>\n</ol>\n<h2>Assessment</h2>\n<p>You are expected to read all assigned papers and submit paper\n  reviews\u00a0each week. Each review must either follow the\n  provided review form [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:b:/g/personal/sk818_cam_ac_uk/ESpwszt8Nr5PtSF5b9GGH4EBT0WLI_z-bj-jPnRd5i2YYw?e=xrDeau\" target=\"_blank\">PDF</a>] [<a href=\"https://universityofcambridgecloud-my.sharepoint.com/:u:/g/personal/sk818_cam_ac_uk/Ecpv3poyjuVKgbvC8fL48jIBRnAGTdVLBFKvMPf2RJjYrw?e=rnWfGE\" target=\"_blank\">Latex source</a>]. Each \u201creview\u201d is worth 5% of\n  your total mark, and is marked out of 100 with 60 a passing\n  grade. Marks will be awarded and penalties for late submission\n  applied according to <a href=\"http://www.cl.cam.ac.uk/teaching/exams/acs_assessment.html\" target=\"_blank\">ACS Assessment Guidelines</a>.\u00a0</p>\n<ul>\n<li>One paper review for the first week, then two paper reviews\n    each week for 6 weeks (13 reviews, 5% each) 65%</li>\n<li>Summative essay - 5% for outline and 30% for the final\n    report (Total 35%)</li>\n</ul>\n<h2>Recommended Reading</h2>\n<p>Narayanan, A. , Bonneau, J., Felten, E., Miller, A. and\n  Goldfeder, S. (2016). <em>Bitcoin and Cryptocurrency\n  Technologies: A Comprehensive Introduction</em>. Princeton\n  University Press.<br/>\n  (2016 Draft available here: <a href=\"https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf\" target=\"_blank\">https://d28rh4a8wq0iu5.cloudfront.net/bitcointech/readings/princeton_bitcoin_book.pdf</a>)</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Distributed Ledger Technologies: Foundations and Applications", "course_code": "R47", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/R47", "lecturers": ["sk818"], "lectures": null, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L361": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Objectives</h2>\n<p>This course aims to extend the machine learning knowledge\n  available to students in Part I (or present in typical\n  undergraduate degrees in other universities), and allow them to\n  understand how these concepts can manifest in a decentralized\n  setting. The course will consider both theoretical (e.g.,\n  decentralized optimization) and practical (e.g., networking\n  efficiency) aspects that combine to define this growing area of\n  machine learning.\u00a0</p>\n<p>At the end of the course students should:</p>\n<ul>\n<li>Understand popular methods used in federated learning</li>\n<li>Be able to construct and scale a simple federated\n    system</li>\n<li>Have gained an appreciation of the core limitations to\n    existing methods, and the approaches available to cope with\n    these issues</li>\n<li>Developed an intuition for related technologies like\n    differential privacy and secure aggregation, and are able to\n    use them within typical federated settings\u00a0</li>\n<li>Can reason about the privacy and security issues with\n    federated systems</li>\n</ul>\n<h2>Lectures</h2>\n<ol>\n<li>Course Overview. Introduction to Federated Learning.</li>\n<li>Decentralized Optimization.</li>\n<li>Statistical and Systems Heterogeneity.</li>\n<li>Variations of Federated Aggregation.</li>\n<li>Secure Aggregation.</li>\n<li>Differential Privacy within Federated Systems.</li>\n<li>Extensions to Federated Analytics.</li>\n<li>Applications to Speech, Video, Images and Robotics.</li>\n</ol>\n<h2>Lab sessions</h2>\n<ol>\n<li>Federating a Centralized ML Classifier.</li>\n<li>Behaviour under Heterogeneity.</li>\n<li>Scaling a Federated Implementation.</li>\n<li>Exploring Privacy with Federated Settings</li>\n</ol>\n<h2>Advanced Material sessions - MPhil / Part III Students</h2>\n<p>These sessions are mandatory for the MPhil and Part III\n  students. Part II CST students may attend if they wish</p>\n<p>Topics and announced the first week of class. Selected to\n  extend beyond topics covered in lectures and labs. Content is a\n  mixture of sessions run in the lecture room the are either (1)\n  presentations by guest lectures by an invited domain expert or\n  (2) class-wide discussions regarding one or more related academic\n  papers. Paper discussions will require students to read the paper\n  ahead of the lecture, and a brief discussion primer presentation\n  will be given students before discussions begin.\u00a0</p>\n<h2>Assessment</h2>\n<p>Four labs are performed during the course, and students\n  receive 12.5% of their total grade for work done as part of each\n  lab. (For a total of 50% of the total grade from lab work alone).\n  Labs will primarily provide hands-on teaching opportunities, that\n  are then utilized within the lab assignment which is completed\n  outside of the lab contact time. MPhil and Part III students will\n  be given additional questions to answer within their version of\n  the lab assignment which will differ from the assignment given to\n  Part II CST students.</p>\n<p>The remainder of the course grade (50%) will be given based on\n  a hands-on project that applies the concepts taught in lectures\n  and labs. This hands-on project will be assessed based on upon a\n  combination of source code, related documentation\u00a0and brief\n  8-minute pre-recorded talk that summarizes key project elements\n  (any slides used are also submitted\u00a0as part of the project).\n  Please note, that in the case of Part II CST students, the talk\n  itself is not examinable -- as such will be made optional to\n  those students.</p>\n<p>A range of possible practical projects will be described and\n  offered to students to select from, or alternatively students may\n  propose their own. MPhil and Part III students will select from a\n  project pool that is separate from those offered to Part II CST\n  students. MPhil and Part III projects will contain a greater\n  emphasis on a research element, and the pre-recorded talks\n  provided by this student group will focus on this research\n  contribution. The project will be assessed on the level of\n  student understanding demonstrated, the degree of difficulty,\n  correctness of implementation -- and for Part III/MPhil students\n  the additional criteria of the quality and execution of the\n  research methodology, and depth and quality of results\n  analysis.</p>\n<p>This project can be done individually or in groups -- although\n  individual projects will be strongly encouraged. It will be\n  required the project is performed using a code repository that\n  also will contain all documentation -- access to this repository\n  will be shared with course staff (e.g., lecturer and TAs). Where\n  needed, marks assigned to students within a group will be\n  differentiated using this repository as an input. Furthermore if\n  groups are formed, members must be either entirely from Part\n  III/MPhil students or Part II CST, i.e., these two student groups\n  should not mix to form a project group.</p>\n<p>Projects will be made available publicly. A maximum word count\n  for written contributions for the project will be\n  enforced.\u00a0</p>\n<h2>Recommended Reading</h2>\n<p>Readings will be assigned for each lecture. Readings will be\n  taken from either research papers, tutorials, source code or\n  blogs that provide more comprehensive treatment of taught\n  concepts.</p>\n", "course_name": "Federated Learning", "course_code": "L361", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L361", "lecturers": [], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "P51": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module provides an introduction to High Performance\n  Networking, it explores both software and hardware aspects and\n  provides the students an opportunity to experience high\n  performance networking design and usage first hand.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Introduction to High Performance Networking</li>\n<li>High throughput networking architecture, design and\n    evaluation</li>\n<li>Low latency networking architecture, design and\n    evaluation</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>Describe the role of high performance networking and where\n    it is used;</li>\n<li>Compare and contrast high throughput and low latency\n    networking devices;</li>\n<li>Define the architecture of a high performance networking\n    device;</li>\n<li>Implement a fully functioning high performance networking\n    device;</li>\n<li>Evaluate the performance of a high performance networking\n    device.</li>\n</ul>\n<h2>Practical work</h2>\n<p>Five 2-hour in-classroom labs will ask students to develop and\n  use skills learned in the course and apply them to the design of\n  a high performance device.</p>\n<p>The first lab will provide an introduction to a development\n  platform, while the remaining labs will focus on a specific\n  design project (starting from a reference design). Time will be\n  allocated for different design stages (architecture, design and\n  validation). Students may find it useful to work in groups or 2\n  or 3 within the lab, and the project can be extended in any\n  relevant direction. Instructors will be on-hand throughout labs\n  to provide guidance.</p>\n<p>Lab participation is not directly included in the final mark,\n  but lab work is a key input to a project that is assessed.</p>\n<p><strong><em>NOTE:</em> <em>This module has a large practical\n  element.\u00a0</em></strong></p>\n<h2>Assessment</h2>\n<p>Assessed practical work, based on 1 practical assignment\n  (including: project submission and its documentation).</p>\n<h2>Further information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "High performance networking", "course_code": "P51", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/P51", "lecturers": ["awm22"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L349": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>The course aims to explore how wearable and mobile systems\n  sensors can be used to gather data relevant to understand health,\n  how the data can be analysed with advanced signal processing and\n  machine learning and the performance of these systems in terms of\n  diagnostics and disease progression detection.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Course Overview. Introduction to Mobile Health. Evaluation\n    metrics and methodology. Basics of Signal Processing.</li>\n<li>Inertial Measurement Units, Human Activity Recognition\n    (HAR) and Gait Analysis and Machine Learning for IMU data.</li>\n<li>Radios, Bluetooth, GPS and Cellular. Epidemiology and\n    contact tracing, Social interaction sensing and\n    applications.\u00a0Location tracking in health monitoring and\n    public health.</li>\n<li>Audio Signal Processing. Voice and Speech Analysis:\n    concepts and data analysis.\u00a0Body Sounds analysis.</li>\n<li>Photoplethysmogram and Light sensing for health (heart and\n    sleep)</li>\n<li>Contactless and wireless behaviour and physiological\n    monitoring</li>\n<li>Mobile Devices and Behaviour intervention</li>\n<li>Topical Guest Lectures</li>\n</ul>\n<h2>Objectives</h2>\n<p>The course aims to explore how wearable and mobile systems\n  sensors can be used to gather data relevant to understand health,\n  how the data can be analysed with advanced signal processing and\n  machine learning and the performance of these systems in terms of\n  diagnostics and disease progression detection.</p>\n<p>Roughly, each lecture contains a theory part about the working\n  of \u201csensor signals\u201d or \u201cdata analysis methods\u201d and an application\n  part which contextualises the concepts.</p>\n<p>At the end of the course students should: Understand how\n  mobile/wearable sensors capture data and their working.\n  Understand different approaches to acquiring and analysing sensor\n  data from different types of sensors. Understand the concept of\n  signal processing applied to time series data and their practical\n  application in health. Be able to extract sensor data and analyse\n  it with basic signal processing and machine learning techniques.\n  Be aware of the different health applications of the various\n  sensor techniques. The course will also touch on privacy and\n  ethics implications of the approaches developed in an orthogonal\n  fashion.</p>\n<h2>Assessment\u00a0-\u00a0Part II students</h2>\n<p>Two assignments will be based on two datasets which will be\n  provided to the students. The first assignment (worth 30% of the\n  final mark) will be set of preprocessing and basic data analysis\n  steps in a \u201ccolab\u201d style report.<br/>\n  The second assignment (worth 70% of the final mark) will be a\n  fuller analysis where the students are asked to compare and\n  contrast ML algorithms/soutions and discuss findings and\n  interpretation in terms of health context. This will be in the\n  form of a colab and a reflection report of 1000 words.</p>\n<h2>Recommended Reading</h2>\n<p>Please see Course Materials for recommended reading for each\n  session.</p>\n", "course_name": "Mobile Health", "course_code": "L349", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L349", "lecturers": ["cm542"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L45": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims and Objectives</h2>\n<p>Most of the patterns we see in nature are elegantly\n  representable using the language of graph structures. Prominent\n  examples include molecules (represented as graphs of atoms and\n  bonds), social networks and transportation networks. Several\n  already-impacted application areas include traffic forecasting,\n  drug discovery, social network analysis and recommender systems.\n  Further, some of the most successful domains of application for\n  machine learning in previous years---images, text and speech\n  processing---can be seen as special cases of graph representation\n  learning, and consequently there has been significant exchange of\n  information between these areas. The module will provide the\n  students the capability to analyse graph-structured data in an\n  effective way, and position graph representation learning in a\n  proper context with related fields. The main aim of the course is\n  to enable students to make direct contributions to the field of\n  graph representation learning, thoroughly assimilate the key\n  concepts in the area, and draw relevant connections to various\n  other fields (such as NLP, Fourier Analysis and Probabilistic\n  Graphical Models). We assume only a basic background in machine\n  learning with deep neural networks. The course aims to empower\n  the students to discover new ideas in this area in future\n  years.</p>\n<h2>Learning outcomes</h2>\n<ul>\n<li>Fundamentals of processing data on graphs, as well as\n    impactful application areas for graph representation\n    learning</li>\n<li>Theoretical principles of graph learning: permutation\n    invariance and equivariance</li>\n<li>The three \"flavours\" of spatial graph neural networks\n    (GNNs) (convolutional, attentional, message passing) and their\n    relative merits</li>\n<li>Relevant connections of graph representation learning to\n    various other fields (such as NLP, Fourier Analysis and\n    Probabilistic Graphical Models)</li>\n<li>The \"bigger picture\" of GNNs within a broader geometric\n    deep learning framework.</li>\n</ul>\n<h2>Lectures</h2>\n<p>The lectures will cover the following topics:</p>\n<ul>\n<li>Why study data on graphs? Success stories: drug screening,\n    travel time estimation, recommender systems. Fundamentals of\n    graph data processing: network science, spectral clustering,\n    node embeddings.</li>\n<li>Inductive biases, with a brief look at CNNs. Permutation\n    invariance and equivariance on sets and graphs. The principal\n    tasks of node, edge and graph classification. Neural networks\n    for point clouds: Deep Sets, PointNet; universal approximation\n    properties.</li>\n<li>The three flavours of spatial GNNs: convolutional,\n    attentional, message passing. Prominent examples: ChebyNets,\n    GCN, SGC, MoNet, GAT, GaAN, IN, MPNN, GraphNets. Tradeoffs of\n    using different GNN variants.</li>\n<li>Revisiting node embeddings, and their link to conv-GNNs.\n    Self-supervised learning with GNNs: random-walk objectives\n    (VGAE), contrastive (DGI, GRACE), bootstrapping (BGRL), feature\n    masking. The effectiveness of randomly initialised GNNs.</li>\n<li>Revisiting Deep Sets: how to apply GNNs when there is no\n    graph? Links to natural language processing---Transformers as a\n    special case of attentional GNNs. Methodologies for latent\n    graph inference: DGCNN, NRI, DGM, PGN.</li>\n<li>Expressive power of graph neural networks: the\n    Weisfeiler-Lehman hierarchy. GINs as a maximally expressive\n    GNN. Links between GNNs and graph algorithms.</li>\n<li>The bigger picture of learning with invariances and\n    symmetries: geometric deep learning. Worked examples: Circulant\n    matrices on grids, the discrete Fourier transform, and\n    convolutional networks on spheres. Graph Fourier transform and\n    the Laplacian eigenbasis. Implications to spatial GNN\n    flavours.</li>\n</ul>\n<h2>Practicals</h2>\n<p>The practicals\u00a0are designed to complement the knowledge\n  learnt in lectures and teach students to derive additional\n  important results and architectures not directly shown in\n  lectures. The practicals will be given as a series of individual\n  exercises (each either code implementation or proof/derivation).\n  Each of these exercises can be individually assessed based on a\n  specified mark budget.</p>\n<p>Possible practical topics include the study of higher-order\n  GNNs and equivariant message passing.</p>\n<h2>Assessment</h2>\n<ul>\n<li>(60%) Mini-project (writeup) at the end of the course. The\n    mini projects can either be self-proposed, or the students can\n    express their preference for one of the provided topics, the\n    list of which will be announced at the start of term. The\n    projects\u00a0will consist of implementing and/or extending\n    graph representation learning models in the literature,\n    applying them to publicly available datasets. Students will\n    undertake the project in pairs and submit a joint writeup would\n    be limited to 4,000 words (in line with other modules);\n    appendix of work logs to be included but ungraded;</li>\n<li style=\"list-style: none\"><br/></li>\n<li>(10%) Short presentation and viva: students will give a\n    short presentation to explain their individual contribution to\n    the mini-project and there will be a short viva following.</li>\n<li style=\"list-style: none\"><br/></li>\n<li>(30%) Practical work completion. Completing the exercises\n    specified in the practicals to a satisfactory standard. The\n    practical assessor should be satisfied that the student derived\n    their answers using insight gained from the course; coupled\n    with original thought, not by simple copy-pasting of relevant\n    related work. The students would submit code and a short\n    report, which would then be marked in line with the\n    predetermined mark budget for each practical item.</li>\n</ul>\n<p>The students will learn how to run advanced architectures on\n  GPU but no specific need for dedicated GPU resources. Practicals\n  will be made possible to do on CPU; if required, students can use\n  GPUs on publicly available free services (such as Colab) for\n  their mini-project work.</p>\n<h2>References</h2>\n<p>The course will be based on the following literature:</p>\n<ul>\n<li>\"Graph Representation Learning\", by Will Hamilton</li>\n<li>\"Geometric Deep Learning: Grids, Graphs, Groups, Geodesics,\n    and Gauges\", by Michael Bronstein, Joan Bruna, Taco Cohen and\n    Petar Veli\u010dkovi\u0107</li>\n<li>\"Deep Learning\", by Ian Goodfellow, Yoshua Bengio and Aaron\n    Courville.</li>\n</ul>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Representation Learning on Graphs and Networks", "course_code": "L45", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L45", "lecturers": ["pl219"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}, "L15": {"supervisions": 0, "prerequisite_for": [], "past_exam_questions": null, "description": "<h2>Aims</h2>\n<p>This module aims to provide an introduction to topics in\n  complexity theory beyond that covered in the undergraduate course\n  and a grounding in research that connects this with methods from\n  logic. The topics covered in the last four lectures will focus on\n  current research and may vary from year to year.</p>\n<h2>Syllabus</h2>\n<ul>\n<li>Complexity theory\u2014a review of the major complexity classes\n    (space, time, nondeterministic, etc.) and their\n    interrelationships. [3 lectures]</li>\n<li>First-order and second-order logic: their expressive power\n    and computational complexity. [3 lectures]</li>\n<li>Lower bounds on expressive power: the use of games and\n    locality. [3 lectures]</li>\n<li>Fixed-point logics and descriptive complexity. [3\n    lectures]</li>\n<li>A selection of topics from the following [4 lectures]:\n      <ul>\n<li>finite-variable logics;</li>\n<li>complexity of constraint satisfaction problems;</li>\n<li>random structures;</li>\n<li>parameterized complexity;</li>\n<li>complexity of logical theories;</li>\n<li>logic and circuit complexity.</li>\n<li>logics of polynomial time computation.</li>\n</ul>\n</li>\n</ul>\n<h2>Objectives</h2>\n<p>On completion of this module, students should:</p>\n<ul>\n<li>be familiar with the basic relationship between the\n    expressive power of logic and computational complexity;</li>\n<li>be able to formulate simple game-based inexpressibility\n    arguments;</li>\n<li>be able to identify current research issues relating logic\n    to complexity.</li>\n</ul>\n<h2>Coursework and practical work</h2>\n<p>None.</p>\n<h2>Assessment</h2>\n<p>The assessment will be based on an essay completed during the\n  term, and a take-home test taken after the course, weighted as\n  follows:</p>\n<ul>\n<li>Essay: 30%</li>\n<li>Test: 70%</li>\n</ul>\n<h2>Recommended reading</h2>\n<p>Arora, S. and Barak, B. (2009). <em>Computational\n  complexity</em>. Cambridge University Press.<br/>\n  Gradel. E. <em>et al</em>. (2007). <em>Finite model theory and\n  its applications</em>. Springer.<br/>\n  Libkin, L. (2004). <em>Elements of finite model theory</em>.\n  Springer.<br/>\n  Immerman, N. (1999). <em>Descriptive complexity</em>.\n  Springer.<br/>\n  Ebbinghaus, H-D. and Flum, J. (1999). <em>Finite model\n  theory</em>. Springer.</p>\n<h2>Further Information</h2>\n<p>Due to infectious respiratory diseases, the method of teaching\n  for this module may be adjusted to cater for physical distancing\n  and students who are working remotely. Unless otherwise advised,\n  this module will be taught in person.</p>\n", "course_name": "Topics in Logic and Complexity", "course_code": "L15", "course_url": "https://www.cl.cam.ac.uk/teaching/2223/L15", "lecturers": ["ad260"], "lectures": 16, "year": "2223", "tripos_part": "3", "michaelmas": false, "lent": true, "easter": false}}